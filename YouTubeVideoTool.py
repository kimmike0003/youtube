import sys
import requests
import subprocess
import os
import collections
import base64
import traceback
import webbrowser  # Added for opening URLs
try:
    from elevenlabs_client import ElevenLabsClient # Import early to avoid mysql-connector/PyQt5 conflict
except ImportError:
    pass
import time
import re
from PyQt5.QtWidgets import (QApplication, QWidget, QVBoxLayout, QTextEdit, 
                             QPushButton, QLabel, QFileDialog, QHBoxLayout, 
                             QTabWidget, QComboBox, QSlider, QSpinBox, QGroupBox, QDoubleSpinBox, 
                             QFormLayout, QLineEdit, QGridLayout, QCheckBox, QMessageBox,
                             QTableWidget, QTableWidgetItem, QHeaderView, QAbstractItemView)
import json
import urllib.request
import urllib.parse
from datetime import datetime, timedelta
from PyQt5.QtCore import QThread, pyqtSignal, Qt, QTimer, QRect, QRectF
from PyQt5.QtGui import (QPalette, QColor, QFont, QImage, QPainter, QPen, QBrush, QPixmap, QFontDatabase, QFontInfo, 
                         QPainterPath, QTextDocument, QAbstractTextDocumentLayout)
import threading
import concurrent.futures
import multiprocessing
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from PIL import Image
# Monkey Patch for Pillow > 9.x not having ANTIALIAS, which MoviePy needs
if not hasattr(Image, 'ANTIALIAS'):
    Image.ANTIALIAS = Image.LANCZOS


import moviepy.editor as mpe
from youtube_workers import YoutubeSearchWorker, ImageLoadWorker

class GenSparkMultiTabWorker(QThread):
    progress = pyqtSignal(str)
    log_signal = pyqtSignal(str) 
    finished = pyqtSignal(str, float)
    error = pyqtSignal(str)

    def copy_to_clipboard(self, text):
        try:
            import pyperclip
            pyperclip.copy(text)
        except ImportError:
            # Fallback to pure python or just send_keys if failed (not ideal for korean)
            pass

    def __init__(self, file_path, items, driver, custom_target_dir=None):
        super().__init__()
        self.file_path = file_path
        self.items = items
        self.driver = driver # ì´ë¯¸ ì—´ë ¤ìˆëŠ” ë“œë¼ì´ë²„ ì‚¬ìš©
        
        if custom_target_dir:
            self.target_dir = custom_target_dir
        else:
            base_name = os.path.splitext(os.path.basename(file_path))[0]
            self.target_dir = os.path.join(r"D:\ai\image", base_name)
            
        os.makedirs(self.target_dir, exist_ok=True)

    def run(self):
        start_timestamp = time.time()
        try:
            if len(self.driver.window_handles) < 2:
                self.error.emit("âŒ ì˜¤ë¥˜: ë¸Œë¼ìš°ì € íƒ­ì´ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. íƒ­ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
                return

            tabs = self.driver.window_handles[:2]
            wait = WebDriverWait(self.driver, 20)

            total = len(self.items)
            tab_status = {tabs[0]: None, tabs[1]: None}
            tab_old_srcs = {tabs[0]: [], tabs[1]: []}
            
            processed_count = 0
            item_idx = 0
            failed_items = []

            self.is_running = True
            while processed_count < total and self.is_running:
                for tab in tabs:
                    if not self.is_running: break
                    self.driver.switch_to.window(tab)
                    
                    if tab_status[tab] is None and item_idx < total:
                        current_item = self.items[item_idx]
                        num, prompt = current_item
                        self.log_signal.emit(f"â–¶ [íƒ­ {tabs.index(tab)+1}] {num}ë²ˆ ìƒì„± ì‹œì‘...")
                        
                        tab_old_srcs[tab] = self.driver.execute_script("return Array.from(document.querySelectorAll('img')).map(img => img.src);")
                        
                        input_box = wait.until(EC.element_to_be_clickable((By.TAG_NAME, "textarea")))
                        input_box.click()
                        input_box.send_keys(Keys.CONTROL + "a")
                        input_box.send_keys(Keys.DELETE)
                        input_box.send_keys(prompt.strip())
                        time.sleep(1)
                        input_box.send_keys(Keys.ENTER)
                        
                        tab_status[tab] = {"item": current_item, "start_time": time.time()}
                        item_idx += 1
                        self.progress.emit(f"ì§„í–‰: {processed_count}/{total}")

                    elif tab_status[tab] is not None:
                        target_num = tab_status[tab]["item"][0]
                        img_data = self.check_image_once(self.driver, tab_old_srcs[tab])
                        
                        if img_data:
                            save_path = os.path.join(self.target_dir, f"{target_num}.png")
                            with open(save_path, "wb") as f:
                                f.write(base64.b64decode(img_data))
                            self.log_signal.emit(f"  âœ… [íƒ­ {tabs.index(tab)+1}] {target_num}ë²ˆ ì €ì¥ ì™„ë£Œ")
                            tab_status[tab] = None
                            processed_count += 1
                        
                        elif time.time() - tab_status[tab]["start_time"] > 220: # íƒ€ì„ì•„ì›ƒ ì•½ê°„ ìƒí–¥
                            self.log_signal.emit(f"  âŒ [íƒ­ {tabs.index(tab)+1}] {target_num}ë²ˆ íƒ€ì„ì•„ì›ƒ")
                            failed_items.append(tab_status[tab]["item"])
                            tab_status[tab] = None
                            processed_count += 1
                
                time.sleep(1) # ë£¨í”„ ì£¼ê¸° ë‹¨ì¶• (ë°˜ì‘ì„± í–¥ìƒ)

            if not self.is_running:
                 self.log_signal.emit("ğŸ›‘ ì‘ì—…ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                 
            elapsed_time = time.time() - start_timestamp
            result_msg = f"ì™„ë£Œ (ì„±ê³µ {total - len(failed_items)} / ì‹¤íŒ¨ {len(failed_items)})" if self.is_running else "ì¤‘ì§€ë¨"
            self.finished.emit(result_msg, elapsed_time)

        except Exception as e:
            self.error.emit(str(e))

    def stop(self):
        self.is_running = False

    def check_image_once(self, driver, old_srcs):
        script = """
        try {
            var old_srcs = arguments[0];
            var imgs = Array.from(document.querySelectorAll('img'));
            
            // ì œì™¸ í‚¤ì›Œë“œ
            var exclude = ['flaticon', 'logo', 'icon', 'svg', 'profile', 'avatar'];
            
            for (var i = 0; i < imgs.length; i++) {
                var img = imgs[i];
                var src = img.src;
                
                if (!src || src.startsWith('data:image/gif')) continue;
                if (img.width < 200 || img.height < 200) continue; 
                
                if (exclude.some(k => src.includes(k))) continue;

                if (!old_srcs.includes(src)) {
                    var canvas = document.createElement("canvas");
                    // í™”ë©´ì— ë³´ì´ëŠ” í¬ê¸°ê°€ ì•„ë‹Œ ì›ë³¸ í•´ìƒë„ ì‚¬ìš©
                    canvas.width = img.naturalWidth || img.width;
                    canvas.height = img.naturalHeight || img.height;
                    var ctx = canvas.getContext("2d");
                    ctx.drawImage(img, 0, 0);
                    return canvas.toDataURL("image/png").replace(/^data:image\\/(png|jpg);base64,/, "");
                }
            }
            return null;
        } catch (e) {
            return null;
        }
        """
        try:
            return driver.execute_script(script, old_srcs)
        except:
            return None

class NanoBananaMultiTabWorker(QThread):
    progress = pyqtSignal(str)
    log_signal = pyqtSignal(str) 
    finished = pyqtSignal(str, float)
    error = pyqtSignal(str)

    def copy_to_clipboard(self, text):
        try:
            import pyperclip
            pyperclip.copy(text)
        except ImportError:
            pass

    def __init__(self, file_path, items, driver, custom_target_dir=None):
        super().__init__()
        self.file_path = file_path
        self.items = items
        self.driver = driver
        
        if custom_target_dir:
            self.target_dir = custom_target_dir
        else:
            base_name = os.path.splitext(os.path.basename(file_path))[0]
            self.target_dir = os.path.join(r"D:\ai\image", base_name)
            
        os.makedirs(self.target_dir, exist_ok=True)

    def run(self):
        start_timestamp = time.time()
        try:
            if len(self.driver.window_handles) < 2:
                self.error.emit("âŒ ì˜¤ë¥˜: ë¸Œë¼ìš°ì € íƒ­ì´ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. íƒ­ì„ ì¶”ê°€í•´ì£¼ì„¸ìš”.")
                return

            tabs = self.driver.window_handles[:2]
            wait = WebDriverWait(self.driver, 20)

            total = len(self.items)
            tab_status = {tabs[0]: None, tabs[1]: None}
            tab_old_srcs = {tabs[0]: [], tabs[1]: []}
            
            processed_count = 0
            item_idx = 0
            failed_items = []

            dumped = False

            self.is_running = True
            while processed_count < total and self.is_running:
                for tab in tabs:
                    if not self.is_running: break
                    self.driver.switch_to.window(tab)
                    
                    # ë””ë²„ê¹…: í˜ì´ì§€ ì†ŒìŠ¤ ì €ì¥ (ìµœì´ˆ 1íšŒ)
                    if not dumped:
                        try:
                            with open(r"d:\python\youtube\gemini_debug.html", "w", encoding="utf-8") as f:
                                f.write(self.driver.page_source)
                            self.log_signal.emit("ğŸ› ë””ë²„ê¹…ìš© í˜ì´ì§€ ì†ŒìŠ¤ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (gemini_debug.html)")
                            dumped = True
                        except Exception as e:
                            print(f"Dump failed: {e}")
                    if not self.is_running: break
                    self.driver.switch_to.window(tab)
                    
                    if tab_status[tab] is None and item_idx < total:
                        current_item = self.items[item_idx]
                        num, prompt = current_item
                        self.log_signal.emit(f"â–¶ [íƒ­ {tabs.index(tab)+1}] {num}ë²ˆ ìƒì„± ì‹œì‘...")
                        
                        tab_old_srcs[tab] = self.driver.execute_script("return Array.from(document.querySelectorAll('img')).map(img => img.src);")
                        
                        # NanoBanana (Gemini) Input Handling
                        # Target rich-textarea editor
                        try:
                            # 1. Try finding the contenteditable div directly
                            input_box = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, "div.ql-editor, div[contenteditable='true']")))
                            input_box.click()
                            time.sleep(0.5)
                            
                            # Clear existing text (Ctrl+A -> Delete)
                            input_box.send_keys(Keys.CONTROL + "a")
                            input_box.send_keys(Keys.DELETE)
                            
                            # Send Prompt
                            # For rich text editors, sending keys usually works best.
                            # Splitting lines might help if it's finicky, but standard send_keys usually fine.
                            input_box.send_keys(prompt.strip())
                            time.sleep(1)
                            
                            # Send Enter
                            input_box.send_keys(Keys.ENTER)
                            
                        except Exception as e:
                            self.log_signal.emit(f"  âš ï¸ ì…ë ¥ì°½ ì°¾ê¸° ì‹¤íŒ¨ (ì¬ì‹œë„ ì¤‘): {e}")
                            # Fallback: JS injection (less reliable for rich text but worth a shot)
                            try:
                                js_script = """
                                var editor = document.querySelector('div.ql-editor');
                                if(editor) {
                                    editor.innerText = arguments[0];
                                    editor.dispatchEvent(new Event('input', { bubbles: true }));
                                    // Enter trigger might need specific key events
                                }
                                """
                                self.driver.execute_script(js_script, prompt.strip())
                                time.sleep(1)
                                input_box.send_keys(Keys.ENTER) 
                            except:
                                pass

                        
                        tab_status[tab] = {"item": current_item, "start_time": time.time()}
                        item_idx += 1
                        self.progress.emit(f"ì§„í–‰: {processed_count}/{total}")

                    elif tab_status[tab] is not None:
                        target_num = tab_status[tab]["item"][0]
                        img_data = self.check_image_once(self.driver, tab_old_srcs[tab])
                        
                        if img_data:
                            save_path = os.path.join(self.target_dir, f"{target_num}.png")
                            with open(save_path, "wb") as f:
                                f.write(base64.b64decode(img_data))
                            self.log_signal.emit(f"  âœ… [íƒ­ {tabs.index(tab)+1}] {target_num}ë²ˆ ì €ì¥ ì™„ë£Œ")
                            tab_status[tab] = None
                            processed_count += 1
                        
                        elif time.time() - tab_status[tab]["start_time"] > 220:
                            self.log_signal.emit(f"  âŒ [íƒ­ {tabs.index(tab)+1}] {target_num}ë²ˆ íƒ€ì„ì•„ì›ƒ")
                            failed_items.append(tab_status[tab]["item"])
                            tab_status[tab] = None
                            processed_count += 1
                
                time.sleep(1)

            if not self.is_running:
                 self.log_signal.emit("ğŸ›‘ ì‘ì—…ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                 
            elapsed_time = time.time() - start_timestamp
            result_msg = f"ì™„ë£Œ (ì„±ê³µ {total - len(failed_items)} / ì‹¤íŒ¨ {len(failed_items)})" if self.is_running else "ì¤‘ì§€ë¨"
            self.finished.emit(result_msg, elapsed_time)

        except Exception as e:
            self.error.emit(str(e))

    def stop(self):
        self.is_running = False

    def check_image_once(self, driver, old_srcs):
        # Initialize failure tracking
        if not hasattr(self, 'failed_srcs'):
            self.failed_srcs = set()

        try:
            images = driver.find_elements(By.TAG_NAME, 'img')
            
            exclude = ['icon', 'svg', 'profile', 'avatar', 'btn', 'button', 'logo', 'gstatic.com', 'googleusercontent.com/gadgets']

            # Search in reverse (newest first)
            for img in reversed(images):
                try:
                    src = img.get_attribute('src')
                    if not src: continue
                    
                    if src in old_srcs or src in self.failed_srcs:
                        continue
                        
                    if any(k in src for k in exclude):
                        continue

                    # Scroll thumbnail into view
                    driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", img)
                    time.sleep(0.5) 

                    # 1. Thumbnail Size Check
                    size = img.size
                    w, h = size['width'], size['height']
                    
                    if w < 200 or h < 200:
                        continue
                        
                    # aspect ratio check
                    ratio = w / h if h > 0 else 0
                    if ratio > 3.0 or ratio < 0.3:
                        continue

                    # ** Try to open Lightbox (High-Res) **
                    try:
                        driver.execute_script("arguments[0].click();", img)
                        
                        best_img = None
                        max_area = 0
                        
                        # Polling for high-res load (up to 10 seconds)
                        for _ in range(10):
                            time.sleep(1.0)
                            
                            current_imgs = driver.find_elements(By.TAG_NAME, 'img')
                            best_img_candidate = None
                            max_area_candidate = 0
                            
                            for m_img in current_imgs:
                                try:
                                    mw = int(m_img.get_attribute('naturalWidth') or 0)
                                    mh = int(m_img.get_attribute('naturalHeight') or 0)
                                    
                                    if mw > 600 and (mw * mh > max_area_candidate):
                                        max_area_candidate = mw * mh
                                        best_img_candidate = m_img
                                except:
                                    continue
                            
                            
                            if best_img_candidate:
                                best_img = best_img_candidate
                                max_area = max_area_candidate
                                if best_img.is_displayed():
                                    break
                                else:
                                    best_img = None
                        
                        result_data = None
                        
                        if best_img:
                            src = best_img.get_attribute('src')
                            
                            # 1. Python Requestsë¡œ ì§ì ‘ ë‹¤ìš´ë¡œë“œ (ê°€ì¥ ê°•ë ¥í•¨ - ì›ë³¸ íŒŒì¼ ê·¸ëŒ€ë¡œ ì €ì¥)
                            if not result_data:
                                try:
                                    session = requests.Session()
                                    # Selenium ì¿ í‚¤ ë³µì‚¬
                                    cookies = driver.get_cookies()
                                    for cookie in cookies:
                                        session.cookies.set(cookie['name'], cookie['value'])
                                    
                                    headers = {
                                        "User-Agent": driver.execute_script("return navigator.userAgent;")
                                    }
                                    
                                    resp = session.get(src, headers=headers, timeout=15)
                                    if resp.status_code == 200:
                                        result_data = base64.b64encode(resp.content).decode('utf-8')
                                        self.log_signal.emit("  ğŸ“¸ Requestsë¡œ ì›ë³¸ ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
                                except Exception as e:
                                    # self.log_signal.emit(f"  âš ï¸ Requests ì‹¤íŒ¨: {e}")
                                    pass

                            # 2. Fetch API (JS) ë°±ì—…
                            if not result_data:
                                try:
                                    script = """
                                    var callback = arguments[arguments.length - 1];
                                    var img = arguments[0];
                                    var src = img.src;
                                    
                                    fetch(src)
                                        .then(response => response.blob())
                                        .then(blob => {
                                            var reader = new FileReader();
                                            reader.onloadend = function() {
                                                callback(reader.result.split(',')[1]);
                                            }
                                            reader.readAsDataURL(blob);
                                        })
                                        .catch(err => {
                                            callback(null);
                                        });
                                    """
                                    result_data = driver.execute_async_script(script, best_img)
                                    if result_data:
                                        self.log_signal.emit("  ğŸ“¸ Fetch APIë¡œ ì›ë³¸ ë‹¤ìš´ë¡œë“œ ì„±ê³µ")
                                except:
                                    pass
                            
                            # 3. ìƒˆ íƒ­ ì—´ê¸° ë°±ì—…
                            if not result_data:
                                try:
                                    current_handle = driver.current_window_handle
                                    driver.execute_script("window.open(arguments[0], '_blank');", src)
                                    time.sleep(2.0)
                                    driver.switch_to.window(driver.window_handles[-1])
                                    full_img = driver.find_element(By.TAG_NAME, 'img')
                                    result_data = full_img.screenshot_as_base64
                                    driver.close()
                                    driver.switch_to.window(current_handle)
                                    self.log_signal.emit("  ğŸ“¸ ìƒˆ íƒ­ ì—´ê¸°ë¡œ ìº¡ì²˜ ì„±ê³µ")
                                except:
                                    try:
                                        if len(driver.window_handles) > 2: driver.close()
                                        driver.switch_to.window(current_handle)
                                    except: pass
                        
                        # 4. ê³ í•´ìƒë„ ì‹¤íŒ¨ ì‹œ ì¸ë„¤ì¼ì´ë¼ë„ ì €ì¥ (Fallback)
                        if not result_data:
                            self.log_signal.emit(f"  âš ï¸ ê³ í•´ìƒë„ ì‹¤íŒ¨ -> ì¸ë„¤ì¼ ì•ˆì „ ìº¡ì²˜ ì‹œë„")
                            # ì¸ë„¤ì¼ì´ í™”ë©´ ë°–ìœ¼ë¡œ ë‚˜ê°€ì§€ ì•Šê²Œ ìŠ¤íƒ€ì¼ ê°•ì œ ì¡°ì •
                            try:
                                driver.execute_script("""
                                    arguments[0].style.position = 'fixed';
                                    arguments[0].style.top = '50%';
                                    arguments[0].style.left = '50%';
                                    arguments[0].style.transform = 'translate(-50%, -50%)';
                                    arguments[0].style.maxWidth = '90vw';
                                    arguments[0].style.maxHeight = '90vh';
                                    arguments[0].style.objectFit = 'contain';
                                    arguments[0].style.zIndex = '99999';
                                    arguments[0].style.backgroundColor = 'black';
                                """, img)
                                time.sleep(0.5)
                                result_data = img.screenshot_as_base64
                            except:
                                result_data = img.screenshot_as_base64 # ì§„ì§œ ìµœí›„ì˜ ìˆ˜ë‹¨
                        
                        # Close Lightbox (ESC)
                        try:
                            driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.ESCAPE)
                        except:
                            pass
                        time.sleep(0.5)
                        
                        if result_data:
                            return result_data
                        else:
                            self.failed_srcs.add(src)
                            
                    except Exception:
                        self.failed_srcs.add(src)
                        pass
                    
                except Exception:
                    continue
                    
            return None
            
        except Exception:
            return None


class ImageFXMultiTabWorker(GenSparkMultiTabWorker):
    def run(self):
        start_timestamp = time.time()
        try:
            if len(self.driver.window_handles) < 1:
                self.error.emit("âŒ ì˜¤ë¥˜: ë¸Œë¼ìš°ì € íƒ­ì´ ì—†ìŠµë‹ˆë‹¤.")
                return

            tabs = self.driver.window_handles[:2] # ìµœëŒ€ 2ê°œ íƒ­ í™œìš©
            wait = WebDriverWait(self.driver, 20)

            total = len(self.items)
            tab_status = {tab: None for tab in tabs}
            tab_old_srcs = {tab: [] for tab in tabs}
            
            processed_count = 0
            item_idx = 0
            failed_items = []

            self.is_running = True
            while processed_count < total and self.is_running:
                for tab in tabs:
                    if not self.is_running: break
                    self.driver.switch_to.window(tab)
                    
                    if tab_status[tab] is None and item_idx < total:
                        current_item = self.items[item_idx]
                        num, prompt = current_item
                        self.log_signal.emit(f"â–¶ [íƒ­ {tabs.index(tab)+1}] {num}ë²ˆ ìƒì„± ì‹œì‘ (ImageFX)...")
                        
                        tab_old_srcs[tab] = self.driver.execute_script("return Array.from(document.querySelectorAll('img')).map(img => img.src);")
                        
                        # ImageFX ì…ë ¥ì°½ ì°¾ê¸° (Gensparkì™€ ë¹„ìŠ·í•˜ê²Œ textarea ì‹œë„)
                        # ImageFX ì…ë ¥ì°½ ì°¾ê¸° ë° ì´ˆê¸°í™”
                        # ImageFX ì…ë ¥ì°½ ì°¾ê¸° ë° ì´ˆê¸°í™” (ìµœì¢…: ActionChains + Clipboard)
                        input_box = None
                        try:
                            # 1. JSë¡œ Shadow DOM ê¹Šìˆ™í•œ ê³³ì˜ textarea ì°¾ê¸°
                            script_find_input = """
                            function findInput(root) {
                                if (!root) return null;
                                // í…ìŠ¤íŠ¸ ì˜ì—­ ìš°ì„  íƒìƒ‰
                                var el = root.querySelector('textarea, [contenteditable="true"], input[type="text"]');
                                if (el) return el;
                                // Shadow Root íƒìƒ‰
                                var walker = document.createTreeWalker(root, NodeFilter.SHOW_ELEMENT, null, false);
                                while(walker.nextNode()) {
                                    if (walker.currentNode.shadowRoot) {
                                        var res = findInput(walker.currentNode.shadowRoot);
                                        if (res) return res;
                                    }
                                }
                                return null;
                            }
                            return findInput(document);
                            """
                            input_box = self.driver.execute_script(script_find_input)
                            
                            # ëª» ì°¾ì•˜ìœ¼ë©´ bodyë¶€í„° ì‹œì‘
                            from selenium.webdriver.common.action_chains import ActionChains
                            actions = ActionChains(self.driver)
                            
                            if input_box:
                                # ì°¾ì•˜ìœ¼ë©´ í•´ë‹¹ ìš”ì†Œë¡œ ì´ë™ í›„ í´ë¦­
                                try:
                                    self.driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", input_box)
                                    time.sleep(0.5)
                                    actions.move_to_element(input_box).click().perform()
                                except:
                                    self.driver.execute_script("arguments[0].click();", input_box)
                            else:
                                # ëª» ì°¾ì•˜ìœ¼ë©´ í™”ë©´ ì¤‘ì•™ í´ë¦­ í›„ íƒ­ í‚¤ ì—°íƒ€ ì‹œë„
                                self.log_signal.emit("âš ï¸ ì…ë ¥ì°½ ìë™ ê°ì§€ ì‹¤íŒ¨. TAB í‚¤ íƒìƒ‰ ì‹œë„...")
                                body = self.driver.find_element(By.TAG_NAME, 'body')
                                actions.move_to_element(body).click().perform()
                                time.sleep(0.2)
                                # íƒ­ í‚¤ 5ë²ˆ ì •ë„ ëˆŒëŸ¬ë³´ë©° active element í™•ì¸ (ìƒëµí•˜ê³  ê·¸ëƒ¥ ë°”ë¡œ ë¶™ì—¬ë„£ê¸° ì‹œë„í•  ìˆ˜ë„ ìˆìŒ)
                                # ì¼ë‹¨ íƒ­ ëª‡ ë²ˆ ëˆ„ë¥´ê³  ë¶™ì—¬ë„£ê¸° ì‹œë„
                                actions.send_keys(Keys.TAB * 3).perform() 

                            time.sleep(0.5)
                            
                        except Exception as e:
                            self.log_signal.emit(f"âš ï¸ ì´ˆê¸°í™” ì˜¤ë¥˜ ì¬ì‹œë„... ({e})")
                            continue
                        
                        # í”„ë¡¬í”„íŠ¸ ì…ë ¥ (ë¬´ì¡°ê±´ í´ë¦½ë³´ë“œ ë¶™ì—¬ë„£ê¸° - ê°€ì¥ í™•ì‹¤)
                        p_text = prompt.strip()
                        
                        try:
                            import pyperclip
                            pyperclip.copy(p_text)
                            
                            # ActionChainsë¡œ Ctrl+A -> Del -> Ctrl+V ìˆ˜í–‰
                            # input_boxê°€ ìˆìœ¼ë©´ ê±°ê¸°ë¡œ, ì—†ìœ¼ë©´ í˜„ì¬ í¬ì»¤ìŠ¤ëœ ê³³ì—
                            actions = ActionChains(self.driver)
                            
                            if input_box:
                                actions.move_to_element(input_box)
                                actions.click()
                            
                            # ê¸°ì¡´ ë‚´ìš© ì§€ìš°ê¸° (Ctrl+A, Del)
                            actions.key_down(Keys.CONTROL).send_keys('a').key_up(Keys.CONTROL).pause(0.1).send_keys(Keys.DELETE).pause(0.2)
                            
                            # ë¶™ì—¬ë„£ê¸° (Ctrl+V)
                            actions.key_down(Keys.CONTROL).send_keys('v').key_up(Keys.CONTROL).pause(0.5)
                            actions.perform()
                            
                        except Exception as e:
                            self.log_signal.emit(f"âš ï¸ ì…ë ¥ ì‹¤íŒ¨: {e}")
                            # ìµœí›„ì˜ ìˆ˜ë‹¨: JS ê°’ ì£¼ì…
                            if input_box:
                                self.driver.execute_script("arguments[0].innerText = arguments[1];", input_box, p_text)

                        time.sleep(1)
                        
                        # ì—”í„° ì…ë ¥ (ìƒì„± ì‹œì‘)
                        ActionChains(self.driver).send_keys(Keys.RETURN).perform()
                        time.sleep(1)
                        
                        # ëª…ì‹œì ìœ¼ë¡œ 'ë§Œë“¤ê¸°' ë²„íŠ¼ ì°¾ì•„ì„œ í´ë¦­
                        try:
                            script_submit = """
                            var buttons = Array.from(document.querySelectorAll('button'));
                            var target = buttons.find(b => {
                                var txt = (b.innerText || b.getAttribute('aria-label') || '').toLowerCase();
                                return txt.includes('create') || txt.includes('generate') || txt.includes('ë§Œë“¤ê¸°') || txt.includes('run');
                            });
                            
                            if (!target) {
                                // ì•„ì´ì½˜ fallback
                                var icons = document.querySelectorAll('.material-symbols-outlined, .material-icons, svg');
                                for(var icon of icons) {
                                    var itxt = (icon.innerText || '').toLowerCase();
                                    if(itxt.includes('send') || itxt.includes('arrow_up') || itxt.includes('spark')) {
                                        target = icon.closest('button');
                                        break;
                                    }
                                }
                            }

                            if (target) {
                                target.click();
                                return true;
                            }
                            return false;
                            """
                            driver_res = self.driver.execute_script(script_submit)
                            
                            if not driver_res:
                                # ì—”í„° í•œë²ˆ ë”
                                ActionChains(self.driver).send_keys(Keys.ENTER).perform()
                        except:
                            pass
                        
                        tab_status[tab] = {"item": current_item, "start_time": time.time()}
                        item_idx += 1
                        self.progress.emit(f"ì§„í–‰: {processed_count}/{total}")

                    elif tab_status[tab] is not None:
                        target_num = tab_status[tab]["item"][0]
                        img_data = self.check_image_once(self.driver, tab_old_srcs[tab])
                        
                        if img_data:
                            save_path = os.path.join(self.target_dir, f"{target_num}.png")
                            with open(save_path, "wb") as f:
                                f.write(base64.b64decode(img_data))
                            self.log_signal.emit(f"  âœ… [íƒ­ {tabs.index(tab)+1}] {target_num}ë²ˆ ì €ì¥ ì™„ë£Œ")
                            tab_status[tab] = None
                            processed_count += 1
                        
                        elif time.time() - tab_status[tab]["start_time"] > 250: # ImageFXëŠ” ì¡°ê¸ˆ ë” ëŠë¦´ ìˆ˜ ìˆìŒ
                            self.log_signal.emit(f"  âŒ [íƒ­ {tabs.index(tab)+1}] {target_num}ë²ˆ íƒ€ì„ì•„ì›ƒ")
                            failed_items.append(tab_status[tab]["item"])
                            tab_status[tab] = None
                            processed_count += 1
                
                time.sleep(1)

            if not self.is_running:
                 self.log_signal.emit("ğŸ›‘ ImageFX ì‘ì—…ì´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                 
            elapsed_time = time.time() - start_timestamp
            result_msg = f"ì™„ë£Œ (ì„±ê³µ {total - len(failed_items)} / ì‹¤íŒ¨ {len(failed_items)})" if self.is_running else "ì¤‘ì§€ë¨"
            self.finished.emit(result_msg, elapsed_time)

        except Exception as e:
            self.error.emit(str(e))

class VideoMergerWorker(QThread):
    progress = pyqtSignal(str)
    log_signal = pyqtSignal(str)
    finished = pyqtSignal(str, float)
    error = pyqtSignal(str)

    def __init__(self, image_dir, audio_dir, output_dir, subtitles=None, style=None, volume=1.0, trim_end=0.0, use_random_effects=False):
        super().__init__()
        self.image_dir = image_dir
        self.audio_dir = audio_dir
        self.output_dir = output_dir
        self.subtitles = subtitles
        self.style = style
        self.volume = volume
        self.trim_end = trim_end
        self.use_random_effects = use_random_effects
        os.makedirs(self.output_dir, exist_ok=True)

    def run(self):
        start_time = time.time()
        try:
            # ì˜¤ë””ì˜¤ íŒŒì¼ ë¦¬ìŠ¤íŠ¸ (.mp3)
            if not os.path.exists(self.audio_dir):
                self.error.emit("âŒ ì˜¤ë””ì˜¤ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return

            audio_files = [f for f in os.listdir(self.audio_dir) if f.lower().endswith('.mp3')]
            
            # ìì—°ìŠ¤ëŸ¬ìš´ ì •ë ¬ (1.mp3, 2.mp3, ... 10.mp3)
            def natural_keys(text):
                return [int(c) if c.isdigit() else c for c in re.split(r'(\d+)', text)]
            audio_files.sort(key=natural_keys)

            total = len(audio_files)
            if total == 0:
                self.error.emit("âŒ ì˜¤ë””ì˜¤ í´ë”ì— mp3 íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return

            # ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì‘ì—… ë¦¬ìŠ¤íŠ¸ ìƒì„±
            tasks = []
            valid_exts = ['.png', '.jpg', '.jpeg', '.webp']
            
            for i, audio_name in enumerate(audio_files):
                base_name = os.path.splitext(audio_name)[0]
                audio_path = os.path.join(self.audio_dir, audio_name)
                
                # ëŒ€ì‘í•˜ëŠ” ì´ë¯¸ì§€ ì°¾ê¸°
                img_path = None
                found_img_name = None
                
                # 1. ê°™ì€ ì´ë¦„ì˜ ì´ë¯¸ì§€ ê²€ìƒ‰
                for ext in valid_exts:
                    check_path = os.path.join(self.image_dir, base_name + ext)
                    if os.path.exists(check_path):
                        img_path = check_path
                        found_img_name = base_name + ext
                        break
                
                if not img_path:
                    self.log_signal.emit(f"âš ï¸ ì´ë¯¸ì§€ ì—†ìŒ ìŠ¤í‚µ: {base_name} (ì˜¤ë””ì˜¤ ê¸°ì¤€ ì²˜ë¦¬ ì¤‘)")
                    continue
                
                output_path = os.path.join(self.output_dir, base_name + ".mp4")
                
                # ëœë¤ íš¨ê³¼ ì„¤ì • ìƒì„±
                item_effect = None
                if self.use_random_effects:
                    import random
                    # íš¨ê³¼: 1(Zoom In), 2(Pan L-R), 3(Pan R-L)
                    # Zoom Out ì€ Zoom In ê³¼ ë°˜ëŒ€ì¸ë°, start/endë¥¼ ë’¤ì§‘ìœ¼ë©´ ë¨.
                    # í•˜ì§€ë§Œ í˜„ì¬ ì½”ë“œ ìƒ Type 1ì€ start->end.
                    # ì‚¬ìš©ì ìš”ì²­: Zoom In, Out, L->R, R->L
                    # Type 1: Zoom (Generic) -> we can randomize start/end scale
                    # Type 2: Pan L->R
                    # Type 3: Pan R->L
                    
                    eff_type = random.choice([1, 1, 2, 3]) # Zoom ë¹„ì¤‘ì„ ì¡°ê¸ˆ ë†’ì„? ì•„ë‹ˆë©´ ê· ë“±í•˜ê²Œ 1,2,3
                    # Zoom In/Out case for Type 1
                    s_scale = 1.0
                    e_scale = 1.1
                    
                    if eff_type == 1:
                        # 50% í™•ë¥ ë¡œ Zoom In or Zoom Out
                        if random.random() > 0.5:
                            # Zoom In
                            s_scale = 1.0
                            e_scale = 1.15
                        else:
                            # Zoom Out
                            s_scale = 1.15
                            e_scale = 1.0
                    
                    item_effect = {
                        'type': eff_type,
                        'start_scale': s_scale,
                        'end_scale': e_scale,
                        'pan_speed': 1.0
                    }
                
                tasks.append((img_path, audio_path, output_path, base_name, item_effect))

            self.log_signal.emit(f"ğŸš€ ì´ {len(tasks)}ê°œì˜ ì˜ìƒ í•©ì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤. (ë³‘ë ¬ ì²˜ë¦¬ ëª¨ë“œ)")
            
            # ThreadPoolExecutorë¥¼ ì‚¬ìš©í•˜ì—¬ ë³‘ë ¬ ì‘ì—… ìˆ˜í–‰
            max_workers = min(3, multiprocessing.cpu_count()) # ì‹œìŠ¤í…œ ë¶€ë‹´ì„ ê³ ë ¤í•´ ìµœëŒ€ 3ê°œë¡œ ì œí•œ
            success_count = 0
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_task = {executor.submit(self.process_single_video, task): task for task in tasks}
                for future in concurrent.futures.as_completed(future_to_task):
                    task_base_name = future_to_task[future][3]
                    try:
                        result = future.result()
                        if result:
                            success_count += 1
                            self.log_signal.emit(f"âœ… ì™„ë£Œ: {task_base_name}.mp4")
                        else:
                            self.log_signal.emit(f"âŒ ì‹¤íŒ¨: {task_base_name}.mp4")
                    except Exception as e:
                        self.log_signal.emit(f"âŒ ì˜¤ë¥˜ ë°œìƒ ({task_base_name}): {e}")

            elapsed = time.time() - start_time
            result_msg = f"ì˜ìƒ í•©ì„± ì™„ë£Œ (ì„±ê³µ {success_count} / ì´ {total})"
            self.finished.emit(result_msg, elapsed)

        except Exception as e:
            self.error.emit(f"ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")

    def process_single_video(self, task):
        img_path, audio_path, output_path, base_name, task_effect_config = task
        
        # ì„ì‹œ íŒŒì¼ ê²½ë¡œë“¤ (ì •ë¦¬ìš©)
        temp_files = []
        
        try:
            # 0. FFmpeg ë°”ì´ë„ˆë¦¬ í™•ë³´
            try:
                import imageio_ffmpeg
                ffmpeg_exe = imageio_ffmpeg.get_ffmpeg_exe()
            except ImportError:
                ffmpeg_exe = "ffmpeg"

            # 1. ì˜¤ë””ì˜¤ ì •ë³´ í™•ì¸ (MoviePyë¡œ ë©”íƒ€ë°ì´í„°ë§Œ ë¹ ë¥´ê²Œ ì½ê¸°)
            #    (ffmpeg probeë¥¼ subprocessë¡œ ë„ìš°ëŠ” ê²ƒë³´ë‹¤ ë¡œë“œë˜ì–´ìˆëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš©ì´ ê°„í¸)
            try:
                import soundfile as sf
                f = sf.SoundFile(audio_path)
                original_duration = len(f) / f.samplerate
                f.close()
            except:
                # Fallback
                audio_clip = mpe.AudioFileClip(audio_path)
                original_duration = audio_clip.duration
                audio_clip.close()

            # 2. ì˜¤ë””ì˜¤ ì˜µì…˜ ê³„ì‚°
            # - Trim End
            final_duration = original_duration
            if self.trim_end > 0:
                final_duration = max(0.1, final_duration - self.trim_end)
            
            # - Volume, Fadeout (Filterë¡œ ì²˜ë¦¬)
            # volume=1.0 (ê¸°ë³¸), afade=t=out:st=duration-0.05:d=0.05
            
            # 3. ìë§‰ ì²˜ë¦¬ (ê¸°ì¡´ create_text_image í™œìš© -> PNG ì €ì¥)
            meta_path = audio_path.replace(".mp3", ".json")
            sub_timing_list = [] 
            
            sub_list = None
            if self.subtitles and base_name in self.subtitles:
                sub_list = self.subtitles[base_name]

            if os.path.exists(meta_path):
                sub_timing_list = self.get_timing_from_metadata(meta_path, sub_list)
                if sub_timing_list:
                    mode_info = "ì…ë ¥ì°½ ê¸°ì¤€" if sub_list else "JSON ì €ì¥ ë°ì´í„°"
                    self.log_signal.emit(f"   â„¹ï¸ [ì •ë°€] {base_name}: {len(sub_timing_list)}ê°œ ìë§‰ êµ¬ê°„ {mode_info} ì‹±í¬ ì ìš©")
            
            if not sub_timing_list and sub_list:
                num_subs = len(sub_list)
                sub_duration = max(0.5, final_duration / num_subs)
                for idx, text in enumerate(sub_list):
                    if isinstance(text, dict): text = text.get("original", "")
                    start_t = idx * sub_duration
                    actual_dur = sub_duration if idx < num_subs - 1 else (final_duration - start_t)
                    sub_timing_list.append((start_t, start_t + actual_dur, text))

            subtitle_inputs = [] # (path, start_t, end_t)
            
            # ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆ í™•ì¸ (ìë§‰ ìƒì„±ì„ ìœ„í•´)
            # [Fix] ìë§‰ì€ ìµœì¢… ì˜ìƒ í•´ìƒë„(1920x1080) ê¸°ì¤€ìœ¼ë¡œ ìƒì„±í•´ì•¼ ì˜¤ë²„ë ˆì´ ì¢Œí‘œê°€ ë§ìŒ
            TARGET_W, TARGET_H = 1920, 1080
            w, h = TARGET_W, TARGET_H
            # img = Image.open(img_path)
            # w, h = img.size
            # if w % 2 != 0: w -= 1
            # if h % 2 != 0: h -= 1
            
            # ìë§‰ PNG ìƒì„±
            if sub_timing_list:
                temp_dir = os.path.join(os.path.dirname(output_path), "temp_subs")
                os.makedirs(temp_dir, exist_ok=True)
                
                for idx, (start_t, end_t, text) in enumerate(sub_timing_list):
                    # í‘œì‹œ ì‹œê°„ì´ ì˜ìƒ ê¸¸ì´ë³´ë‹¤ ê¸¸ë©´ ë¬´ì‹œ
                    if start_t >= final_duration: continue
                    real_end = min(end_t, final_duration)
                    
                    # [Fix] íƒ€ì„ìŠ¤íƒ¬í”„ ë°ì´í„° ì˜¤ë¥˜ë¡œ ê¸¸ì´ê°€ 0ì¸ ê²½ìš° ê°•ì œ ë³´ì •
                    if real_end <= start_t:
                        real_end = min(start_t + 3.0, final_duration)
                        
                    if real_end <= start_t: continue
                    
                    # [Gap Filling Logic]
                    # ë§Œì•½ ë‹¤ìŒ ìë§‰ê³¼ ë§¤ìš° ê°€ê¹Œìš°ë©´(ì˜ˆ: 0.5ì´ˆ ì´ë‚´), í˜„ì¬ ìë§‰ì„ ëŠ˜ë ¤ì„œ ë°°ê²½ ê¹œë¹¡ì„ ë°©ì§€
                    # ë‹¨, ë§ˆì§€ë§‰ ìë§‰ì€ ì œì™¸
                    if idx < len(sub_timing_list) - 1:
                        next_start = sub_timing_list[idx+1][0]
                        # ê°„ê²©ì´ ì‘ìœ¼ë©´ í˜„ì¬ ìë§‰ì˜ ëì„ ë‹¤ìŒ ìë§‰ ì‹œì‘ê¹Œì§€ ì—°ì¥
                        if 0 < (next_start - real_end) < 0.5:
                            real_end = next_start

                    # í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± (numpy array)
                    rgba_arr = self.create_text_image(text, (w, h))
                    
                    # PNGë¡œ ì €ì¥
                    sub_filename = f"sub_{base_name}_{idx}.png"
                    sub_path = os.path.join(temp_dir, sub_filename)
                    
                    # numpy -> Image -> save
                    start_t_str = f"{start_t:.3f}"
                    end_t_str = f"{real_end:.3f}"
                    
                    result_img = Image.fromarray(rgba_arr, 'RGBA')
                    result_img.save(sub_path)
                    
                    temp_files.append(sub_path)
                    subtitle_inputs.append((sub_path, start_t, real_end))

            # 4. FFmpeg ëª…ë ¹ì–´ êµ¬ì„±
            command = [ffmpeg_exe]
            
            # [Input 0] ë°°ê²½ ì´ë¯¸ì§€ (Loop)
            command.extend(["-loop", "1", "-t", f"{final_duration:.6f}", "-i", img_path])
            
            # [Input 1] ì˜¤ë””ì˜¤
            command.extend(["-i", audio_path])
            
            # [Input 2~N] ìë§‰ PNGë“¤
            for s_path, _, _ in subtitle_inputs:
                command.extend(["-i", s_path])
                
            filter_complex = ""
            
            # ========== Video Filter ==========
            # ========== Video Filter ==========
            # ì „ì²˜ë¦¬: Image Input [0:v] -> Scale/Padded to 1920x1080 (or 1280x720)
            # ì‚¬ìš©ì ìš”ì²­ì— ë”°ë¼ "ìœ íŠœë¸Œ ì˜ìƒ ì œì‘ í•´ìƒë„" -> FHD (1920x1080) ê¶Œì¥
            # TARGET_W, TARGET_H = 1920, 1080 (Moved up)
            FPS = 30
            
            # 1. Base Image Processing (Scale & Pad)
            # ì›ë³¸ ì´ë¯¸ì§€ë¥¼ íƒ€ê²Ÿ í•´ìƒë„ ë¹„ìœ¨ì— ë§ê²Œ ì¡°ì • (Fit)
            
            # Effect Config í™•ì¸
            # 1. Task ë³„ ê°œë³„ ì„¤ì • (ëœë¤ íš¨ê³¼ ë“±) ìš°ì„ 
            # 2. í´ë˜ìŠ¤ ì†ì„± (Single Video ë“±) ì°¨ì„ 
            effect_config = task_effect_config if task_effect_config else getattr(self, 'effect_config', None)
            effect_type = effect_config.get('type', 0) if effect_config else 0
            
            # Debugging Effect Config
            if effect_config:
                self.log_signal.emit(f"   [Debug] Effect Type: {effect_type}")
                self.log_signal.emit(f"   [Debug] Config: {effect_config}")
            else:
                pass # self.log_signal.emit("   [Debug] No effect config found.")
            
            zoom_expr = ""
            # Zoom/Pan Logic (Java Reference Style)
            # zoompan filter needs input to be sufficiently large or handled carefully.
            # Java: scale=3840:2160 -> zoompan -> scale=1280:720
            # We will use explicit logic:
            
            # A) ì´ë¯¸ì§€ [0:v]ë¥¼ ê³ í™”ì§ˆë¡œ ë»¥íŠ€ê¸° (Zoom ëŒ€ë¹„, Lanczos)
            #    ìµœëŒ€ ì¤Œ(ì˜ˆ: 1.5ë°°) ê³ ë ¤í•˜ì—¬ ë„‰ë„‰í•˜ê²Œ 1.5ë°° or 4Kë¡œ ì—…ìŠ¤ì¼€ì¼
            #    [Fix] fps=30 ëª…ì‹œí•˜ì—¬ zoompanì˜ d=1 ì„¤ì •ê³¼ í”„ë ˆì„ ìˆ˜ ë™ê¸°í™” (ê¸°ì¡´ 25fps -> 30fps ë¶ˆì¼ì¹˜ë¡œ ì‹œê°„ ë‹¨ì¶• ë¬¸ì œ í•´ê²°)
            filter_complex += f"[0:v]scale=3840:2160:flags=lanczos,setsar=1:1,fps={FPS}[v_high];"
            
            # B) Zoom/Pan Expression
            # Default (No Effect): z=1
            start_scale = effect_config.get('start_scale', 1.0) if effect_config else 1.0
            end_scale = effect_config.get('end_scale', 1.0) if effect_config else 1.0
            
            # duration (total frames)
            total_frames = int(final_duration * FPS)
            if total_frames <= 0: total_frames = 1
            
            if effect_type == 1: # Zoom (Unified)
                # Linear Interpolation: start + (end-start) * on/duration
                # [Fix] total_frames-1 ë¡œ ë‚˜ëˆ„ì–´ ë§ˆì§€ë§‰ í”„ë ˆì„ì—ì„œ ì •í™•íˆ end_scale ë„ë‹¬
                denom = total_frames - 1 if total_frames > 1 else 1
                z_expr = f"{start_scale}+({end_scale}-{start_scale})*on/{denom}"
                x_expr = "iw/2-(iw/2/zoom)"
                y_expr = "ih/2-(ih/2/zoom)"

            elif effect_type == 2: # Pan Left -> Right
                # Camera moves Left to Right -> Viewport moves Right to Left relative to image?
                # Usually "Pan Left to Right" means we see the left side first, then pan to the right side.
                # Left Side (x=0) -> Right Side (x=max)
                # [Correction] User says it's reversed. So current implementation (0->max) is what they think is "Right -> Left"?
                # Let's SWAP them.
                
                # New Logic for Type 2 (Left->Right label):
                # Start: x=max (Right side of image) -> End: x=0 (Left side of image)?
                # Wait, "Pan Left to Right" typically means "Move camera to right".
                # If camera moves right, the image frame moves left.
                # Let's simply SWAP the formulas as requested.
                
                pan_z = max(start_scale, 1.05)
                p_speed = effect_config.get('pan_speed', 1.0)
                z_expr = f"{pan_z}"
                
                # Swapped to (max -> 0)
                denom = total_frames - 1 if total_frames > 1 else 1
                progress_expr = f"(on*{p_speed}/{denom})"
                x_expr = f"(iw-iw/zoom)*(1-min(1,{progress_expr}))"
                y_expr = "ih/2-(ih/2/zoom)"
                
            elif effect_type == 3: # Pan Right -> Left
                # Swapped to (0 -> max)
                pan_z = max(start_scale, 1.05)
                p_speed = effect_config.get('pan_speed', 1.0)
                z_expr = f"{pan_z}"

                denom = total_frames - 1 if total_frames > 1 else 1
                progress_expr = f"(on*{p_speed}/{denom})"
                x_expr = f"(iw-iw/zoom)*min(1,{progress_expr})"
                y_expr = "ih/2-(ih/2/zoom)"
            else:
                z_expr = "1"
                x_expr = "0"
                y_expr = "0"
                
            # C) Apply Zoompan
            # [Fix] zoompanì€ ê¸°ë³¸ì ìœ¼ë¡œ ì…ë ¥ í”„ë ˆì„ í•˜ë‚˜ë‹¹ 1í”„ë ˆì„ì„ ì¶œë ¥í•˜ë ¤ í•¨.
            # í•˜ì§€ë§Œ ìš°ë¦¬ëŠ” ì´ë¯¸ì§€ë¥¼ loopì³ì„œ ì˜ìƒ ìŠ¤íŠ¸ë¦¼ìœ¼ë¡œ ë§Œë“¤ì—ˆìŒ (-loop 1 -t duration ...)
            # ë”°ë¼ì„œ ì…ë ¥ ìŠ¤íŠ¸ë¦¼ì€ ì´ë¯¸ total_frames ë§Œí¼ì˜ ê¸¸ì´ë¥¼ ê°€ì§.
            # ì´ ê²½ìš° d=1 (input duration 1 frame -> output 1 frame)ë¡œ ì„¤ì •í•˜ë©´ 1:1 ë§¤í•‘ë˜ì–´
            # on (output frame number)ì´ 0ë¶€í„° total_framesê¹Œì§€ ì¦ê°€í•˜ë©° ì• ë‹ˆë©”ì´ì…˜ì´ ì ìš©ë¨.
            
            # ë‹¨, ë§Œì•½ ì…ë ¥ì´ ë‹¨ì¼ ì´ë¯¸ì§€(1í”„ë ˆì„)ì˜€ë‹¤ë©´ d=total_frames ê°€ ë˜ì–´ì•¼ í•¨.
            # í˜„ì¬ ì½”ë“œëŠ” [0:v]ê°€ -loop 1 ë¡œ ë“¤ì–´ì˜¤ë¯€ë¡œ ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì„. -> d=1 ì´ ë§ìŒ.
            
            # [Check] z_exprì—ì„œ 'on' ë³€ìˆ˜ê°€ ì œëŒ€ë¡œ ì¦ê°€í•˜ëŠ”ì§€ í™•ì¸ í•„ìš”.
            # zoompan í•„í„°ì—ì„œ onì€ 'current input frame'ì´ ì•„ë‹ˆë¼ 'current output frame of the zoompan instance'ì„.
            # ì…ë ¥ì´ ë™ì˜ìƒ ìŠ¤íŠ¸ë¦¼ì¼ ë•Œ d=1ì´ë©´ onë„ ë§¤ í”„ë ˆì„ ì¦ê°€í•¨.
            
            # í˜¹ì‹œ ëª¨ë¥´ë‹ˆ s=WxHë¥¼ ëª…ì‹œí•˜ê³ , fpsë„ ëª…ì‹œ.
            filter_complex += (f"[v_high]zoompan=z='{z_expr}':x='{x_expr}':y='{y_expr}':"
                               f"d=1:s=3840x2160:fps={FPS}[v_zoomed];")
            
            # D) Downscale to Target (FHD) & Pad
            filter_complex += (f"[v_zoomed]scale={TARGET_W}:{TARGET_H}:force_original_aspect_ratio=decrease:flags=lanczos,"
                               f"pad={TARGET_W}:{TARGET_H}:(ow-iw)/2:(oh-ih)/2,setsar=1:1[v_bg];")
            
            # ========== Subtitle Filters ==========
            last_v_label = "[v_bg]"
            
            # Apply overlays
            # Input index for subs starts at 2
            for i, (_, start_t, end_t) in enumerate(subtitle_inputs):
                sub_idx = i + 2
                next_v_label = f"[v_sub{i}]"
                # enable='between(t, start, end)' -> Inclusive both sides -> Possible overlap flash
                # Use 'gte(t,start)*lt(t,end)' for exclusive end -> Seamless transition
                
                # Check if this is the last one or separate to ensure coverage
                # gte(t, S) * lt(t, E)
                filter_complex += f"{last_v_label}[{sub_idx}:v]overlay=enable='gte(t,{start_t:.3f})*lt(t,{end_t:.3f})'[v_sub{i}];"
                last_v_label = next_v_label
            
            final_v_label = last_v_label
            
            # ========== Audio Filter ==========
            # Volume + Trim + Resample(48k) + Fadeout
            # [1:a] -> ... -> [a_out]
            # atrim: duration ì œí•œ
            
            fade_duration = 0.05
            fade_start = max(0, final_duration - fade_duration)
            
            # vol filter -> aresample -> afade
            # vol: volume=1.5
            vol_val = self.volume
            
            filter_complex += (f"[1:a]volume={vol_val},"
                               f"atrim=duration={final_duration},"
                               f"aresample=48000:async=1,"
                               f"afade=t=out:st={fade_start}:d={fade_duration}[a_out]")
            
            # ========== Final Assembly ==========
            command.extend(["-filter_complex", filter_complex])
            command.extend(["-map", final_v_label, "-map", "[a_out]"])
            
            # Encoding Options
            command.extend(["-c:v", "libx264", "-preset", "medium", "-pix_fmt", "yuv420p"])
            command.extend(["-c:a", "aac", "-b:a", "192k"])
            command.extend(["-y", output_path])
            
            # log
            # self.log_signal.emit(f"   Command: {' '.join(command)}")
            print(f"[Debug] Filter Complex: {filter_complex}")
            if effect_config:
                print(f"[Debug] Effect Config: {effect_config}")
            
            # Run
            creation_flags = 0x08000000 if os.name == 'nt' else 0
            process = subprocess.Popen(
                command, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE, 
                universal_newlines=True, 
                encoding='utf-8',
                creationflags=creation_flags
            )
            
            try:
                out, err = process.communicate(timeout=final_duration*5 + 60) # íƒ€ì„ì•„ì›ƒ ë„‰ë„‰íˆ
                if process.returncode != 0:
                    raise Exception(f"FFmpeg Error: {err}")
            except subprocess.TimeoutExpired:
                process.kill()
                raise Exception("FFmpeg Timeout")

            # Cleanup Temp Files
            for tmp in temp_files:
                try: os.remove(tmp)
                except: pass
            
            # temp_subs í´ë” ì‚­ì œ
            try:
                temp_subs_dir = os.path.join(os.path.dirname(output_path), "temp_subs")
                if os.path.exists(temp_subs_dir):
                    os.rmdir(temp_subs_dir)
            except:
                pass
            
            return True

        except Exception as e:
            print(f"Error processing {base_name}: {e}")
            import traceback
            traceback.print_exc()
            # Cleanup on error
            for tmp in temp_files:
                try: os.remove(tmp)
                except: pass
            return False

    def get_timing_from_metadata(self, meta_path, sub_list=None):
        """JSON ë©”íƒ€ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„¸ê·¸ë¨¼íŠ¸ë“¤ê³¼ ì‹±í¬ ë§¤ì¹­
        sub_listê°€ ì—†ìœ¼ë©´ JSON ë‚´ì˜ sub_segments ì •ë³´ë¥¼ ì‚¬ìš©í•¨.
        """
        import json
        try:
            if not os.path.exists(meta_path):
                return []
                
            with open(meta_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            chars = data["characters"]
            starts = data["character_start_times_seconds"]
            ends = data["character_end_times_seconds"]
            
            # sub_listê°€ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìœ¼ë©´ JSONì— ì €ì¥ëœ sub_segments ì‚¬ìš©
            if sub_list is None:
                sub_list = data.get("sub_segments", [])
                
            if not sub_list:
                return []

            results = []
            current_char_idx = 0
            
            for item in sub_list:
                # item can be a string (old format) or dict (new format)
                if isinstance(item, dict):
                    original_text = item.get("original", "")
                    tts_text = item.get("tts", "")
                else:
                    original_text = item
                    tts_text = item

                # [Robust Match] ê³µë°± ë° íŠ¹ìˆ˜ë¬¸ì ì œê±° í›„ ë§¤ì¹­
                # (ElevenLabsê°€ ë§ˆì¹¨í‘œë¥¼ ìƒëµí•˜ê±°ë‚˜ ë‹¤ë¥´ê²Œ ì¤„ ìˆ˜ ìˆìŒ)
                text_clean = re.sub(r'[^\w]', '', tts_text)
                if not text_clean: continue
                
                seg_start_time = None
                seg_end_time = None
                
                temp_match = ""
                match_start_idx = -1
                
                search_idx = current_char_idx
                while search_idx < len(chars):
                    # ê³µë°±/íŠ¹ìˆ˜ë¬¸ì ì œì™¸ ë¬¸ì ë§¤ì¹­
                    c_char = chars[search_idx]
                    c_clean = re.sub(r'[^\w]', '', c_char)
                    
                    if c_clean:
                        if match_start_idx == -1: match_start_idx = search_idx
                        temp_match += chars[search_idx]
                    
                    # í˜„ì¬ ë¬¸ì¥ì´ ë§¤ì¹­ë˜ì—ˆëŠ”ì§€ í™•ì¸
                    if text_clean in temp_match:
                        seg_start_time = starts[match_start_idx]
                        
                        # [Safety Fix] end_times ë°°ì—´ì´ characters ë³´ë‹¤ ì§§ì€ ê²½ìš° ë°©ì–´
                        if search_idx < len(ends):
                            seg_end_time = ends[search_idx]
                        else:
                            # ë ì‹œê°„ì´ ì—†ìœ¼ë©´ ì‹œì‘ ì‹œê°„ê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬í•˜ê±°ë‚˜ ì„ì˜ê°’ ë¶€ì—¬
                            # ì—¬ê¸°ì„œëŠ” ì•ˆì „í•˜ê²Œ ë§ˆì§€ë§‰ ìœ íš¨ ì‹œê°„ ë˜ëŠ” ì‹œì‘ ì‹œê°„ ì‚¬ìš©
                            seg_end_time = starts[search_idx] if search_idx < len(starts) else seg_start_time

                        current_char_idx = search_idx + 1 # ë‹¤ìŒ ë¬¸ì¥ì€ ì—¬ê¸°ì„œë¶€í„° ê²€ìƒ‰
                        break
                    search_idx += 1
                
                if seg_start_time is not None:
                    # ê²°ê³¼ì—ëŠ” 'ì›ë³¸' í…ìŠ¤íŠ¸ë¥¼ ë‹´ì•„ ë¦¬í„´
                    results.append((seg_start_time, seg_end_time, original_text))
            
            return results
        except Exception as e:
            print(f"ë§¤ì¹­ ì˜¤ë¥˜ ({meta_path}): {e}")
            return []

    def create_text_image(self, text, size):
        # í°íŠ¸ ì´ë¯¸ì§€ ìºì‹±
        if not hasattr(self, '_text_cache'): self._text_cache = {}
        cache_key = f"{text}_{size}_{self.style['font_family']}_{self.style['font_size']}_{self.style['text_color']}_{self.style['outline_color']}_{self.style['bg_color']}"
        if cache_key in self._text_cache:
            return self._text_cache[cache_key]

        width, height = size
        image = QImage(width, height, QImage.Format_RGBA8888)
        image.fill(Qt.transparent)
        
        painter = QPainter(image)
        painter.setRenderHint(QPainter.Antialiasing)
        painter.setRenderHint(QPainter.TextAntialiasing)
        
        font_family = self.style['font_family']
        font_family = self.style['font_family']
        base_font_size = self.style['font_size']
        
        # í•´ìƒë„ ë°˜ì‘í˜• í°íŠ¸ í¬ê¸° ê³„ì‚° (ê¸°ì¤€: ìµœì†Œ ë³€ì˜ ê¸¸ì´ 1024px)
        # ì„¸ë¡œ ì˜ìƒ(Portrait)ê³¼ ê°€ë¡œ ì˜ìƒ(Landscape) ëª¨ë‘ì—ì„œ ì¼ê´€ëœ í…ìŠ¤íŠ¸ í¬ê¸°ë¥¼ ìœ ì§€í•˜ê¸° ìœ„í•´ width/height ì¤‘ ì‘ì€ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ í•¨
        min_dim = min(width, height)
        scale_factor = min_dim / 1024.0
        
        # ê¸€ì í¬ê¸°ê°€ ê³¼ë„í•˜ê²Œ ì»¤ì§€ëŠ” ê²ƒì„ ë°©ì§€ (User Feedback: Video Compositeì™€ Dubbing ê°„ ì°¨ì´ ë°œìƒ ì´ìœ  ì¶”ì •)
        # ë³´ìˆ˜ì ì¸ ìŠ¤ì¼€ì¼ë§ ì ìš© 
        
        scaled_font_size = int(base_font_size * scale_factor)
        
        font = QFont(font_family)
        font.setPixelSize(scaled_font_size)
        
        if not any(kw in font_family.lower() for kw in ['bold', 'heavy', 'black', 'eb', 'b']):
            font.setBold(True)
        else:
            font.setBold(False)
            
        painter.setFont(font)
        
        
        # ë¡œê·¸ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        try:
            print(f"[TextGen] Res: {width}x{height}, Scale: {scale_factor:.2f}, Font: {base_font_size} -> {scaled_font_size}px")
        except: pass
        
        # í…ìŠ¤íŠ¸ í¬ê¸° ê³„ì‚°
        # ì¢Œìš° ì—¬ë°±ë„ ìŠ¤ì¼€ì¼ë§
        margin_lr = int(40 * scale_factor)
        max_rect = QRect(margin_lr, 0, width - (margin_lr * 2), height) 
        text_rect = painter.boundingRect(max_rect, Qt.AlignCenter | Qt.TextWordWrap, text)
        
        # 1. ì „ì²´ ìœ„ì¹˜ í•˜ë‹¨ìœ¼ë¡œ ë” ë‚´ë¦¼ (7% -> 5% ì—¬ë°±)
        margin_bottom = int(height * 0.05)
        
        
        # 2. ë°°ê²½ ë°•ìŠ¤ í¬ê¸° ë° ìœ„ì¹˜ ê²°ì • (User Feedback: ìƒí•˜ ì¤„ì´ê³  ì¢Œìš° ë„‰ë„‰íˆ, ë‘¥ê·¼ ëª¨ì„œë¦¬)
        padding_h = int(40 * scale_factor) # ì¢Œìš° íŒ¨ë”© ë„‰ë„‰íˆ
        padding_v = int(12 * scale_factor) # ìƒí•˜ íŒ¨ë”© ì¶•ì†Œ (30 -> 12)
        
        bg_rect = text_rect.adjusted(-padding_h, -padding_v, padding_h, padding_v)
        
        # ë°°ê²½ ë°•ìŠ¤ê°€ í™”ë©´ ì•„ë˜ìª½ ì¤‘ì•™ì— ìœ„ì¹˜í•˜ë„ë¡ ì¡°ì •
        # í…ìŠ¤íŠ¸ ë°•ìŠ¤ì˜ ë†’ì´
        box_h = bg_rect.height()
        # ë°”ë‹¥ì—ì„œ margin_bottom ë§Œí¼ ë„ìš´ ìœ„ì¹˜
        target_bottom = height - margin_bottom
        target_top = target_bottom - box_h
        
        # ì´ë™ëŸ‰ ê³„ì‚° (í˜„ì¬ bg_rect.top()ì—ì„œ target_topìœ¼ë¡œ)
        dy = target_top - bg_rect.top()
        bg_rect.translate(0, dy)
        text_rect.translate(0, dy)

        # 1. ë°°ê²½ë°•ìŠ¤ (ì²´í¬ë°•ìŠ¤ê°€ ì¼œì ¸ ìˆì„ ë•Œë§Œ)
        if self.style.get('use_bg', True) and self.style['bg_color'] != "Transparent":
            # íˆ¬ëª…ë„ ì ìš©
            color = QColor(self.style['bg_color'])
            opacity = self.style.get('bg_opacity', 255)
            color.setAlpha(opacity)
            
            painter.setBrush(QBrush(color))
            painter.setPen(Qt.NoPen)
            # ë‘¥ê·¼ ëª¨ì„œë¦¬ ì ìš© (Rounded Rect) - ë°˜ì§€ë¦„ 15 ì •ë„
            radius = int(15 * scale_factor)
            painter.drawRoundedRect(bg_rect, radius, radius)

        # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° ìœ„ì¹˜ (6px ë‚´ë¦¼ ë³´ì •)
        text_draw_area = bg_rect.translated(0, 6)

        # í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° ìœ„ì¹˜ (ì„¼í„° ì •ë ¬ì„ ìœ„í•´ rect ì¡°ì • ë¶ˆí•„ìš”, text_rect ì‚¬ìš©)
        # í•˜ì§€ë§Œ ê¸°ì¡´ ë¡œì§ì—ì„œ bg_rect ê¸°ì¤€ ì •ë ¬ì„ í–ˆìœ¼ë¯€ë¡œ text_rect ìœ„ì¹˜ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë©´ ë¨
        
        # 3. í…ìŠ¤íŠ¸ (QPainterPathë¥¼ ì´ìš©í•œ ê³ í’ˆì§ˆ ì™¸ê³½ì„  + ì±„ìš°ê¸°)
        path = QPainterPath()
        
        # QPainterPathì— í…ìŠ¤íŠ¸ ì¶”ê°€
        # drawTextëŠ” rect ì•ˆì— ì•Œì•„ì„œ ì •ë ¬í•´ì„œ ê·¸ë¦¬ì§€ë§Œ, addTextëŠ” ê¸°ì¤€ì (baseline)ì´ í•„ìš”í•¨.
        # ë”°ë¼ì„œ drawTextì™€ ë™ì¼í•œ ë°°ì¹˜ë¥¼ ìœ„í•´ painterì˜ ë ˆì´ì•„ì›ƒ ë¡œì§ì„ í‰ë‚´ë‚´ê±°ë‚˜, 
        # ë‹¨ìˆœíˆ drawTextUnformattedê°€ ì•„ë‹Œ ì •ë ¬ ê¸°ëŠ¥ì„ ì¨ì•¼ í•˜ëŠ”ë° pathì—ëŠ” ê·¸ëŸ°ê²Œ ì—†ìŒ.
        # ê°€ì¥ ì‰¬ìš´ ë°©ë²•: QPainterPath.addTextëŠ” í•œ ì¤„ ì”© ì¢Œí‘œë¥¼ ì¡ì•„ì•¼ í•´ì„œ ë³µì¡í•¨.
        # ëŒ€ì•ˆ: QPainter.strokePath ì‚¬ìš© ë¶ˆê°€ (pathê°€ ì—†ìœ¼ë©´).
        
        # í•´ê²°ì±…: í…ìŠ¤íŠ¸ ë ˆì´ì•„ì›ƒì„ ìœ„í•´ strokeìš© pathë¥¼ ìƒì„±í•˜ëŠ” ì‰¬ìš´ ë°©ë²• -> QPainterPath.addText ëŒ€ì‹ 
        # ë‹¨ìˆœíˆ í…ìŠ¤íŠ¸ë¥¼ ê·¸ë¦¬ëŠ” ìœ„ì¹˜ë¥¼ ì •í™•íˆ ì¡ì•„ì„œ pathë¡œ ë³€í™˜í•´ì•¼ í•¨.
        # í•˜ì§€ë§Œ word-wrapì´ í¬í•¨ë˜ì–´ ìˆì–´ì„œ ì§ì ‘ êµ¬í˜„ì€ ê¹Œë‹¤ë¡œì›€.
        
        # -> Qtì˜ ê·¸ë¦¬ê¸° ìˆœì„œ ë³€ê²½:
        # 1. Stroke (ì™¸ê³½ì„ )
        # 2. Fill (ì±„ìš°ê¸°)
        # Strokeë¥¼ í•˜ë ¤ë©´ Pathê°€ í•„ìš”í•œë°, Word Wrappingëœ í…ìŠ¤íŠ¸ì˜ Pathë¥¼ ì–»ê¸°ëŠ” ì‰½ì§€ ì•ŠìŒ.
        
        # ì°¨ì„ ì±…: QPainter.drawTextë¡œ Stroke íš¨ê³¼ë¥¼ ë‚´ëŠ” StrokePath ë°©ì‹ ë§ê³ ,
        # ê·¸ëƒ¥ ê²¹ì³ ê·¸ë¦¬ê¸°ë¥¼ í•˜ë˜, loop ë°©ì‹(ë¸”ëŸ¬) ëŒ€ì‹  8ë°©í–¥+4ë°©í–¥ (ì´ 12~16íšŒ) ì •ë„ë§Œ í•˜ê±°ë‚˜
        # â˜… ì •ì„: QTextLayout ì‚¬ìš©.
        
        # ì´ë²ˆì—ëŠ” ë¹ ë¥´ê³  í™•ì‹¤í•œ ê°œì„ ì„ ìœ„í•´ "outline layer"ë¥¼ ë³„ë„ë¡œ ê·¸ë¦¬ì§€ ì•Šê³ 
        # Pathë¥¼ ìƒì„±í•´ì„œ Stroking í•˜ëŠ” ë°©ì‹ì„ ì‹œë„.
        # Word Wrappingì„ ì§€ì›í•˜ëŠ” drawTextì˜ Path ë²„ì „ì´ ì—†ìœ¼ë¯€ë¡œ,
        # ê°„ë‹¨íˆ text_rect ì•ˆì—ì„œ ì¤„ë°”ê¿ˆ ì²˜ë¦¬ë¥¼ ì§ì ‘ í•˜ê±°ë‚˜... ë„ˆë¬´ ë³µì¡.
        
        # ë‹¤ì‹œ ì‰¬ìš´ ê¸¸: "QPainterPath"ë¥¼ ì“°ë˜, í°íŠ¸ ìƒì„±ì‹œ setStyleStrategyë¡œ ì•„ì›ƒë¼ì¸? ì•„ë‹˜.
        
        # ê°€ì¥ í˜„ì‹¤ì ì¸ "ê¹”ë”í•œ ì•„ì›ƒë¼ì¸" ë°©ë²•:
        # path.addTextëŠ” ì¤„ë°”ê¿ˆì„ ì•ˆí•´ì¤Œ.
        # í…ìŠ¤íŠ¸ê°€ ê¸¸ì§€ ì•Šê±°ë‚˜, ìš°ë¦¬ê°€ ì¤„ë°”ê¿ˆì„ ì§ì ‘ 'split' í•´ì„œ ë„£ìœ¼ë©´ ë¨.
        # text_rectë¥¼ êµ¬í•  ë•Œ ì´ë¯¸ wrappingëœ ë†’ì´ë¥¼ êµ¬í–ˆìŒ -> í•˜ì§€ë§Œ ì–´ë””ì„œ ëŠê²¼ëŠ”ì§€ëŠ” ëª¨ë¦„.
        
        # Userê°€ "1ë²ˆ(Composite)ì²˜ëŸ¼ ë‚˜ì™€ì•¼ í•œë‹¤"ê³  í•¨.
        # Compositeì˜ ì½”ë“œê°€ ì´ loop ë°©ì‹ì´ë¼ë©´? -> Composite ì´ë¯¸ì§€ê°€ 1024pxì´ë¼ì„œ loop 10pxì´ í‹°ê°€ ëœ ë‚¬ì„ ìˆ˜ë„.
        # í•˜ì§€ë§Œ Dubbingì€ 1080p+ ë¼ì„œ í‹°ê°€ í™• ë‚¨.
        
        # ê°œì„ ëœ Loop ë°©ì‹ (miter limit ë¬¸ì œ í”¼í•˜ê¸° ìœ„í•´):
        # 10px ë‘ê»˜ë©´ loop range(-10, 11)ì€ ë„ˆë¬´ ë§ìŒ.
        # ë‘ê»˜ë¥¼ scaled_factorì— ë§ì¶¤.
        outline_width = int(6 * scale_factor) # ê¸°ë³¸ 6pxë¡œ ì¡°ì •í•˜ê³  ìŠ¤ì¼€ì¼ë§
        
        if self.style.get('use_outline', True) and self.style['outline_color'] and self.style['outline_color'].lower() != "none":
            # ì™¸ê³½ì„  ê·¸ë¦¬ê¸° (Circular Stroke Algorithm)
            # QTextDocumentë¥¼ ì“°ë©´ ì“°ë ˆë“œ ì¶©ëŒ(QPaintDevice Crash)ì´ë‚˜ NameError ë“± ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŒ.
            # ëŒ€ì‹  drawTextë¥¼ ì›í˜•ìœ¼ë¡œ ì—¬ëŸ¬ ë²ˆ ì°ì–´ì„œ ì™¸ê³½ì„ ì„ í‘œí˜„í•¨.
            # ê¸°ì¡´ì˜ "ì‚¬ê°í˜• ì±„ìš°ê¸° Loop"ëŠ” ìˆ˜ë°± ë²ˆ ê·¸ë ¤ì„œ íë ¤ì¡Œìœ¼ë‚˜, 
            # "ì›í˜• ë¼ì¸ Loop"ëŠ” íšŸìˆ˜ê°€ ì ê³ (16~32íšŒ) ê²½ê³„ê°€ ëª…í™•í•˜ì—¬ í›¨ì”¬ ì„ ëª…í•¨.

            painter.setPen(QColor(self.style['outline_color']))
            
            # ì™¸ê³½ì„  ë‘ê»˜
            outline_width = int(6 * scale_factor)
            if outline_width < 2: outline_width = 2
            
            # ê°ë„ ë‹¨ê³„ (ë‘ê»˜ì— ë”°ë¼ ìœ ë™ì ìœ¼ë¡œ ì¡°ì ˆí•˜ê±°ë‚˜ ê³ ì •)
            # 15ë„ ê°„ê²© = 24 steps -> ì¶©ë¶„íˆ ë¶€ë“œëŸ¬ì›€
            steps = 24 
            import math
            
            # 1. ì™¸ê³½ì„  ê·¸ë¦¬ê¸° (Main Stroke)
            for i in range(steps):
                angle = 2 * math.pi * i / steps
                dx = int(round(outline_width * math.cos(angle)))
                dy = int(round(outline_width * math.sin(angle)))
                painter.drawText(text_draw_area.translated(dx, dy), Qt.AlignCenter | Qt.TextWordWrap, text)
            
            # 2. ë‘ê»˜ê°€ ë‘êº¼ìš¸ ê²½ìš° ë‚´ë¶€ ë¹ˆí‹ˆ ë©”ìš°ê¸° (Inner Stroke)
            # ë‘ê»˜ê°€ 4px ì´ìƒì´ë©´ ì¤‘ê°„ì— í•˜ë‚˜ ë” ê·¸ë ¤ì¤Œ
            if outline_width > 3:
                inner_width = outline_width / 2.0
                for i in range(steps):
                    angle = 2 * math.pi * i / steps
                    dx = int(round(inner_width * math.cos(angle)))
                    dy = int(round(inner_width * math.sin(angle)))
                    painter.drawText(text_draw_area.translated(dx, dy), Qt.AlignCenter | Qt.TextWordWrap, text)

        # 3. í…ìŠ¤íŠ¸ ë³¸ë¬¸ (ë§¨ ìœ„ì— ë®ì–´ì“°ê¸°)
        painter.setPen(QColor(self.style['text_color']))
        painter.drawText(text_draw_area, Qt.AlignCenter | Qt.TextWordWrap, text)
        
        painter.end()
        
        # Numpy ë³€í™˜ ë° ìºì‹±
        ptr = image.bits()
        ptr.setsize(image.byteCount())
        import numpy as np
        arr = np.frombuffer(ptr, np.uint8).copy().reshape((height, width, 4))
        
        if len(self._text_cache) > 50:
            self._text_cache.clear()
        self._text_cache[cache_key] = arr
        return arr

class SingleVideoWorker(VideoMergerWorker):
    def __init__(self, img_path, audio_path, output_path, subtitles=None, style=None, volume=1.0, trim_end=0.0, effect_config=None):
        # ìƒìœ„ í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ë“¤ì„ ì´ˆê¸°í™”í•˜ê¸° ìœ„í•´ ë¶€ëª¨ ìƒì„±ì í˜¸ì¶œ (ë””ë ‰í† ë¦¬ëŠ” ë”ë¯¸ë¡œ ì „ë‹¬)
        super().__init__(os.path.dirname(img_path), os.path.dirname(audio_path), os.path.dirname(output_path), 
                         subtitles=None, style=style, volume=volume, trim_end=trim_end)
        self.single_img = img_path
        self.single_audio = audio_path
        self.single_output = output_path
        self.single_subtitles = subtitles # list of items
        self.effect_config = effect_config # Store effect config

    def run(self):
        start_time = time.time()
        try:
            base_name = os.path.splitext(os.path.basename(self.single_audio))[0]
            # ê°œë³„ ìë§‰ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¶€ëª¨ í´ë˜ìŠ¤ê°€ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ë§µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            if self.single_subtitles:
                self.subtitles = {base_name: self.single_subtitles}
            else:
                self.subtitles = None
            
            # SingleVideoWorkerì˜ ê²½ìš° task tupleì— effect_configë¥¼ Noneìœ¼ë¡œ ì¶”ê°€í•´ì•¼ í•¨ (ë¶€ëª¨ í´ë˜ìŠ¤ initì„ ë”°ëë‹¤ë©´) 
            # í•˜ì§€ë§Œ SingleVideoWorkerëŠ” ë¶€ëª¨ process_single_videoë¥¼ í˜¸ì¶œí•¨.
            # ë¶€ëª¨ê°€ task ì–¸íŒ¨í‚¹ì„ 5ê°œë¡œ ë°”ê¿¨ìœ¼ë¯€ë¡œ ë§ì¶°ì¤˜ì•¼ í•¨.
            
            # Single VideoëŠ” effect_configë¥¼ self.effect_configì— ì €ì¥í•´ë‘ .
            # taskì—ëŠ” Noneì„ ë„˜ê¸°ê³  process_single_video ë‚´ë¶€ì—ì„œ getattr(self) fallbackì„ ì´ìš©í•˜ë„ë¡ ìœ ë„.
            task = (self.single_img, self.single_audio, self.single_output, base_name, None)
            self.log_signal.emit(f"ğŸï¸ ê°œë³„ ì˜ìƒ ì œì‘ ì‹œì‘: {base_name}...")
            
            success = self.process_single_video(task)
            
            elapsed = time.time() - start_time
            if success:
                self.finished.emit(f"âœ… ì˜ìƒ ì œì‘ ì™„ë£Œ: {os.path.basename(self.single_output)}", elapsed)
            else:
                self.error.emit("âŒ ì˜ìƒ ì œì‘ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            self.error.emit(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

class VideoDubbingWorker(VideoMergerWorker):
    def __init__(self, video_path, audio_path, output_path, subtitles=None, style=None, volume=1.0):
        # ë¶€ëª¨ ìƒì„±ì í˜¸ì¶œ
        super().__init__(os.path.dirname(video_path) if video_path else "", 
                         os.path.dirname(audio_path) if audio_path else "", 
                         os.path.dirname(output_path) if output_path else "", 
                         subtitles=None, style=style, volume=volume)
        self.video_path = video_path
        self.audio_path = audio_path
        self.output_path = output_path
        self.subtitle_data = subtitles # list of strings (manual input) or None
        
    def run(self):
        start_time = time.time()
        try:
            self.log_signal.emit(f"ğŸ¬ ë™ì˜ìƒ ë”ë¹™ ì‘ì—… ì‹œì‘: {os.path.basename(self.video_path)}...")
            self.log_signal.emit(f"   ì˜¤ë””ì˜¤: {os.path.basename(self.audio_path)}")
            
            # 0. FFmpeg ë°”ì´ë„ˆë¦¬ í™•ë³´
            try:
                import imageio_ffmpeg
                ffmpeg_exe = imageio_ffmpeg.get_ffmpeg_exe()
            except ImportError:
                try:
                    import moviepy.config
                    ffmpeg_exe = moviepy.config.get_setting("FFMPEG_BINARY")
                except:
                    ffmpeg_exe = "ffmpeg"
            
            # 1. ì˜¤ë””ì˜¤ ì •ë³´ í™•ì¸ (ê¸¸ì´)
            if not os.path.exists(self.audio_path):
                self.error.emit(f"âŒ ì˜¤ë””ì˜¤ íŒŒì¼ ì—†ìŒ: {self.audio_path}")
                return
            
            # soundfileë¡œ ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì • (ì •í™•ë„ í–¥ìƒ)
            try:
                import soundfile as sf
                f = sf.SoundFile(self.audio_path)
                audio_duration = len(f) / f.samplerate
                f.close()
            except ImportError:
                # Fallback to moviepy
                clip = mpe.AudioFileClip(self.audio_path)
                audio_duration = clip.duration
                clip.close()
                
            self.log_signal.emit(f"   ì˜¤ë””ì˜¤ ê¸¸ì´: {audio_duration:.2f}ì´ˆ")
            
            # 2. ë¹„ë””ì˜¤ ê¸¸ì´ í™•ì¸
            # ffprobe or moviepy used just for duration check
            # For simplicity, we can use mpe for metadata reading or ffprobe if implemented.
            # Let's use mpe for metadata safe read
            v_clip = mpe.VideoFileClip(self.video_path)
            video_duration = v_clip.duration
            v_clip.close()
            
            self.log_signal.emit(f"   ì›ë³¸ ë¹„ë””ì˜¤ ê¸¸ì´: {video_duration:.2f}ì´ˆ")
            
            # 3. ìë§‰ ì¤€ë¹„ (Generate PNGs)
            # VideoMergerWorkerì™€ ìœ ì‚¬í•œ ë¡œì§
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            # Case-insensitive replacement
            base, ext = os.path.splitext(self.audio_path)
            meta_path = base + ".json"
            self.log_signal.emit(f"   â„¹ï¸ ìë§‰ JSON ê²½ë¡œ í™•ì¸: {meta_path}")
            sub_timing_list = []
            
            if os.path.exists(meta_path):
                # self.subtitles (manual input) vs JSON
                # Priority: JSON if exists
                # But parent class get_timing_from_metadata logic handles sub_list as help
                # Manual input is stored in self.subtitle_data (list of strings)
                
                # SingleVideoWorker's logic for parsing manual subs:
                # In Dubbing, we usually rely on JSON or manual input mapped to list
                
                # Try to use parent's method if possible, but we need to adapt arguments
                # For now, let's just use the robust JSON loader here
                import json
                try:
                    with open(meta_path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                    # data is list of {"start":.., "end":.., "text":..}
                    # JSON êµ¬ì¡° í™•ì¸: ElevenLabsì˜ ê²½ìš° {"characters": [], "character_start_times_seconds": [], ...} í˜•íƒœì¼ ìˆ˜ ìˆìŒ
                    # ë˜ëŠ” ìš°ë¦¬ê°€ ì €ì¥í•œ {"saved_sub_segments": [...]} í˜•íƒœì¼ ìˆ˜ ìˆìŒ.
                    # TTSWorkerì—ì„œ ì €ì¥í•˜ëŠ” ë°©ì‹: 
                    # 1) alignment (characters, start_times, durations)
                    # 2) sub_segments (ìš°ë¦¬ê°€ ë§Œë“  ë¬¸ì¥ ë‹¨ìœ„: start, end, text) - ì´ê²ƒì´ ê°€ì¥ í™•ì‹¤í•¨.
                    
                    keys = list(data.keys()) if isinstance(data, dict) else "List"
                    self.log_signal.emit(f"   â„¹ï¸ JSON í‚¤ í™•ì¸: {keys}")

                    if "saved_sub_segments" in data:
                        # ìš°ë¦¬ê°€ ê°€ê³µí•´ë‘” ë¬¸ì¥ ë‹¨ìœ„ ë°ì´í„°
                        for item in data["saved_sub_segments"]:
                             s = float(item['start'])
                             e = float(item['end'])
                             t = item['text']
                             sub_timing_list.append((s, e, t))
                    elif "sub_segments" in data:
                        # Fallback for alternative key
                        # Case A: sub_segments has timing (dict with start/end or list)
                        # Case B: sub_segments has ONLY text, and timing is in 'characters' (User Case)
                        
                        has_timing_in_segments = True
                        temp_list = []
                        
                        # Check first item to decide
                        if data["sub_segments"]:
                            first = data["sub_segments"][0]
                            if isinstance(first, dict) and "start" not in first:
                                has_timing_in_segments = False
                        
                        if has_timing_in_segments:
                            for item in data["sub_segments"]:
                                 if isinstance(item, dict):
                                     s = float(item.get('start', 0))
                                     e = float(item.get('end', 0))
                                     t = item.get('text', "")
                                 elif isinstance(item, (list, tuple)) and len(item) >= 3:
                                     # Saved as [start, end, text]
                                     s = float(item[0])
                                     e = float(item[1])
                                     t = item[2]
                                 else:
                                     continue
                                 sub_timing_list.append((s, e, t))
                        else:
                            # Mapping 'sub_segments' text strings to 'characters' timing
                            if "characters" in data and "character_start_times_seconds" in data:
                                all_chars = data["characters"]
                                all_starts = data["character_start_times_seconds"]
                                all_ends = data["character_end_times_seconds"] if "character_end_times_seconds" in data else []
                                if not all_ends: # make rudimentary ends if missing
                                    all_ends = [s + 0.1 for s in all_starts]
                                
                                current_char_idx = 0
                                total_chars = len(all_chars)
                                
                                for item in data["sub_segments"]:
                                    text = item.get("original", "") or item.get("tts", "") or item.get("text", "")
                                    if not text: continue
                                    
                                    # Length of text to match
                                    seg_len = len(text)
                                    
                                    if current_char_idx + seg_len > total_chars:
                                        # Out of bounds? Try best effort or just break
                                        if current_char_idx < total_chars:
                                            # Partial match?
                                            seg_len = total_chars - current_char_idx
                                        else:
                                            break
                                            
                                    s = all_starts[current_char_idx]
                                    # End of this segment is the end of the last character
                                    e = all_ends[current_char_idx + seg_len - 1]
                                    
                                    sub_timing_list.append((s, e, text))
                                    current_char_idx += seg_len
                                    
                                self.log_signal.emit(f"   â„¹ï¸ ë¬¸ì ì •ë ¬ ë°ì´í„°ë¡œ ìë§‰ {len(sub_timing_list)}ê°œ ë§¤í•‘ ì„±ê³µ")
                            else:
                                self.log_signal.emit("   âš ï¸ sub_segmentsì— ì‹œê°„ ì •ë³´ê°€ ì—†ê³  characters ë°ì´í„°ë„ ì—†ìŠµë‹ˆë‹¤.")
                    elif "characters" in data and "character_start_times_seconds" in data:
                        # Raw Character Alignment Data -> Reconstruct sentences
                        # ElevenLabs returns character-level timestamps. We need to group them.
                        # Simple logic: Group characters until a pause > 0.5s or simple length limits?
                        # Or just use the full duration as one subtitle if it's short?
                        # Better: Use the raw text and split by punctuation, mapping times.
                        # Complexity High. Fallback: Create one single subtitle/segment for now?
                        # Or try to group by ~3-5 seconds blocks.
                        
                        chars = data["characters"]
                        starts = data["character_start_times_seconds"]
                        ends = data["character_end_times_seconds"] if "character_end_times_seconds" in data else starts[1:] + [starts[-1]+0.1]
                        
                        # Very simple grouping strategy:
                        # accumulate text until duration > 3s or pause > 0.5s
                        current_text = ""
                        current_start = starts[0] if starts else 0
                        last_end = 0
                        
                        for i, char in enumerate(chars):
                            t_start = starts[i]
                            t_end = ends[i]
                            
                            # If gap from last_end is big, start new segment (unless it's space)
                            if last_end > 0 and (t_start - last_end) > 0.5 and current_text.strip():
                                sub_timing_list.append((current_start, last_end, current_text.strip()))
                                current_text = ""
                                current_start = t_start
                            
                            current_text += char
                            last_end = t_end
                            
                            # If text gets too long (~50 chars), split at next space
                            if len(current_text) > 50 and char == ' ':
                                sub_timing_list.append((current_start, t_end, current_text.strip()))
                                current_text = ""
                                current_start = t_end

                        # Append remaining
                        if current_text.strip():
                            sub_timing_list.append((current_start, last_end, current_text.strip()))
                            
                        self.log_signal.emit(f"   â„¹ï¸ ë¬¸ì ë°ì´í„°ì—ì„œ ìë§‰ {len(sub_timing_list)}ê°œ ì¬êµ¬ì„±ë¨")
                    elif isinstance(data, list):
                         # í˜¹ì‹œ ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¼ë©´?
                         for item in data:
                             if isinstance(item, dict):
                                 s = float(item.get('start', 0))
                                 e = float(item.get('end', 0))
                                 t = item.get('text', "")
                                 sub_timing_list.append((s, e, t))
                    else:
                        self.log_signal.emit("   âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” JSON êµ¬ì¡°")

                    self.log_signal.emit(f"   â„¹ï¸ JSON ìë§‰ ë¡œë“œ ì„±ê³µ ({len(sub_timing_list)}ê°œ)")
                except Exception as e:
                    self.log_signal.emit(f"   âš ï¸ JSON ë¡œë“œ ì‹¤íŒ¨: {e}")
            
            # If JSON failed or empty, try manual/auto split?
            # Dubbing mode implies strict syncing, so usually JSON is key.
            # If no JSON, maybe manual subtitles spread evenly? 
            if not sub_timing_list and self.subtitle_data:
                # Spread available subtitles over audio duration
                count = len(self.subtitle_data)
                seg_len = audio_duration / count
                for i, txt in enumerate(self.subtitle_data):
                    s = i * seg_len
                    e = (i+1) * seg_len
                    sub_timing_list.append((s, e, txt))
            
            # Generate PNGs
            temp_files = []
            subtitle_inputs = [] # (path, start, end)
            TARGET_W, TARGET_H = 1920, 1080 # Dubbing outputs also standardized to FHD? Yes recommended.
            
            if sub_timing_list:
                temp_dir = os.path.join(os.path.dirname(self.output_path), "temp_subs_dub")
                os.makedirs(temp_dir, exist_ok=True)
                
                from PIL import Image
                for idx, (start_t, end_t, text) in enumerate(sub_timing_list):
                    if start_t >= audio_duration: continue
                    real_end = min(end_t, audio_duration)
                    
                    # [Fix] íƒ€ì„ìŠ¤íƒ¬í”„ ë°ì´í„° ì˜¤ë¥˜(3.04ë¡œ ê³ ì • ë“±)ë¡œ ê¸¸ì´ê°€ 0ì¸ ê²½ìš° ê°•ì œ ë³´ì •
                    if real_end <= start_t:
                        real_end = min(start_t + 3.0, audio_duration)
                        
                    if real_end <= start_t: continue

                    # [Gap Filling Logic]
                    if idx < len(sub_timing_list) - 1:
                        next_start = sub_timing_list[idx+1][0]
                        if 0 < (next_start - real_end) < 0.5:
                            real_end = next_start
                    else:
                        # ë§ˆì§€ë§‰ ìë§‰ì€ ëê¹Œì§€ ìœ ì§€
                        real_end = audio_duration
                    
                    # í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ ìƒì„± (numpy array)
                    rgba_arr = self.create_text_image(text, (TARGET_W, TARGET_H))
                    
                    # PNGë¡œ ì €ì¥
                    sub_filename = f"dub_sub_{idx}.png"
                    sub_path = os.path.join(temp_dir, sub_filename)
                    
                    result_img = Image.fromarray(rgba_arr, 'RGBA')
                    result_img.save(sub_path)
                    
                    temp_files.append(sub_path)
                    subtitle_inputs.append((sub_path, start_t, real_end))

                self.log_signal.emit(f"   ğŸ“ ìë§‰ ì´ë¯¸ì§€ {len(subtitle_inputs)}ì¥ ìƒì„± ì™„ë£Œ")
            else:
                self.log_signal.emit("   â„¹ï¸ ì ìš©í•  ìë§‰ì´ ì—†ìŠµë‹ˆë‹¤.")

            # 4. FFmpeg Command Construction
            command = [ffmpeg_exe]
            command.append("-y")
            
            # Input 0: Video (Infinite Loop for Background)
            # MUST be before -i
            command.extend(["-stream_loop", "-1"])
            command.extend(["-i", self.video_path]) # [0:v]
            
            # Input 1: Audio
            command.extend(["-i", self.audio_path]) # [1:a]
            
            # Input 2..N: Subtitles
            for s_path, _, _ in subtitle_inputs:
                command.extend(["-i", s_path])
                
            # Filter Complex
            filter_complex = ""
            
            # 1. Process Video [0:v]
            # scale, pad, fps, setsar
            # No trim here, we rely on -shortest
            filter_complex += f"[0:v]scale={TARGET_W}:{TARGET_H}:force_original_aspect_ratio=decrease,pad={TARGET_W}:{TARGET_H}:(ow-iw)/2:(oh-ih)/2,setsar=1:1,fps=30[v_bg];"
            
            # 2. Subtitle Overlays
            last_v = "[v_bg]"
            for i, (_, start_t, end_t) in enumerate(subtitle_inputs):
                sub_idx = i + 2
                next_v = f"[v_sub{i}]"
                # Check bounds
                filter_complex += f"{last_v}[{sub_idx}:v]overlay=enable='gte(t,{start_t:.3f})*lt(t,{end_t:.3f})'{next_v};"
                last_v = next_v
            
            # 3. Audio Processing
            # [1:a] -> Volume -> Resample
            vol_val = self.volume
            filter_complex += f"[1:a]volume={vol_val},aresample=48000:async=1[a_out]"
            
            command.extend(["-filter_complex", filter_complex])
            command.extend(["-map", last_v, "-map", "[a_out]"])
            
            # Output Options
            # Explicitly set duration to match audio based on measured duration
            command.extend(["-t", f"{audio_duration:.3f}"])
            
            command.extend(["-c:v", "libx264", "-preset", "medium", "-pix_fmt", "yuv420p"])
            command.extend(["-c:a", "aac", "-b:a", "192k"])
            # [Fix] Inputê³¼ Outputì´ ê°™ìœ¼ë©´ FFmpeg ì—ëŸ¬ ë°œìƒí•˜ë¯€ë¡œ ì„ì‹œ íŒŒì¼ ì‚¬ìš©
            # (ì‚¬ìš©ì í”¼ë“œë°±: ê¸€ì”¨ê°€ ì•ˆ ë‚˜ì˜¤ëŠ” ì´ìœ ëŠ” ì¸ì½”ë”© ìì²´ê°€ ì‹¤íŒ¨í–ˆê¸° ë•Œë¬¸ì„)
            temp_output = self.output_path + f".temp_{int(time.time())}.mp4"
            command.extend([temp_output])
            
            self.log_signal.emit(f"ğŸ’¾ ìµœì¢… ì¸ì½”ë”© ì‹œì‘ (Native FFmpeg)...")
            
            # Run
            creation_flags = 0x08000000 if os.name == 'nt' else 0
            process = subprocess.Popen(
                command, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE, 
                universal_newlines=True, 
                encoding='utf-8',
                creationflags=creation_flags
            )
            
            stdout, stderr = process.communicate()
            
            if process.returncode != 0:
                self.error.emit(f"âŒ FFmpeg ì˜¤ë¥˜: {stderr}")
                if os.path.exists(temp_output):
                    try: os.remove(temp_output)
                    except: pass
                return
            
            # ì„±ê³µ ì‹œ ì›ë³¸ êµì²´
            try:
                if os.path.exists(self.output_path):
                    os.remove(self.output_path)
                os.rename(temp_output, self.output_path)
                self.log_signal.emit(f"âœ… íŒŒì¼ ë®ì–´ì“°ê¸° ì™„ë£Œ: {os.path.basename(self.output_path)}")
            except Exception as e:
                self.error.emit(f"âŒ íŒŒì¼ êµì²´ ì‹¤íŒ¨: {e}")
                return
            
            # Clean up temp subs
            for path in temp_files:
                try: os.remove(path)
                except: pass
            try:
                 if temp_files: os.rmdir(os.path.dirname(temp_files[0]))
            except: pass

            elapsed = time.time() - start_time
            self.finished.emit(f"âœ… ì‘ì—… ì™„ë£Œ: {os.path.basename(self.output_path)}", elapsed)
            
        except Exception as e:
            self.error.emit(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()

# [ì°¸ê³ ] ê¸°ì¡´ ë°©ì‹(VideoConcatenatorWorkerOld)ì€ ê° íŒŒì¼ë§ˆë‹¤ scale, pad ë“± í•„í„° ë¬¸ìì—´ì´ ì•½ 300ìì”© ì¶”ê°€ë˜ì–´
# 130ê°œ íŒŒì¼ ê¸°ì¤€ ëª…ë ¹ì¤„ ê¸¸ì´ê°€ 40,000ìë¥¼ ì´ˆê³¼í•˜ê²Œ ë©ë‹ˆë‹¤. (Windows ì œí•œ 32,767ì)
# ì‚¬ìš©ìì˜ íŒŒì¼ ê²½ë¡œë§Œ í•©ì¹˜ë©´ 6,000ìì—¬ë„ í•„í„° ì˜µì…˜ ë•Œë¬¸ì— ì´ˆê³¼ë©ë‹ˆë‹¤.
# ë”°ë¼ì„œ Concat Demuxer ë°©ì‹ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ì´ ë¬¸ì œë¥¼ í•´ê²°í–ˆìŠµë‹ˆë‹¤.
class VideoConcatenatorWorkerOld(QThread):
    log_signal = pyqtSignal(str)
    finished = pyqtSignal(str, float)
    error = pyqtSignal(str)

    def __init__(self, video_dir, output_file, watermark_path=None):
        super().__init__()
        self.video_dir = video_dir
        self.output_file = output_file
        self.watermark_path = watermark_path

    def run(self):
        start_time = time.time()
        try:
            # 0. FFmpeg ë°”ì´ë„ˆë¦¬ í™•ë³´
            try:
                import imageio_ffmpeg
                ffmpeg_exe = imageio_ffmpeg.get_ffmpeg_exe()
            except ImportError:
                # Fallback: ì‹œìŠ¤í…œ PATHì— ìˆê¸¸ ê¸°ëŒ€í•˜ê±°ë‚˜, moviepy config í™•ì¸
                try:
                    import moviepy.config
                    ffmpeg_exe = moviepy.config.get_setting("FFMPEG_BINARY")
                except:
                    ffmpeg_exe = "ffmpeg"

            # 1. íŒŒì¼ ëª©ë¡ ë° ì •ë ¬
            files = [f for f in os.listdir(self.video_dir) if f.lower().endswith('.mp4')]
            
            def natural_sort_key(s):
                return [int(text) if text.isdigit() else text.lower()
                        for text in re.split(r'(\d+)', s)]
            
            files.sort(key=natural_sort_key)

            if not files:
                self.error.emit("âŒ í•©ì¹  MP4 íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return

            self.log_signal.emit(f"ğŸš€ ì´ {len(files)}ê°œì˜ ì˜ìƒ í•©ì¹˜ê¸°ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤ (Native FFmpeg)...")
            if self.watermark_path:
                self.log_signal.emit(f"   ğŸ–¼ï¸ ì›Œí„°ë§ˆí¬ ì ìš©: {os.path.basename(self.watermark_path)}")
            
            # 2. FFmpeg ëª…ë ¹ì–´ êµ¬ì„±
            command = [ffmpeg_exe]
            
            # Inputs
            # [0] ~ [N-1]: Video Files
            for f in files:
                path = os.path.join(self.video_dir, f).replace("\\", "/") # FFmpegëŠ” / ê²½ë¡œ ì„ í˜¸
                command.extend(["-i", path])
            
            # [N]: Watermark (if exists)
            watermark_idx = -1
            if self.watermark_path and os.path.exists(self.watermark_path):
                command.extend(["-i", self.watermark_path])
                watermark_idx = len(files)

            filter_complex = ""
            
            # Filter Construction
            # 1920x1080, 30fps, 48kHz (High Quality Standard)
            for i in range(len(files)):
                # Video Filter: Scale fit to 1920x1080, Pad if needed, SetSAR 1:1, FPS 30
                # force_original_aspect_ratio=decrease: ì›ë³¸ ë¹„ìœ¨ ìœ ì§€í•˜ë©° 1920x1080 ì•ˆì— ë§ì¶¤
                # pad: ì¤‘ì•™ ì •ë ¬í•˜ì—¬ ë‚˜ë¨¸ì§€ ê²€ì€ìƒ‰ ì±„ì›€
                filter_complex += (f"[{i}:v]scale=1920:1080:force_original_aspect_ratio=decrease,"
                                   f"pad=1920:1080:(ow-iw)/2:(oh-ih)/2,setsar=1:1,fps=30[v{i}];")
                
                # Audio Filter: Resample to 48000Hz (Java code used 44100, but we agreed on 48000 for HQ)
                # async=1: Timestamp correction
                filter_complex += f"[{i}:a]aresample=48000:async=1[a{i}];"
                
            # Concat Filter with Gaps
            # ì˜ìƒ ì‚¬ì´ 0.2ì´ˆ ì •ì§€ í™”ë©´(Freeze Frame) ë° ë¬´ìŒ(Silence) ì¶”ê°€ ì „ëµ
            
            gap_duration = 0.2
            concat_inputs = []
            
            for i in range(len(files)):
                v_source = f"[v{i}]"
                a_source = f"[a{i}]"
                
                if i < len(files) - 1:
                     # ì¤‘ê°„ ì˜ìƒë“¤: 0.2ì´ˆ Padding (tpad, apad)
                     # tpad: stop_mode=clone (ë§ˆì§€ë§‰ í”„ë ˆì„ ë³µì œ)
                     pad_v_label = f"[v{i}_pad]"
                     pad_a_label = f"[a{i}_pad]"
                     
                     filter_complex += (f"{v_source}tpad=stop_mode=clone:stop_duration={gap_duration}{pad_v_label};"
                                        f"{a_source}apad=pad_dur={gap_duration}{pad_a_label};")
                     
                     concat_inputs.append(pad_v_label)
                     concat_inputs.append(pad_a_label)
                else:
                     # ë§ˆì§€ë§‰ ì˜ìƒ: Padding ì—†ìŒ
                     concat_inputs.append(v_source)
                     concat_inputs.append(a_source)
            
            # Append input labels for concat
            for label in concat_inputs:
                filter_complex += label
            
            filter_complex += f"concat=n={len(files)}:v=1:a=1[v_concat][out_a];"
            
            # Watermark Overlay
            final_v_label = "[v_concat]"
            if watermark_idx != -1:
                # Scale watermark to width 100 (half of previous 200) -> [wm]
                # Overlay at 20:20
                filter_complex += f"[{watermark_idx}:v]scale=100:-1[wm];"
                filter_complex += f"[v_concat][wm]overlay=20:20[v_final]"
                final_v_label = "[v_final]"
            
            # Remove trailing semicolon if overlay not used (but we added ';' above safely?)
            # Actually scale/concat output labels are internal, map expects final label.
            # If no watermark, we map [v_concat]. If yes, [v_final]
            # Semicolons between filters are needed.
            
            # Clean up filter string logic slightly
            if filter_complex.endswith(";"): 
                filter_complex = filter_complex[:-1] # Remove last ; if any

            command.extend(["-filter_complex", filter_complex])
            command.extend(["-map", final_v_label, "-map", "[out_a]"])
            
            # Encoding Settings
            # Video: libx264, preset medium (balanced speed/compression), pixel format yuv420p (compatibility)
            # CRF 23 used by default (good quality). To match "High Quality" feeling, maybe use CRF 21 or rely on default.
            # Java used ultrafast (fast but big file). We use medium.
            command.extend(["-c:v", "libx264", "-preset", "medium", "-pix_fmt", "yuv420p"])
            
            # Audio: AAC, 192k (High Quality)
            command.extend(["-c:a", "aac", "-b:a", "192k"])
            
            # Overwrite output
            command.extend(["-y", self.output_file])
            
            self.log_signal.emit(f"   FFmpeg í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰ ì¤‘... (ì‹œê°„ì´ ì¡°ê¸ˆ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
            
            # 3. ì‹¤í–‰ (subprocess)
            # creationflags=0x08000000 (CREATE_NO_WINDOW) to hide console on Windows
            creation_flags = 0
            if os.name == 'nt':
                creation_flags = 0x08000000
                
            process = subprocess.Popen(
                command, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE, 
                universal_newlines=True, 
                encoding='utf-8',
                creationflags=creation_flags
            )
            
            # ëŒ€ê¸° ë° ê²°ê³¼ í™•ì¸
            stdout, stderr = process.communicate()
            
            if process.returncode != 0:
                self.error.emit(f"âŒ FFmpeg ì˜¤ë¥˜: {stderr}")
                return

            elapsed = time.time() - start_time
            self.finished.emit(f"âœ… ìµœì¢… ì˜ìƒ í•©ì¹˜ê¸° ì™„ë£Œ: {os.path.basename(self.output_file)} (Native)", elapsed)

        except Exception as e:
            self.error.emit(f"âŒ í•©ì¹˜ê¸° ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()

class AudioNormalWorker(QThread):
    log_signal = pyqtSignal(str)
    finished = pyqtSignal(str) # msg
    error = pyqtSignal(str)

    def __init__(self, input_dir, output_dir):
        super().__init__()
        self.input_dir = input_dir
        self.output_dir = output_dir

    def run(self):
        try:
            # 0. FFmpeg ì¤€ë¹„
            try:
                import imageio_ffmpeg
                ffmpeg_exe = imageio_ffmpeg.get_ffmpeg_exe()
            except ImportError:
                ffmpeg_exe = "ffmpeg"

            if not os.path.exists(self.input_dir):
                self.error.emit(f"âŒ ì…ë ¥ í´ë” ì—†ìŒ: {self.input_dir}")
                return
                
            if not os.path.exists(self.output_dir):
                os.makedirs(self.output_dir, exist_ok=True)

            files = [f for f in os.listdir(self.input_dir) if f.lower().endswith('.mp3')]
            if not files:
                self.error.emit("âŒ MP3 íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return

            total = len(files)
            self.log_signal.emit(f"ğŸ”Š ì˜¤ë””ì˜¤ í‰ì¤€í™”(Normalization) ì‹œì‘... ì´ {total}ê°œ")
            
            success_count = 0
            
            # Windows creation flags
            creation_flags = 0x08000000 if os.name == 'nt' else 0

            for i, filename in enumerate(files):
                in_path = os.path.join(self.input_dir, filename)
                out_path = os.path.join(self.output_dir, filename)
                
                self.log_signal.emit(f"[{i+1}/{total}] ì²˜ë¦¬ ì¤‘: {filename}")
                
                # loudnorm filter
                cmd = [
                    ffmpeg_exe, "-y", "-i", in_path,
                    "-filter:a", "loudnorm,aresample=48000",
                    "-c:a", "libmp3lame", "-q:a", "2",
                    out_path
                ]
                
                try:
                    subprocess.run(
                        cmd, 
                        stdout=subprocess.PIPE, 
                        stderr=subprocess.PIPE, 
                        check=True,
                        creationflags=creation_flags
                    )
                    success_count += 1
                except subprocess.CalledProcessError as e:
                    self.log_signal.emit(f"   âŒ ì‹¤íŒ¨: {e.stderr.decode('utf-8') if e.stderr else 'Unknown Error'}")
                except Exception as ex:
                    self.log_signal.emit(f"   âŒ ì˜¤ë¥˜: {ex}")

            self.finished.emit(f"ì‘ì—… ì™„ë£Œ (ì„±ê³µ {success_count}/{total})")

        except Exception as e:
            self.error.emit(f"ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")

class MainApp(QWidget):
    # Signals must be class variables
    log_signal = pyqtSignal(str)
    error_signal = pyqtSignal(str)
    enable_button_signal = pyqtSignal(bool)

    def __init__(self):
        super().__init__()
        self.driver = None
        self.start_time_gen = 0
        self.start_time_nano = 0
        self.start_time_fx = 0
        self.loaded_items = []
        self.current_file_path = ""
        self.initUI()
        self.ui_timer = QTimer()
        self.ui_timer.timeout.connect(self.update_timer_display)

    def initUI(self):
        self.setWindowTitle("YouTube Video Creator Master")
        self.setGeometry(200, 100, 900, 850)
        layout = QVBoxLayout()

        # ë©”ì¸ ë ˆì´ì•„ì›ƒì„ íƒ­ ìœ„ì ¯ìœ¼ë¡œ ë³€ê²½
        self.tabs = QTabWidget()
        self.tabs.setElideMode(Qt.ElideNone) # í…ìŠ¤íŠ¸ ì˜ë¦¼ ë°©ì§€
        self.tabs.setUsesScrollButtons(True) # íƒ­ì´ ë§ìœ¼ë©´ ìŠ¤í¬ë¡¤ ë²„íŠ¼ ì‚¬ìš©
        self.tabs.tabBar().setExpanding(False) # íƒ­ì´ ê°•ì œë¡œ ëŠ˜ì–´ë‚˜ì§€ ì•Šê³  ê¸€ì í¬ê¸°ì— ë§ê²Œ ì„¤ì •

        # íƒ­ ìŠ¤íƒ€ì¼ ê°œì„ 
        self.tabs.setStyleSheet("""
            QTabWidget::pane { border: 1px solid #444; top: -1px; }
            QTabBar::tab {
                background: #2b2b2b;
                color: #b1b1b1;
                border: 1px solid #444;
                padding: 8px 15px;      /* ì¢Œìš° íŒ¨ë”© ìœ ì§€ */
                font-size: 13px;
                font-family: 'Malgun Gothic';
                min-width: 110px;       /* í•µì‹¬: íƒ­ì˜ ìµœì†Œ ë„ˆë¹„ë¥¼ ì§€ì •í•˜ì—¬ ê¸€ì ì˜ë¦¼ ë°©ì§€ */
            }
            QTabBar::tab:selected {
                background: #444444;
                color: #ffffff;
                border-bottom-color: #444444;
            }
        """)
        
        layout.addWidget(self.tabs)

        # íƒ­ 1: GenSpark Image
        self.tab1 = QWidget()
        self.initTab1()
        self.tabs.addTab(self.tab1, "GenSpark Image")

        # íƒ­ 1-3: NanoBanana Image (Added next to GenSpark)
        self.tab_nano = QWidget()
        self.initTabNanoBanana()
        self.tabs.addTab(self.tab_nano, "NanoBanana Image")

        # íƒ­ 1-2: ImageFX Image
        self.tab_fx = QWidget()
        self.initTabImageFX()
        self.tabs.addTab(self.tab_fx, "ImageFX Image")

        # íƒ­ 2: ElevenLabs TTS
        self.tab2 = QWidget()
        self.initTab2()
        self.tabs.addTab(self.tab2, "ElevenLabs TTS")

        # íƒ­ 3: Video Composite
        self.tab3 = QWidget()
        self.initTab3()
        self.tabs.addTab(self.tab3, "Video Composite")

        # íƒ­ 4: Video Concat
        self.tab4 = QWidget()
        self.initTab4()
        self.tabs.addTab(self.tab4, "Video Concat")
        
        # íƒ­ 5: Single Video
        self.tab5 = QWidget()
        self.initTab5()
        self.tabs.addTab(self.tab5, "Video Effects")

        # íƒ­ 6: Video Dubbing
        self.tab6 = QWidget()
        self.initTab6()
        self.tabs.addTab(self.tab6, "Video Dubbing")

        # íƒ­ 6-2: Audio Normalization
        self.tab_audio_normal = QWidget()
        self.initTabAudioNormal()
        self.tabs.addTab(self.tab_audio_normal, "Audio Normal")

        # íƒ­ 7: YouTube Analysis
        self.tab7 = QWidget()
        self.initTab7()
        self.tabs.addTab(self.tab7, "YouTube ë¶„ì„")


        self.setLayout(layout)

    def initTab1(self):
        layout = QVBoxLayout()

        self.status_label = QLabel("1ë‹¨ê³„: ë¸Œë¼ìš°ì €ë¥¼ ë¨¼ì € ì¤€ë¹„í•´ ì£¼ì„¸ìš”.")
        self.status_label.setStyleSheet("font-size: 15px; font-weight: bold; color: #D4D4D4;")
        layout.addWidget(self.status_label)

        self.timer_label = QLabel("ì†Œìš” ì‹œê°„: 00:00:00")
        layout.addWidget(self.timer_label)

        # ì €ì¥ ê²½ë¡œ ì„¤ì •
        path_layout = QHBoxLayout()
        self.image_path_edit = QLineEdit(r"D:\youtube")
        self.image_path_edit.setStyleSheet("background-color: #2D2D2D; color: #D4D4D4; height: 25px;")
        btn_browse_image = QPushButton("ì°¾ì•„ë³´ê¸°")
        btn_browse_image.clicked.connect(self.browse_image_path)
        path_layout.addWidget(QLabel("ì €ì¥ í´ë”:"))
        path_layout.addWidget(self.image_path_edit)
        path_layout.addWidget(btn_browse_image)
        layout.addLayout(path_layout)

        # ë²„íŠ¼ë“¤
        self.btn_prepare = QPushButton("ğŸŒ 1. ë¸Œë¼ìš°ì € ë° íƒ­ ì¤€ë¹„ (ì„¤ì •ìš©)")
        self.btn_prepare.setStyleSheet("height: 50px; font-weight: bold; background-color: #673AB7; color: white; border-radius: 8px;")
        self.btn_prepare.clicked.connect(self.launch_browser_and_tabs)
        layout.addWidget(self.btn_prepare)

        # í…ìŠ¤íŠ¸ ì…ë ¥ì°½ ì¶”ê°€
        layout.addWidget(QLabel("ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ì…ë ¥:"))
        self.image_prompt_input = QTextEdit()
        self.image_prompt_input.setPlaceholderText("í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”.\n1. í”„ë¡¬í”„íŠ¸1\n2. í”„ë¡¬í”„íŠ¸2")
        self.image_prompt_input.setStyleSheet("background-color: #1E1E1E; color: #D4D4D4;")
        layout.addWidget(self.image_prompt_input)

        btn_h_layout = QHBoxLayout()
        self.btn_start = QPushButton("ğŸš€ 2. ì´ë¯¸ì§€ ìƒì„± ì‹œì‘")
        self.btn_start.setEnabled(True)
        self.btn_start.setStyleSheet("""
            QPushButton { height: 50px; font-weight: bold; background-color: #28a745; color: white; border-radius: 8px; }
            QPushButton:disabled { background-color: #6c757d; }
        """)
        self.btn_start.clicked.connect(self.start_automation)
        
        self.btn_stop = QPushButton("ğŸ›‘ ì¤‘ì§€")
        self.btn_stop.setEnabled(False)
        self.btn_stop.setStyleSheet("""
            QPushButton { height: 50px; font-weight: bold; background-color: #dc3545; color: white; border-radius: 8px; }
            QPushButton:disabled { background-color: #6c757d; }
        """)
        self.btn_stop.clicked.connect(self.stop_automation)

        btn_h_layout.addWidget(self.btn_start)
        btn_h_layout.addWidget(self.btn_stop)
        layout.addLayout(btn_h_layout)

        # ì••ì¶• ë²„íŠ¼ ì¶”ê°€
        self.btn_compress = QPushButton("ğŸ—œï¸ 3. ì´ë¯¸ì§€ ì••ì¶• (ìš©ëŸ‰ ì¤„ì´ê¸°)")
        self.btn_compress.setStyleSheet("height: 50px; font-weight: bold; background-color: #FF9800; color: white; border-radius: 8px; margin-top: 5px;")
        self.btn_compress.clicked.connect(self.compress_images)
        layout.addWidget(self.btn_compress)

        # ë¡œê·¸ ë””ìŠ¤í”Œë ˆì´ (í•˜ë‹¨ìœ¼ë¡œ ì´ë™)
        self.log_display = QTextEdit()
        self.log_display.setReadOnly(True)
        self.log_display.setStyleSheet("background-color: #1E1E1E; color: #D4D4D4; font-family: 'Consolas', 'Malgun Gothic';")
        self.log_display.setMaximumHeight(150) # ì¡°ê¸ˆ ë” ì—¬ìœ  ìˆê²Œ
        layout.addWidget(self.log_display)

        self.tab1.setLayout(layout)

    def initTabNanoBanana(self):
        layout = QVBoxLayout()

        self.nano_status_label = QLabel("1ë‹¨ê³„: NanoBanana ë¸Œë¼ìš°ì €ë¥¼ ë¨¼ì € ì¤€ë¹„í•´ ì£¼ì„¸ìš”.")
        self.nano_status_label.setStyleSheet("font-size: 15px; font-weight: bold; color: #D4D4D4;")
        layout.addWidget(self.nano_status_label)

        self.nano_timer_label = QLabel("ì†Œìš” ì‹œê°„: 00:00:00")
        layout.addWidget(self.nano_timer_label)

        # ì €ì¥ ê²½ë¡œ ì„¤ì •
        path_layout = QHBoxLayout()
        self.nano_image_path_edit = QLineEdit(r"D:\youtube")
        self.nano_image_path_edit.setStyleSheet("background-color: #2D2D2D; color: #D4D4D4; height: 25px;")
        btn_browse_image = QPushButton("ì°¾ì•„ë³´ê¸°")
        btn_browse_image.clicked.connect(lambda: self.browse_image_path_custom(self.nano_image_path_edit))
        path_layout.addWidget(QLabel("ì €ì¥ í´ë”:"))
        path_layout.addWidget(self.nano_image_path_edit)
        path_layout.addWidget(btn_browse_image)
        layout.addLayout(path_layout)

        # ë²„íŠ¼ë“¤
        self.btn_nano_prepare = QPushButton("ğŸŒ 1. NanoBanana ë¸Œë¼ìš°ì € ë° íƒ­ ì¤€ë¹„")
        self.btn_nano_prepare.setStyleSheet("height: 50px; font-weight: bold; background-color: #673AB7; color: white; border-radius: 8px;")
        self.btn_nano_prepare.clicked.connect(self.launch_browser_nanobanana)
        layout.addWidget(self.btn_nano_prepare)

        # í…ìŠ¤íŠ¸ ì…ë ¥ì°½ ì¶”ê°€
        layout.addWidget(QLabel("ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ì…ë ¥:"))
        self.nano_prompt_input = QTextEdit()
        self.nano_prompt_input.setPlaceholderText("í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ ì…ë ¥í•˜ì„¸ìš”.\n1. í”„ë¡¬í”„íŠ¸1\n2. í”„ë¡¬í”„íŠ¸2")
        self.nano_prompt_input.setStyleSheet("background-color: #1E1E1E; color: #D4D4D4;")
        layout.addWidget(self.nano_prompt_input)

        btn_h_layout = QHBoxLayout()
        self.btn_nano_start = QPushButton("ğŸš€ 2. NanoBanana ì´ë¯¸ì§€ ìƒì„± ì‹œì‘")
        self.btn_nano_start.setEnabled(True)
        self.btn_nano_start.setStyleSheet("""
            QPushButton { height: 50px; font-weight: bold; background-color: #28a745; color: white; border-radius: 8px; }
            QPushButton:disabled { background-color: #6c757d; }
        """)
        self.btn_nano_start.clicked.connect(self.start_automation_nanobanana)
        
        self.btn_nano_stop = QPushButton("ğŸ›‘ ì¤‘ì§€")
        self.btn_nano_stop.setEnabled(False)
        self.btn_nano_stop.setStyleSheet("""
            QPushButton { height: 50px; font-weight: bold; background-color: #dc3545; color: white; border-radius: 8px; }
            QPushButton:disabled { background-color: #6c757d; }
        """)
        self.btn_nano_stop.clicked.connect(self.stop_automation_nanobanana)

        btn_h_layout.addWidget(self.btn_nano_start)
        btn_h_layout.addWidget(self.btn_nano_stop)
        layout.addLayout(btn_h_layout)

        # ì••ì¶• ë²„íŠ¼ ì¶”ê°€
        self.btn_nano_compress = QPushButton("ğŸ—œï¸ 3. ì´ë¯¸ì§€ ì••ì¶• (ìš©ëŸ‰ ì¤„ì´ê¸°)")
        self.btn_nano_compress.setStyleSheet("height: 50px; font-weight: bold; background-color: #FF9800; color: white; border-radius: 8px; margin-top: 5px;")
        self.btn_nano_compress.clicked.connect(lambda: self.compress_images_custom(self.nano_image_path_edit, self.nano_log_display))
        layout.addWidget(self.btn_nano_compress)

        # ë¡œê·¸ ë””ìŠ¤í”Œë ˆì´
        self.nano_log_display = QTextEdit()
        self.nano_log_display.setReadOnly(True)
        self.nano_log_display.setStyleSheet("background-color: #1E1E1E; color: #D4D4D4; font-family: 'Consolas', 'Malgun Gothic';")
        self.nano_log_display.setMaximumHeight(150)
        layout.addWidget(self.nano_log_display)

        self.tab_nano.setLayout(layout)

    def initTabImageFX(self):
        layout = QVBoxLayout()

        self.fx_status_label = QLabel("1ë‹¨ê³„: ImageFX ë¸Œë¼ìš°ì €ë¥¼ ì¤€ë¹„í•´ ì£¼ì„¸ìš”.")
        self.fx_status_label.setStyleSheet("font-size: 15px; font-weight: bold; color: #D4D4D4;")
        layout.addWidget(self.fx_status_label)

        self.fx_timer_label = QLabel("ì†Œìš” ì‹œê°„: 00:00:00")
        layout.addWidget(self.fx_timer_label)

        # ì €ì¥ ê²½ë¡œ
        path_layout = QHBoxLayout()
        self.fx_image_path_edit = QLineEdit(r"D:\youtube")
        self.fx_image_path_edit.setStyleSheet("background-color: #2D2D2D; color: #D4D4D4; height: 25px;")
        btn_browse_fx = QPushButton("ì°¾ì•„ë³´ê¸°")
        btn_browse_fx.clicked.connect(lambda: self.browse_image_path_custom(self.fx_image_path_edit))
        path_layout.addWidget(QLabel("ì €ì¥ í´ë”:"))
        path_layout.addWidget(self.fx_image_path_edit)
        path_layout.addWidget(btn_browse_fx)
        layout.addLayout(path_layout)
        
        # ë¸Œë¼ìš°ì € ì¤€ë¹„ ë²„íŠ¼
        self.btn_fx_prepare = QPushButton("ğŸŒ 1. ImageFX ë¸Œë¼ìš°ì € ì¤€ë¹„")
        self.btn_fx_prepare.setStyleSheet("height: 50px; font-weight: bold; background-color: #673AB7; color: white; border-radius: 8px;")
        self.btn_fx_prepare.clicked.connect(self.launch_browser_imagefx)
        layout.addWidget(self.btn_fx_prepare)
        
        # í”„ë¡¬í”„íŠ¸ ì…ë ¥
        layout.addWidget(QLabel("ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ ì…ë ¥:"))
        self.fx_prompt_input = QTextEdit()
        self.fx_prompt_input.setPlaceholderText("í”„ë¡¬í”„íŠ¸ ì…ë ¥ (ì˜ˆ: 1. ê³ ì–‘ì´)")
        self.fx_prompt_input.setStyleSheet("background-color: #1E1E1E; color: #D4D4D4;")
        layout.addWidget(self.fx_prompt_input)
        
        # ì‹œì‘ ë²„íŠ¼
        btn_fx_h_layout = QHBoxLayout()
        self.btn_fx_start = QPushButton("ğŸš€ 2. ImageFX ìƒì„± ì‹œì‘")
        self.btn_fx_start.setStyleSheet("""
            QPushButton { height: 50px; font-weight: bold; background-color: #28a745; color: white; border-radius: 8px; }
            QPushButton:disabled { background-color: #6c757d; }
        """)
        self.btn_fx_start.clicked.connect(self.start_automation_imagefx)

        self.btn_fx_stop = QPushButton("ğŸ›‘ ì¤‘ì§€")
        self.btn_fx_stop.setEnabled(False)
        self.btn_fx_stop.setStyleSheet("""
            QPushButton { height: 50px; font-weight: bold; background-color: #dc3545; color: white; border-radius: 8px; }
            QPushButton:disabled { background-color: #6c757d; }
        """)
        self.btn_fx_stop.clicked.connect(self.stop_automation_imagefx)

        btn_fx_h_layout.addWidget(self.btn_fx_start)
        btn_fx_h_layout.addWidget(self.btn_fx_stop)
        layout.addLayout(btn_fx_h_layout)
        
        # ì••ì¶• ë²„íŠ¼
        self.btn_fx_compress = QPushButton("ğŸ—œï¸ 3. ì´ë¯¸ì§€ ì••ì¶•")
        self.btn_fx_compress.setStyleSheet("height: 50px; font-weight: bold; background-color: #FF9800; color: white; border-radius: 8px; margin-top: 5px;")
        self.btn_fx_compress.clicked.connect(lambda: self.compress_images_custom(self.fx_image_path_edit, self.fx_log_display))
        layout.addWidget(self.btn_fx_compress)
        
        # ë¡œê·¸ì°½
        self.fx_log_display = QTextEdit()
        self.fx_log_display.setReadOnly(True)
        self.fx_log_display.setStyleSheet("background-color: #1E1E1E; color: #D4D4D4; font-family: 'Consolas', 'Malgun Gothic';")
        self.fx_log_display.setMaximumHeight(150)
        layout.addWidget(self.fx_log_display)
        
        self.tab_fx.setLayout(layout)

    def initTab2(self):
        layout = QVBoxLayout()
        
        # ë¡œê·¸ ë””ìŠ¤í”Œë ˆì´ ë¨¼ì € ìƒì„±í•˜ì—¬ API ì˜¤ë¥˜ ì‹œì—ë„ ì•ˆì „í•˜ê²Œ ë¡œê·¸ ì¶œë ¥ ê°€ëŠ¥í•˜ê²Œ í•¨
        self.tts_log = QTextEdit()
        self.tts_log.setReadOnly(True)
        self.tts_log.setMaximumHeight(100)

        # API ì´ˆê¸°í™” (íŒŒì¼ ê²½ë¡œ ì—†ìŒ)
        try:
            self.tts_client = ElevenLabsClient()
            self.api_keys = self.tts_client.get_api_keys()
            self.voices = self.tts_client.get_voices()
            self.models = self.tts_client.get_models()
        except Exception as e:
            layout.addWidget(QLabel(f"API/DB ì´ˆê¸°í™” ì˜¤ë¥˜: {e}"))
            layout.addWidget(self.tts_log) # ì˜¤ë¥˜ ìƒí™©ì—ì„œë„ ë¡œê·¸ì°½ì€ ë³´ì—¬ì¤Œ
            self.tab2.setLayout(layout)
            return


        # ì €ì¥ ê²½ë¡œ ì„¤ì •
        path_layout = QHBoxLayout()
        self.audio_path_edit = QLineEdit(r"D:\youtube")
        self.audio_path_edit.setStyleSheet("background-color: #2D2D2D; color: #D4D4D4; height: 25px;")
        btn_browse_audio = QPushButton("ì°¾ì•„ë³´ê¸°")
        btn_browse_audio.clicked.connect(self.browse_audio_path)
        path_layout.addWidget(QLabel("ì €ì¥ í´ë”:"))
        path_layout.addWidget(self.audio_path_edit)
        path_layout.addWidget(btn_browse_audio)
        layout.addLayout(path_layout)

        # ì„¤ì • ê·¸ë£¹
        settings_group = QGroupBox("TTS ì„¤ì •")
        form_layout = QFormLayout()

        # API Key ì„ íƒ
        self.combo_apikey = QComboBox()
        for k in self.api_keys:
            self.combo_apikey.addItem(k['name'], k['api_key']) # name displayed, api_key as data
        
        # ê¸°ë³¸ ì„ íƒëœ í‚¤ ì„¤ì •
        if self.api_keys:
            self.tts_client.set_api_key(self.api_keys[0]['api_key'])
            
        self.combo_apikey.currentIndexChanged.connect(self.on_apikey_changed)
        form_layout.addRow("API Key:", self.combo_apikey)

        # ì„±ìš° ì„ íƒ
        self.combo_voice = QComboBox()
        for v in self.voices:
            self.combo_voice.addItem(f"{v['name']}", v['voice_id'])
        form_layout.addRow("ì„±ìš° (Voice):", self.combo_voice)

        # ëª¨ë¸ ì„ íƒ
        self.combo_model = QComboBox()
        for m in self.models:
            self.combo_model.addItem(m['name'], m['model_id'])
        form_layout.addRow("ëª¨ë¸ (Model):", self.combo_model)

        # ì„¤ì • ìŠ¬ë¼ì´ë”ë“¤
        # ì•ˆì •ì„±
        self.slider_stability = self.create_slider(0, 100, 50)
        self.lbl_stability = QLabel("0.50")
        self.lbl_stability.setFixedWidth(40)
        self.slider_stability.valueChanged.connect(lambda v: self.lbl_stability.setText(f"{v/100:.2f}"))
        row_stability = QHBoxLayout()
        row_stability.addWidget(self.slider_stability)
        row_stability.addWidget(self.lbl_stability)
        form_layout.addRow("ì•ˆì •ì„± (Stability):", row_stability)

        # ìœ ì‚¬ì„±
        self.slider_similarity = self.create_slider(0, 100, 75)
        self.lbl_similarity = QLabel("0.75")
        self.lbl_similarity.setFixedWidth(40)
        self.slider_similarity.valueChanged.connect(lambda v: self.lbl_similarity.setText(f"{v/100:.2f}"))
        row_similarity = QHBoxLayout()
        row_similarity.addWidget(self.slider_similarity)
        row_similarity.addWidget(self.lbl_similarity)
        form_layout.addRow("ìœ ì‚¬ì„± (Similarity):", row_similarity)
        
        # ìŠ¤íƒ€ì¼
        self.slider_style = self.create_slider(0, 100, 0)
        self.lbl_style = QLabel("0.00")
        self.lbl_style.setFixedWidth(40)
        self.slider_style.valueChanged.connect(lambda v: self.lbl_style.setText(f"{v/100:.2f}"))
        row_style = QHBoxLayout()
        row_style.addWidget(self.slider_style)
        row_style.addWidget(self.lbl_style)
        form_layout.addRow("ìŠ¤íƒ€ì¼ (Style):", row_style)

        # ìŒì„± ì†ë„
        self.slider_speed = self.create_slider(70, 120, 100)
        self.lbl_speed = QLabel("1.00")
        self.lbl_speed.setFixedWidth(40)
        self.slider_speed.valueChanged.connect(lambda v: self.lbl_speed.setText(f"{v/100:.2f}"))
        row_speed = QHBoxLayout()
        row_speed.addWidget(self.slider_speed)
        row_speed.addWidget(self.lbl_speed)
        form_layout.addRow("ìŒì„± ì†ë„ (Speed):", row_speed)

        # ìŒì„± ë³¼ë¥¨ (TTS ìƒì„± ì‹œ ìì²´ ë³¼ë¥¨)
        self.slider_tts_volume = self.create_slider(0, 300, 100)
        self.lbl_tts_volume = QLabel("100%")
        self.lbl_tts_volume.setFixedWidth(40)
        self.slider_tts_volume.valueChanged.connect(lambda v: self.lbl_tts_volume.setText(f"{v}%"))
        row_tts_vol = QHBoxLayout()
        row_tts_vol.addWidget(self.slider_tts_volume)
        row_tts_vol.addWidget(self.lbl_tts_volume)
        form_layout.addRow("ìŒì„± ë³¼ë¥¨ (Volume):", row_tts_vol)
        
        # ë…¸ì´ì¦ˆ ì œê±°ìš© íŠ¸ë¦¬ë°
        self.spin_tts_trim = QDoubleSpinBox()
        self.spin_tts_trim.setRange(0.0, 2.0)
        self.spin_tts_trim.setSingleStep(0.05)
        self.spin_tts_trim.setValue(0.0)
        self.spin_tts_trim.setSuffix(" ì´ˆ")
        form_layout.addRow("ì¡ìŒ ì œê±° (Trim End):", self.spin_tts_trim)

        settings_group.setLayout(form_layout)
        layout.addWidget(settings_group)

        # ìƒì„± ë²„íŠ¼
        self.btn_generate_tts = QPushButton("ğŸ”Š ì˜¤ë””ì˜¤ ìƒì„± (Generate Audio)")
        self.btn_generate_tts.setStyleSheet("height: 50px; font-weight: bold; background-color: #28a745; color: white; border-radius: 10px;")
        self.btn_generate_tts.clicked.connect(self.generate_audio)
        layout.addWidget(self.btn_generate_tts)

        # í…ìŠ¤íŠ¸ ì…ë ¥
        layout.addWidget(QLabel("ì…ë ¥ í…ìŠ¤íŠ¸:"))
        self.tts_input = QTextEdit()
        self.tts_input.setPlaceholderText("ë³€í™˜í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”...")
        layout.addWidget(self.tts_input)

        # ë¡œê·¸ì°½ì€ ìœ„ì—ì„œ ì´ë¯¸ ìƒì„±ë¨
        layout.addWidget(self.tts_log)

        self.tab2.setLayout(layout)

        # Connect signals for thread safety (AFTER UI creation)
        self.log_signal.connect(self.tts_log.append)
        self.enable_button_signal.connect(self.set_btn_enable)
        self.error_signal.connect(self.tts_log.append)

    def initTab3(self):
        layout = QVBoxLayout()

        # ìƒë‹¨ í†µí•© ì‘ì—… í´ë” ì„ íƒ
        workspace_layout = QHBoxLayout()
        self.video_workspace_path = QLineEdit(r"D:\youtube")
        btn_workspace = QPushButton("í´ë” ì„ íƒ")
        btn_workspace.clicked.connect(lambda: self.browse_folder(self.video_workspace_path))
        workspace_layout.addWidget(QLabel("ì‘ì—… í´ë” (Image/Audio ê°€ ìˆëŠ” ê³³):"))
        workspace_layout.addWidget(self.video_workspace_path)
        workspace_layout.addWidget(btn_workspace)
        layout.addLayout(workspace_layout)

        # ìŠ¤íƒ€ì¼ ì„¤ì • ê·¸ë£¹ (Shared)
        self.style_group = self.create_style_group()
        layout.addWidget(self.style_group)
        
        # ì•ˆë‚´ ë¬¸êµ¬ (JSON ìë™ ë¡œë“œ ì•Œë¦¼)
        layout.addWidget(QLabel("â„¹ï¸ ìë§‰ì€ ì˜¤ë””ì˜¤ íŒŒì¼(MP3)ê³¼ ê°™ì€ ì´ë¦„ì˜ .json íŒŒì¼ì—ì„œ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."))

        # ì‹œì‘ ë²„íŠ¼
        self.btn_merge_video = QPushButton("ğŸ¬ ì˜ìƒ í•©ì„± ì‹œì‘ (ìë§‰ í¬í•¨)")
        self.btn_merge_video.setStyleSheet("height: 50px; font-weight: bold; background-color: #673AB7; color: white; border-radius: 8px; margin-top: 10px;")
        self.btn_merge_video.clicked.connect(self.start_video_merge)
        layout.addWidget(self.btn_merge_video)

        # ë¡œê·¸
        self.video_log = QTextEdit()
        self.video_log.setReadOnly(True)
        self.video_log.setStyleSheet("background-color: #1E1E1E; color: #D4D4D4;")
        layout.addWidget(self.video_log)

        # ì—¬ë°± ìµœì í™”
        layout.setSpacing(5)
        layout.setContentsMargins(10, 10, 10, 10)

        # ê¸°ë³¸ í°íŠ¸ ë¡œë“œ
        self.load_custom_fonts()
        self.update_color_indicators()

        self.tab3.setLayout(layout)

    def create_style_group(self):
        # ìŠ¤íƒ€ì¼ ì„¤ì • ê·¸ë£¹
        group = QGroupBox("ìë§‰ ìŠ¤íƒ€ì¼ ì„¤ì •")
        style_layout = QGridLayout()
        
        self.chk_use_sub = QCheckBox("ìë§‰ ì‚¬ìš©")
        self.chk_use_sub.setChecked(True)
        style_layout.addWidget(self.chk_use_sub, 0, 0)
        
        # ëœë¤ íš¨ê³¼ ì²´í¬ë°•ìŠ¤ ì¶”ê°€
        self.chk_random_effect = QCheckBox("ëœë¤ í™”ë©´ íš¨ê³¼ (Zoom/Pan 1.0->1.1)")
        self.chk_random_effect.setChecked(False)
        style_layout.addWidget(self.chk_random_effect, 0, 1, 1, 3)

        # 1í–‰: í°íŠ¸ í´ë”
        font_folder_label = QLabel("í°íŠ¸ í´ë”:")
        self.font_folder_path = QLineEdit(r"D:\youtube\fonts")
        btn_font_folder = QPushButton("ì°¾ê¸°")
        btn_font_folder.clicked.connect(lambda: self.browse_folder(self.font_folder_path, self.load_custom_fonts))
        style_layout.addWidget(font_folder_label, 1, 0)
        style_layout.addWidget(self.font_folder_path, 1, 1, 1, 3)
        style_layout.addWidget(btn_font_folder, 1, 4)

        # 2í–‰: í°íŠ¸ ë° í¬ê¸°
        self.combo_font = QComboBox()
        
        self.spin_font_size = QSpinBox()
        self.spin_font_size.setRange(10, 200)
        self.spin_font_size.setValue(60)
        
        style_layout.addWidget(QLabel("í°íŠ¸ ì„ íƒ:"), 2, 0)
        style_layout.addWidget(self.combo_font, 2, 1, 1, 2)
        style_layout.addWidget(QLabel("í¬ê¸°:"), 2, 3)
        style_layout.addWidget(self.spin_font_size, 2, 4)

        # 3í–‰: ìƒ‰ìƒ ì„ íƒ
        self.color_text = "black"
        self.color_outline = "white"
        self.color_bg = "Transparent"

        # ê¸€ììƒ‰
        self.btn_text_color = QPushButton("ê¸€ììƒ‰")
        self.btn_text_color.clicked.connect(lambda: self.pick_color('text'))
        self.ind_text_color = QLabel()
        self.ind_text_color.setFixedSize(20, 20)
        
        # í…Œë‘ë¦¬ìƒ‰
        self.btn_outline_color = QPushButton("í…Œë‘ë¦¬ìƒ‰")
        self.btn_outline_color.clicked.connect(lambda: self.pick_color('outline'))
        self.ind_outline_color = QLabel()
        self.ind_outline_color.setFixedSize(20, 20)
        
        # ë°°ê²½ìƒ‰
        self.btn_bg_color = QPushButton("ë°°ê²½ìƒ‰")
        self.btn_bg_color.clicked.connect(lambda: self.pick_color('bg'))
        self.ind_bg_color = QLabel()
        self.ind_bg_color.setFixedSize(20, 20)
        
        self.checkbox_use_outline = QCheckBox("í…Œë‘ë¦¬ ì‚¬ìš©")
        self.checkbox_use_outline.setChecked(True)
        self.checkbox_use_outline.stateChanged.connect(self.update_color_indicators)
        
        style_layout.addWidget(self.btn_text_color, 3, 0)
        style_layout.addWidget(self.ind_text_color, 3, 1)
        style_layout.addWidget(self.btn_outline_color, 3, 2)
        style_layout.addWidget(self.ind_outline_color, 3, 3)
        style_layout.addWidget(self.checkbox_use_outline, 3, 4)

        # 4í–‰: ë°°ê²½ìƒ‰ ë° ì‚¬ìš© ì—¬ë¶€
        self.checkbox_use_bg = QCheckBox("ë°°ê²½ìƒ‰ ì‚¬ìš©")
        self.checkbox_use_bg.setChecked(True)
        self.checkbox_use_bg.stateChanged.connect(self.update_color_indicators)
        
        style_layout.addWidget(self.checkbox_use_bg, 4, 0)
        style_layout.addWidget(self.btn_bg_color, 4, 1, 1, 2)
        style_layout.addWidget(self.ind_bg_color, 4, 3)
        
        # 5í–‰: ë°°ê²½ íˆ¬ëª…ë„ ì¡°ì ˆ
        style_layout.addWidget(QLabel("ë°°ê²½ íˆ¬ëª…ë„:"), 5, 0)
        self.slider_bg_opacity = QSlider(Qt.Horizontal)
        self.slider_bg_opacity.setRange(0, 100)
        self.slider_bg_opacity.setValue(80) 
        self.lbl_bg_opacity = QLabel("80%")
        self.lbl_bg_opacity.setFixedWidth(40)
        self.slider_bg_opacity.valueChanged.connect(self.update_color_indicators)
        self.slider_bg_opacity.valueChanged.connect(lambda v: self.lbl_bg_opacity.setText(f"{v}%"))
        
        row_opacity = QHBoxLayout()
        row_opacity.addWidget(self.slider_bg_opacity)
        row_opacity.addWidget(self.lbl_bg_opacity)
        style_layout.addLayout(row_opacity, 5, 1, 1, 3)

        # 6í–‰: ì†Œë¦¬ ë³¼ë¥¨ ì¡°ì ˆ (ë°°ê²½ íˆ¬ëª…ë„ ë°”ë¡œ ë°‘)
        style_layout.addWidget(QLabel("ì†Œë¦¬ ë³¼ë¥¨:"), 6, 0)
        self.slider_volume = QSlider(Qt.Horizontal)
        self.slider_volume.setRange(0, 300)
        self.slider_volume.setValue(100)
        self.lbl_volume = QLabel("100%")
        self.lbl_volume.setFixedWidth(40)
        self.slider_volume.valueChanged.connect(lambda v: self.lbl_volume.setText(f"{v}%"))
        
        row_vol = QHBoxLayout()
        row_vol.addWidget(self.slider_volume)
        row_vol.addWidget(self.lbl_volume)
        style_layout.addLayout(row_vol, 6, 1, 1, 3)

        group.setLayout(style_layout)
        return group



    def initTab4(self):
        layout = QVBoxLayout()

        # ê²½ë¡œ ì„¤ì • ê·¸ë£¹
        path_group = QGroupBox("ì˜ìƒ ê²½ë¡œ ì„¤ì •")
        path_layout = QGridLayout()

        self.concat_input_dir = QLineEdit(r"D:\youtube")
        btn_browse_input = QPushButton("ì˜ìƒ í´ë” ì„ íƒ")
        btn_browse_input.clicked.connect(lambda: self.browse_folder(self.concat_input_dir))
        
        path_layout.addWidget(QLabel("ì…ë ¥ ì˜ìƒ í´ë”:"), 0, 0)
        path_layout.addWidget(self.concat_input_dir, 0, 1)
        path_layout.addWidget(btn_browse_input, 0, 2)

        self.concat_output_file = QLineEdit(r"D:\youtube\final_video.mp4")
        btn_browse_output = QPushButton("ì €ì¥ íŒŒì¼ ì§€ì •")
        btn_browse_output.clicked.connect(self.browse_save_file)
        
        path_layout.addWidget(QLabel("ìµœì¢… íŒŒì¼ ì´ë¦„:"), 1, 0)
        path_layout.addWidget(self.concat_output_file, 1, 1)
        path_layout.addWidget(btn_browse_output, 1, 2)
        
        # ì›Œí„°ë§ˆí¬ ì„ íƒ (New)
        self.watermark_path = QLineEdit()
        self.watermark_path.setPlaceholderText("ì›Œí„°ë§ˆí¬ ì´ë¯¸ì§€ (ì„ íƒ ì‚¬í•­)")
        btn_browse_wm = QPushButton("ì›Œí„°ë§ˆí¬ ì„ íƒ")
        btn_browse_wm.clicked.connect(lambda: self.browse_single_file(self.watermark_path, "Images (*.png *.jpg)"))
        
        path_layout.addWidget(QLabel("ì›Œí„°ë§ˆí¬(ë¡œê³ ):"), 2, 0)
        path_layout.addWidget(self.watermark_path, 2, 1)
        path_layout.addWidget(btn_browse_wm, 2, 2)

        path_group.setLayout(path_layout)
        layout.addWidget(path_group)


        # í•©ì¹˜ê¸° ë²„íŠ¼
        self.btn_start_concat = QPushButton("ğŸï¸ ì˜ìƒ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸° (Combine Videos)")
        self.btn_start_concat.setStyleSheet("height: 50px; font-weight: bold; background-color: #ff5722; color: white; border-radius: 8px;")
        self.btn_start_concat.clicked.connect(self.start_video_concat)
        layout.addWidget(self.btn_start_concat)

        # ë¡œê·¸ì°½
        layout.addWidget(QLabel("ì§„í–‰ ë¡œê·¸:"))
        self.concat_log = QTextEdit()
        self.concat_log.setReadOnly(True)
        self.concat_log.setStyleSheet("background-color: #1E1E1E; color: #D4D4D4;")
        layout.addWidget(self.concat_log)

        self.tab4.setLayout(layout)

    def initTab5(self):
        layout = QVBoxLayout()

        # íŒŒì¼ ì„ íƒ ê·¸ë£¹
        # í´ë” ì„ íƒ ê·¸ë£¹ (Batch Processing)
        file_group = QGroupBox("í´ë” ì„¤ì • (ì¼ê´„ ì²˜ë¦¬)")
        file_layout = QGridLayout()

        # ì…ë ¥ í´ë” (ì˜¤ë””ì˜¤ + ì´ë¯¸ì§€)
        self.eff_input_dir = QLineEdit()
        self.eff_input_dir.setPlaceholderText("ì˜¤ë””ì˜¤(.mp3)ì™€ ì´ë¯¸ì§€ íŒŒì¼ì´ ìˆëŠ” í´ë”")
        btn_browse_in = QPushButton("ì…ë ¥ í´ë” ì„ íƒ")
        btn_browse_in.clicked.connect(lambda: self.browse_folder(self.eff_input_dir))
        
        file_layout.addWidget(QLabel("ì…ë ¥(ì†ŒìŠ¤) í´ë”:"), 0, 0)
        file_layout.addWidget(self.eff_input_dir, 0, 1)
        file_layout.addWidget(btn_browse_in, 0, 2)

        # ì¶œë ¥ í´ë”
        self.eff_output_dir = QLineEdit()
        self.eff_output_dir.setPlaceholderText("ê²°ê³¼ë¬¼(.mp4) ì €ì¥ í´ë”")
        btn_browse_out = QPushButton("ì¶œë ¥ í´ë” ì„ íƒ")
        btn_browse_out.clicked.connect(lambda: self.browse_folder(self.eff_output_dir))
        
        file_layout.addWidget(QLabel("ì¶œë ¥(ì €ì¥) í´ë”:"), 1, 0)
        file_layout.addWidget(self.eff_output_dir, 1, 1)
        file_layout.addWidget(btn_browse_out, 1, 2)

        file_group.setLayout(file_layout)
        layout.addWidget(file_group)

        # ì˜¤ë””ì˜¤ íŠ¸ë¦¬ë° ì„¤ì •
        trim_layout = QHBoxLayout()
        self.spin_trim_end = QDoubleSpinBox()
        self.spin_trim_end.setRange(0.0, 10.0)
        self.spin_trim_end.setSingleStep(0.1)
        self.spin_trim_end.setValue(0.0)
        self.spin_trim_end.setSuffix(" ì´ˆ")
        trim_layout.addWidget(QLabel("ì˜¤ë””ì˜¤ ë’·ë¶€ë¶„ ìë¥´ê¸° (íŠ¸ë¦¬ë°):"))
        trim_layout.addWidget(self.spin_trim_end)
        trim_layout.addWidget(QLabel("â€» ElevenLabs ì¡ìŒ ì œê±°ìš©"))
        
        self.btn_trim_audio_only = QPushButton("âœ‚ï¸ MP3ë§Œ ìë¥´ê¸°")
        self.btn_trim_audio_only.setStyleSheet("height: 30px; font-weight: bold; background-color: #757575; color: white; border-radius: 5px;")
        self.btn_trim_audio_only.clicked.connect(self.run_mp3_trimming)
        trim_layout.addWidget(self.btn_trim_audio_only)
        
        trim_layout.addStretch()
        layout.addLayout(trim_layout)
        
        # ì˜ìƒ íš¨ê³¼ ì„¤ì • (Ken Burns Effect)
        effect_group = QGroupBox("ì˜ìƒ íš¨ê³¼ ì„¤ì • (Ken Burns Effect)")
        effect_layout = QGridLayout()
        
        self.combo_effect_type = QComboBox()
        self.combo_effect_type.addItems(["íš¨ê³¼ ì—†ìŒ", "Zoom (í™•ëŒ€/ì¶•ì†Œ)", "Pan Left to Right (ì¢Œâ†’ìš°)", "Pan Right to Left (ìš°â†’ì¢Œ)"])
        
        self.spin_start_scale = QDoubleSpinBox()
        self.spin_start_scale.setRange(0.1, 5.0)
        self.spin_start_scale.setSingleStep(0.05)
        self.spin_start_scale.setValue(1.0) # ê¸°ë³¸ 1.0 (ì›ë³¸ í¬ê¸°)
        self.spin_start_scale.setSuffix("x")
        
        self.spin_end_scale = QDoubleSpinBox()
        self.spin_end_scale.setRange(0.1, 5.0)
        self.spin_end_scale.setSingleStep(0.05)
        self.spin_end_scale.setValue(1.15) # ê¸°ë³¸ 1.15 (115% í™•ëŒ€)
        self.spin_end_scale.setSuffix("x")
        
        self.combo_effect_type.addItems(["íš¨ê³¼ ì—†ìŒ", "Zoom (í™•ëŒ€/ì¶•ì†Œ)", "Pan Left to Right (ì¢Œâ†’ìš°)", "Pan Right to Left (ìš°â†’ì¢Œ)"])
        
        # [NEW] ëœë¤ íš¨ê³¼ ì²´í¬ë°•ìŠ¤
        self.chk_random_effect = QCheckBox("ğŸ² ëœë¤ ì ìš©")
        self.chk_random_effect.setStyleSheet("font-weight: bold; color: #00BCD4;")
        self.chk_random_effect.toggled.connect(lambda checked: self.combo_effect_type.setDisabled(checked))
        
        effect_layout.addWidget(QLabel("íš¨ê³¼ ì¢…ë¥˜:"), 0, 0)
        effect_layout.addWidget(self.combo_effect_type, 0, 1)
        effect_layout.addWidget(self.chk_random_effect, 0, 2)
        
        effect_layout.addWidget(QLabel("ì‹œì‘ ë°°ìœ¨:"), 1, 0)
        effect_layout.addWidget(self.spin_start_scale, 1, 1)
        effect_layout.addWidget(QLabel("ì¢…ë£Œ ë°°ìœ¨:"), 1, 2)
        effect_layout.addWidget(self.spin_end_scale, 1, 3)
        
        # Pan Speed Control
        self.spin_pan_speed = QDoubleSpinBox()
        self.spin_pan_speed.setRange(0.1, 10.0)
        self.spin_pan_speed.setSingleStep(0.1)
        self.spin_pan_speed.setValue(1.0)
        self.spin_pan_speed.setSuffix("x")
        self.spin_pan_speed.setToolTip("1.0: ì˜ìƒ ê¸¸ì´ì— ë§ì¶° ì™„ì£¼\n2.0: 2ë°° ë¹ ë¥´ê²Œ ì™„ì£¼ í›„ ì •ì§€\n0.5: ì ˆë°˜ë§Œ ì´ë™")
        
        effect_layout.addWidget(QLabel("Pan ì†ë„(ë°°ì†):"), 2, 0)
        effect_layout.addWidget(self.spin_pan_speed, 2, 1)
        
        effect_group.setLayout(effect_layout)
        layout.addWidget(effect_group)

        # ìŠ¤íƒ€ì¼ ì •ë³´ ì•ˆë‚´ (íŠ¸ë¦¬ë° ë°”ë¡œ ë°‘ìœ¼ë¡œ ì´ë™)
        share_label = QLabel("â„¹ï¸ ìƒë‹¨ Video Composite íƒ­ì˜ ìŠ¤íƒ€ì¼ ì„¤ì •(í°íŠ¸, ìƒ‰ìƒ, ì†Œë¦¬ ë³¼ë¥¨ ë“±)ì´ ê³µìœ ë©ë‹ˆë‹¤.")
        share_label.setStyleSheet("color: #008CBA; font-style: italic; margin-bottom: 5px;")
        layout.addWidget(share_label)

        # ìƒì„± ë²„íŠ¼
        self.btn_start_single = QPushButton("ğŸ¬ ì˜ìƒ íš¨ê³¼ ì ìš© ì¼ê´„ ì‹œì‘ (Batch Effect)")
        self.btn_start_single.setStyleSheet("height: 50px; font-weight: bold; background-color: #008CBA; color: white; border-radius: 8px;")
        self.btn_start_single.clicked.connect(self.start_batch_video_effect)
        layout.addWidget(self.btn_start_single)

        # ë¡œê·¸
        self.single_log = QTextEdit()
        self.single_log.setReadOnly(True)
        self.single_log.setStyleSheet("background-color: #1E1E1E; color: #D4D4D4;")
        layout.addWidget(self.single_log)

        self.tab5.setLayout(layout)

    def initTab6(self):
        layout = QVBoxLayout()
        
        # íŒŒì¼ ì„ íƒ ê·¸ë£¹
        file_group = QGroupBox("íŒŒì¼ ì„ íƒ")
        file_layout = QGridLayout()

        # ë™ì˜ìƒ ì„ íƒ
        self.dub_video_path = QLineEdit()
        btn_browse_vid = QPushButton("ë°°ê²½ ë™ì˜ìƒ ì„ íƒ")
        btn_browse_vid.clicked.connect(lambda: self.browse_single_file(self.dub_video_path, "Video Files (*.mp4 *.avi *.mkv *.mov)"))
        file_layout.addWidget(QLabel("ë°°ê²½ ë™ì˜ìƒ:"), 0, 0)
        file_layout.addWidget(self.dub_video_path, 0, 1)
        file_layout.addWidget(btn_browse_vid, 0, 2)

        # ì˜¤ë””ì˜¤ ì„ íƒ
        self.dub_audio_path = QLineEdit()
        btn_browse_aud = QPushButton("ìŒì„±(ì˜¤ë””ì˜¤) ì„ íƒ")
        btn_browse_aud.clicked.connect(lambda: self.browse_single_file(self.dub_audio_path, "Audio (*.mp3 *.wav)"))
        file_layout.addWidget(QLabel("ìŒì„± íŒŒì¼:"), 1, 0)
        file_layout.addWidget(self.dub_audio_path, 1, 1)
        file_layout.addWidget(btn_browse_aud, 1, 2)

        # ì¶œë ¥ ì„ íƒ
        self.dub_output_path = QLineEdit()
        btn_browse_out = QPushButton("ì €ì¥ ê²½ë¡œ")
        btn_browse_out.clicked.connect(lambda: self.browse_single_save_file(self.dub_output_path))
        file_layout.addWidget(QLabel("ì¶œë ¥ íŒŒì¼:"), 2, 0)
        file_layout.addWidget(self.dub_output_path, 2, 1)
        file_layout.addWidget(btn_browse_out, 2, 2)
        
        file_group.setLayout(file_layout)
        layout.addWidget(file_group)

        # ìë§‰ ê´€ë ¨ ì•ˆë‚´
        note_label = QLabel("â„¹ï¸ ìë§‰ì€ ì˜¤ë””ì˜¤ íŒŒì¼(MP3)ê³¼ ê°™ì€ ì´ë¦„ì˜ .json íŒŒì¼ì—ì„œ ìë™ìœ¼ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.\n   (ElevenLabs JSON í˜•ì‹ ì§€ì›)")
        note_label.setStyleSheet("color: #008CBA; font-weight: bold; padding: 5px;")
        layout.addWidget(note_label)
        
        # ìŠ¤íƒ€ì¼ ì•ˆë‚´
        layout.addWidget(QLabel("â„¹ï¸ ìë§‰ ìŠ¤íƒ€ì¼(í°íŠ¸, í¬ê¸°, ìƒ‰ìƒ)ì€ 'Video Composite' íƒ­ì˜ ì„¤ì •ì„ ë”°ë¦…ë‹ˆë‹¤."))

        # ì‹œì‘ ë²„íŠ¼
        self.btn_start_dubbing = QPushButton("ğŸ¬ ë™ì˜ìƒ í•©ì¹˜ê¸° ë° ìë§‰ ìƒì„± (Start Dubbing)")
        self.btn_start_dubbing.setStyleSheet("height: 50px; font-weight: bold; background-color: #9C27B0; color: white; border-radius: 8px;")
        self.btn_start_dubbing.clicked.connect(self.start_video_dubbing)
        layout.addWidget(self.btn_start_dubbing)

        # ë¡œê·¸
        self.dub_log = QTextEdit()
        self.dub_log.setReadOnly(True)
        self.dub_log.setStyleSheet("background-color: #1E1E1E; color: #D4D4D4;")
        layout.addWidget(self.dub_log)

        self.tab6.setLayout(layout)

    def start_video_dubbing(self):
        v_path = self.dub_video_path.text().strip()
        a_path = self.dub_audio_path.text().strip()
        o_path = self.dub_output_path.text().strip()
        
        if not os.path.exists(v_path) or not os.path.exists(a_path):
            QMessageBox.warning(self, "ê²½ê³ ", "ë™ì˜ìƒ ë˜ëŠ” ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
            
        if not o_path:
            QMessageBox.warning(self, "ê²½ê³ ", "ì¶œë ¥ ê²½ë¡œë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.")
            return

        # ìë§‰: Noneìœ¼ë¡œ ì„¤ì •í•˜ì—¬ Workerê°€ JSONì—ì„œ ìë™ìœ¼ë¡œ ì°¾ê²Œ í•¨
        subtitles = None
                    
        # ìŠ¤íƒ€ì¼ (íƒ­3ì—ì„œ ê°€ì ¸ì˜´)
        style = {
            'font_family': self.combo_font.currentText(),
            'font_size': self.spin_font_size.value(),
            'text_color': self.color_text,
            'outline_color': self.color_outline,
            'bg_color': self.color_bg,
            'bg_opacity': int(self.slider_bg_opacity.value() * 2.55),
            'use_bg': self.checkbox_use_bg.isChecked(),
            'use_outline': self.checkbox_use_outline.isChecked()
        }
        
        volume = self.slider_volume.value() / 100.0

        self.btn_start_dubbing.setEnabled(False)
        self.dub_log.append("â³ ì‘ì—… ì‹œì‘...")
        self.dub_log.append(f"âš™ï¸ ì ìš© ìŠ¤íƒ€ì¼: í°íŠ¸[{style['font_family']}] í¬ê¸°[{style['font_size']}] ìƒ‰ìƒ[{style['text_color']}]")
        self.dub_log.append(f"   (í°íŠ¸ í¬ê¸°ê°€ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìœ¼ë©´ 'Video Composite' íƒ­ì—ì„œ ì¡°ì ˆí•˜ì„¸ìš”.)")
        
        self.dub_worker = VideoDubbingWorker(v_path, a_path, o_path, subtitles, style, volume)
        self.dub_worker.log_signal.connect(self.dub_log.append)
        self.dub_worker.finished.connect(lambda m, e: [self.dub_log.append(f"ğŸ {m}"), self.btn_start_dubbing.setEnabled(True)])
        self.dub_worker.error.connect(lambda e: [self.dub_log.append(f"âŒ {e}"), self.btn_start_dubbing.setEnabled(True)])
        self.dub_worker.start()

    def browse_single_file(self, line_edit, filter):
        file, _ = QFileDialog.getOpenFileName(self, "íŒŒì¼ ì„ íƒ", "", filter)
        if file:
            line_edit.setText(file)
            # ì´ë¯¸ì§€ë‚˜ ì˜¤ë””ì˜¤ ì„ íƒ ì‹œ ìë™ìœ¼ë¡œ ì¶œë ¥ íŒŒì¼ëª… ì œì•ˆ (mp4)
            if hasattr(self, 'single_output_path') and not self.single_output_path.text():
                base = os.path.splitext(file)[0]
                self.single_output_path.setText(base + ".mp4")

    def browse_single_save_file(self, line_edit):
        file, _ = QFileDialog.getSaveFileName(self, "ì €ì¥ íŒŒì¼ ì§€ì •", line_edit.text(), "Video Files (*.mp4)")
        if file:
            line_edit.setText(file)

    def start_single_video_merge(self):
        img_path = self.single_img_path.text().strip()
        audio_path = self.single_audio_path.text().strip()
        out_path = self.single_output_path.text().strip()

        if not os.path.exists(img_path) or not os.path.exists(audio_path):
            QMessageBox.warning(self, "ê²½ê³ ", "ì´ë¯¸ì§€ ë˜ëŠ” ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        # ìë§‰ íŒŒì‹± (JSON ìë™ ë¡œë“œì´ë¯€ë¡œ subtitlesëŠ” Noneìœ¼ë¡œ ì „ë‹¬í•˜ì—¬ workerê°€ JSONì„ ì°¾ê²Œ í•¨)
        subtitles = None

        style = {
            'font_family': self.combo_font.currentText(),
            'font_size': self.spin_font_size.value(),
            'text_color': self.color_text,
            'outline_color': self.color_outline,
            'bg_color': self.color_bg,
            'bg_opacity': int(self.slider_bg_opacity.value() * 2.55),
            'use_bg': self.checkbox_use_bg.isChecked(),
            'use_outline': self.checkbox_use_outline.isChecked()
        }

        self.btn_start_single.setEnabled(False)
        self.single_log.append("â³ ê°œë³„ ì˜ìƒ í•©ì„± ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

        volume_factor = self.slider_volume.value() / 100.0 # ì„¤ì •ê°’ ìˆ˜ì§‘
        trim_end = self.spin_trim_end.value()
        
        effect_config = {
            'type': self.combo_effect_type.currentIndex(), # 0:None, 1:Zoom, 2:PanL->R, 3:PanR->L
            'start_scale': self.spin_start_scale.value(),
            'end_scale': self.spin_end_scale.value(),
            'pan_speed': self.spin_pan_speed.value()
        }
        
        # ì›Œì»¤ ì‹œì‘
        self.single_worker = SingleVideoWorker(img_path, audio_path, out_path, subtitles, style, volume_factor, trim_end, effect_config)
        self.single_worker.log_signal.connect(self.single_log.append)
        self.single_worker.finished.connect(lambda m, e: [self.single_log.append(f"ğŸ {m}"), self.btn_start_single.setEnabled(True)])
        self.single_worker.error.connect(lambda e: [self.single_log.append(f"âŒ ì˜¤ë¥˜: {e}"), self.btn_start_single.setEnabled(True)])
        self.single_worker.start()

    def start_video_merge(self):
        # ì‘ì—… í´ë” í™•ì¸
        workspace = self.video_workspace_path.text().strip()
        if not os.path.exists(workspace):
            QMessageBox.warning(self, "ê²½ë¡œ ì˜¤ë¥˜", "ì‘ì—… í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        # ìŠ¤íƒ€ì¼ dict ìƒì„±
        style = {
            'font_family': self.combo_font.currentText(),
            'font_size': self.spin_font_size.value(),
            'text_color': self.color_text,
            'outline_color': self.color_outline if self.checkbox_use_outline.isChecked() else None,
            'bg_color': self.color_bg if self.checkbox_use_bg.isChecked() else "Transparent",
            'bg_opacity': self.slider_bg_opacity.value(),
            'use_bg': self.checkbox_use_bg.isChecked(),
            'use_outline': self.checkbox_use_outline.isChecked()
        }
        
        # í°íŠ¸ ê²€ì¦
        if not style['font_family']:
            QMessageBox.warning(self, "í°íŠ¸ ì˜¤ë¥˜", "í°íŠ¸ê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
            
        # ìë§‰ ë¦¬ìŠ¤íŠ¸ ë¡œë“œ (JSON ìš°ì„ )
        # VideoMergerWorker ë‚´ë¶€ì—ì„œ ê° mp3ì— ë§ëŠ” JSONì„ ì°¾ì•„ì„œ ë¡œë“œí•¨.
        # ì—¬ê¸°ì„œëŠ” "ìë§‰ ì‚¬ìš©" ì—¬ë¶€ë§Œ ì•Œë¦¬ë©´ ë¨ (í˜¹ì€ ë¹ˆ ë”•ì…”ë„ˆë¦¬ ì „ë‹¬)
        subtitles = {} # Worker will load from JSON
        if not self.chk_use_sub.isChecked():
            subtitles = None # ì•„ì˜ˆ ìë§‰ ë”
            
        # ëœë¤ íš¨ê³¼ ì—¬ë¶€
        use_random = getattr(self, 'chk_random_effect', None) and self.chk_random_effect.isChecked()

        # ì›Œì»¤ ì‹œì‘
        # output_dir = workspace/output
        output_dir = os.path.join(workspace, "output_video")
        
        # Vol, Trim settings from Tab 5 (Single) - shared or distinct?
        # User said shared.
        vol = self.slider_volume.value() / 100.0
        trim = self.spin_trim_end.value()
        
        self.merger_worker = VideoMergerWorker(
            image_dir=workspace,
            audio_dir=workspace,
            output_dir=output_dir,
            subtitles=subtitles,
            style=style,
            volume=vol,
            trim_end=trim,
            use_random_effects=use_random
        )
        self.merger_worker.log_signal.connect(self.video_log.append)
        self.merger_worker.finished.connect(self.on_video_merge_finished)
        self.merger_worker.error.connect(self.on_error)
        
        self.set_btn_enable(False)
        self.merger_worker.start()

    def run_mp3_trimming(self):
        audio_path = self.single_audio_path.text().strip()
        trim_val = self.spin_trim_end.value()

        if not os.path.exists(audio_path):
            QMessageBox.warning(self, "ê²½ê³ ", "ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
        
        if trim_val <= 0:
            QMessageBox.information(self, "ì•Œë¦¼", "ìë¥¼ ì‹œê°„(ì´ˆ)ì´ 0ì…ë‹ˆë‹¤.")
            return

        try:
            self.single_log.append(f"â³ MP3 íŠ¸ë¦¬ë° ì‹œì‘: {os.path.basename(audio_path)} (ë’·ë¶€ë¶„ {trim_val}ì´ˆ ì œê±°)")
            
            # ìƒˆ íŒŒì¼ëª… ìƒì„±
            base, ext = os.path.splitext(audio_path)
            output_trimmed = base + "_trimmed" + ext
            
            audio_clip = mpe.AudioFileClip(audio_path)
            new_duration = max(0.1, audio_clip.duration - trim_val)
            trimmed_clip = audio_clip.subclip(0, new_duration)
            
            trimmed_clip.write_audiofile(output_trimmed, logger=None)
            
            audio_clip.close()
            trimmed_clip.close()
            
            self.single_log.append(f"âœ… íŠ¸ë¦¬ë° ì™„ë£Œ! ì €ì¥ë¨: {os.path.basename(output_trimmed)}")
            QMessageBox.information(self, "ì™„ë£Œ", f"íŠ¸ë¦¬ë°ëœ íŒŒì¼ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤:\n{output_trimmed}")
            
            # ì…ë ¥ ì¹¸ì„ íŠ¸ë¦¬ë°ëœ íŒŒì¼ë¡œ ìë™ êµì²´í•´ì¤„ì§€ ì—¬ë¶€ (í¸ì˜ì„±)
            # self.single_audio_path.setText(output_trimmed)
            
        except Exception as e:
            self.single_log.append(f"âŒ íŠ¸ë¦¬ë° ì˜¤ë¥˜: {e}")
            QMessageBox.critical(self, "ì˜¤ë¥˜", f"íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ:\n{e}")

    def browse_save_file(self):
        filename, _ = QFileDialog.getSaveFileName(self, "ìµœì¢… ì˜ìƒ ì €ì¥", self.concat_output_file.text(), "Video Files (*.mp4)")
        if filename:
            self.concat_output_file.setText(filename)

    def start_video_concat(self):
        in_dir = self.concat_input_dir.text().strip()
        out_file = self.concat_output_file.text().strip()
        wm_path = self.watermark_path.text().strip() # New

        if not os.path.exists(in_dir):
            QMessageBox.warning(self, "ê²½ê³ ", "ì…ë ¥ ì˜ìƒ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        self.btn_start_concat.setEnabled(False)
        self.concat_log.append("â³ ì˜ìƒ í•©ì¹˜ê¸° ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

        self.concat_worker = VideoConcatenatorWorker(in_dir, out_file, wm_path) # Pass wm_path
        self.concat_worker.log_signal.connect(self.concat_log.append)
        self.concat_worker.finished.connect(self.on_video_concat_finished)
        self.concat_worker.error.connect(lambda e: self.concat_log.append(f"âŒ ì˜¤ë¥˜: {e}"))
        self.concat_worker.start()

    def on_video_concat_finished(self, msg, elapsed):
        self.btn_start_concat.setEnabled(True)
        h, m, s = int(elapsed // 3600), int((elapsed % 3600) // 60), int(elapsed % 60)
        self.concat_log.append(f"{msg} (ì†Œìš” ì‹œê°„: {h:02d}:{m:02d}:{s:02d})")

    def update_color_indicators(self):
        # ì„ íƒëœ ìƒ‰ìƒì„ ì‘ì€ ë„¤ëª¨ë¡œ í‘œì‹œ
        self.ind_text_color.setStyleSheet(f"background-color: {self.color_text}; border: 1px solid white;")
        
        out_col = self.color_outline if self.color_outline.lower() != "none" else "transparent"
        self.ind_outline_color.setStyleSheet(f"background-color: {out_col}; border: 1px solid white;")
        
        # ë°°ê²½ìƒ‰ì€ íˆ¬ëª…ë„ ìŠ¬ë¼ì´ë” ê°’ ë°˜ì˜í•˜ì—¬ ì¸ë””ì¼€ì´í„°ì— í‘œì‹œ
        opacity = int(self.slider_bg_opacity.value() * 2.55)
        if self.color_bg.lower() == "transparent" or not self.checkbox_use_bg.isChecked():
            self.ind_bg_color.setStyleSheet("background-color: transparent; border: 1px solid white;")
        else:
            col = QColor(self.color_bg)
            self.ind_bg_color.setStyleSheet(f"background-color: rgba({col.red()}, {col.green()}, {col.blue()}, {self.slider_bg_opacity.value()/100.0}); border: 1px solid white;")
            
        # í…Œë‘ë¦¬ ì¸ë””ì¼€ì´í„° íˆ¬ëª… ì²˜ë¦¬
        if not self.checkbox_use_outline.isChecked():
            self.ind_outline_color.setStyleSheet("background-color: transparent; border: 1px solid white;")
        else:
            self.ind_outline_color.setStyleSheet(f"background-color: {self.color_outline}; border: 1px solid white;")

    def pick_color(self, target):
        from PyQt5.QtWidgets import QColorDialog
        color = QColorDialog.getColor()
        if color.isValid():
            hex_color = color.name()
            if target == 'text': self.color_text = hex_color
            elif target == 'outline': self.color_outline = hex_color
            elif target == 'bg': self.color_bg = hex_color
            self.update_color_indicators() # ë„¤ëª¨ì¹¸ ìƒ‰ìƒ ê°±ì‹ 

    def parse_subtitles(self, text):
        # returns { major_id: [ {"original": "...", "tts": "..."}, ... ] }
        subs = collections.defaultdict(list)
        
        # 1. ì „ì—­ ì •ê·œì‹ íŒŒì‹± (Global Regex Parsing)
        # í•œ ì¤„ì— ì—¬ëŸ¬ í•­ëª©ì´ ìˆê±°ë‚˜ ì¤„ë°”ê¿ˆì´ ë¶ˆê·œì¹™í•´ë„ "ID ì›ë³¸: ... TTS: ..." íŒ¨í„´ì„ ëª¨ë‘ ì°¾ì•„ëƒ„.
        # íŒ¨í„´: 12-34 ì›ë³¸: ... TTS: ... (ë‹¤ìŒ ID íŒ¨í„´ì´ë‚˜ í—¤ë”ê°€ ë‚˜ì˜¤ê¸° ì „ê¹Œì§€)
        # Lookahead: ë‹¤ìŒ "ìˆ«ì-ìˆ«ì ì›ë³¸:" í˜¹ì€ "ìˆ«ì. {}" í—¤ë” í˜¹ì€ ë¬¸ì¥ ë
        
        regex_pattern = r'(\d+)-(\d+)\s*ì›ë³¸:(.*?)\s*TTS:(.*?)(?=\s*\d+-\d+\s*ì›ë³¸:|\s*\d+\.\s*\{|$)'
        
        # re.DOTALL: .ì´ ê°œí–‰ë¬¸ìë„ í¬í•¨ (ì—¬ëŸ¬ ì¤„ ê±¸ì¹œ ë‚´ìš©ë„ ë§¤ì¹­)
        matches = list(re.finditer(regex_pattern, text, re.DOTALL | re.IGNORECASE))
        
        if len(matches) > 0:
            self.log_signal.emit(f"ğŸ“‹ íŒ¨í„´ ê°ì§€ ì„±ê³µ: {len(matches)}ê°œì˜ í•­ëª©ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
            for match in matches:
                major_id = match.group(1)
                # sub_id = match.group(2)
                original_text = match.group(3).strip()
                tts_text = match.group(4).strip()
                
                # ëë¶€ë¶„ì˜ ì½¤ë§ˆ ì œê±°
                if original_text.endswith(','): original_text = original_text[:-1].strip()
                if tts_text.endswith(','): tts_text = tts_text[:-1].strip()
                
                subs[major_id].append({
                    "original": original_text,
                    "tts": tts_text
                })
            return subs
        
        # 2. ê¸°ì¡´ ë¼ì¸ ë‹¨ìœ„ íŒŒì‹± (Fallback)
        # ìœ„ íŒ¨í„´ë§¤ì¹­ì— ì‹¤íŒ¨í•œ ê²½ìš° (ì˜ˆ: ì›ë³¸/TTS í‚¤ì›Œë“œê°€ ì—†ê±°ë‚˜ í¬ë§·ì´ ë‹¤ë¥¸ ê²½ìš°)
        
        lines = text.strip().split('\n')
        current_id = None
        current_item = {"original": "", "tts": ""}
        
        for line in lines:
            line = line.strip()
            if not line: continue
            
            # Skip major group headers like "1. {}" if pure header
            if re.match(r'^\d+\.\s*\{.*\}', line):
                 if "ì›ë³¸:" not in line and "TTS:" not in line:
                    continue
            
            # ì—¬ê¸°ì„œë¶€í„°ëŠ” í‚¤ì›Œë“œê°€ ì •í™•í•˜ì§€ ì•Šì€ êµ¬í˜• í¬ë§· ë“±ì„ ì²˜ë¦¬
            # í•˜ì§€ë§Œ 1ë²ˆ ë¡œì§ì—ì„œ ì¡ì§€ ëª»í•œ "ID ì›ë³¸: ... TTS: ..."ëŠ” ì‚¬ì‹¤ìƒ í˜•ì‹ì´ ê¹¨ì§„ ê²ƒì´ë¯€ë¡œ
            # ì—¬ê¸°ì„œëŠ” ì „í†µì ì¸ ID ì¤„ë°”ê¿ˆ ë°©ì‹ ë“±ì„ ì²˜ë¦¬.
            
            id_match = re.match(r'^(\d+)-(\d+)$', line)
            if id_match:
                current_id = id_match.group(1)
                current_item = {"original": "", "tts": ""}
                continue
                
            if line.startswith("ì›ë³¸:"):
                current_item["original"] = line[len("ì›ë³¸:"):].strip()
            elif line.startswith("TTS:"):
                current_item["tts"] = line[len("TTS:"):].strip()
                if current_id:
                    if not current_item["original"]:
                        current_item["original"] = current_item["tts"]
                    subs[current_id].append(dict(current_item))
                    current_item = {"original": "", "tts": ""}
            else:
                # êµ¬í˜• í¬ë§·: í‚¤ì›Œë“œ ì—†ì´ "1-1 ë‚´ìš©"
                # ë‹¨, ì›ë³¸/TTS í‚¤ì›Œë“œê°€ ìˆëŠ” ì¤„ì€ ìœ„ì—ì„œ ì²˜ë¦¬ë˜ì–´ì•¼ í•˜ë¯€ë¡œ ì œì™¸
                if "ì›ë³¸:" in line or "TTS:" in line:
                    continue 

                match = re.match(r'^(\d+)-\d+\.?\s*(.*)', line)
                if match:
                    major_id = match.group(1)
                    content = match.group(2)
                    subs[major_id].append({"original": content, "tts": content})
                    
        return subs

    def browse_folder(self, line_edit, callback=None):
        path = QFileDialog.getExistingDirectory(self, "í´ë” ì„ íƒ")
        if path:
            line_edit.setText(path)
            if callback:
                callback()

    def load_custom_fonts(self):
        font_dir = self.font_folder_path.text().strip()
        
        # 1. í°íŠ¸ í´ë”ì—ì„œ í°íŠ¸ íŒŒì¼ ë¡œë“œ & ë¡œë“œëœ íŒ¨ë°€ë¦¬ ì¶”ì 
        loaded_families = set()
        if os.path.exists(font_dir) and font_dir.lower() != r"c:\windows\fonts":
            for f in os.listdir(font_dir):
                if f.lower().endswith(('.ttf', '.otf')):
                    font_path = os.path.join(font_dir, f)
                    font_id = QFontDatabase.addApplicationFont(font_path)
                    if font_id != -1:
                        fams = QFontDatabase.applicationFontFamilies(font_id)
                        for fam in fams:
                            loaded_families.add(fam)
        
        # 2. ëª¨ë“  ì‚¬ìš© ê°€ëŠ¥í•œ í°íŠ¸ íŒ¨ë°€ë¦¬ ê°€ì ¸ì˜¤ê¸°
        all_families = QFontDatabase().families()
        
        # 3. í•„í„°ë§ (ì‚¬ìš©ì ìš”ì²­: Gmarket, Nanum, Malgun)
        # ë””ë ‰í† ë¦¬ì—ì„œ ë¡œë“œëœ í°íŠ¸ëŠ” ë¬´ì¡°ê±´ í¬í•¨
        target_keywords = ["Gmarket", "Nanum", "Malgun"]
        
        matched_families = set(loaded_families) # ë¡œë“œëœ í°íŠ¸ ìš°ì„  í¬í•¨
        
        for family in all_families:
            # ì´ë¯¸ í¬í•¨ëœ ê±´ íŒ¨ìŠ¤
            if family in matched_families:
                continue
                
            # í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
            for kw in target_keywords:
                if kw.lower() in family.lower():
                    matched_families.add(family)
                    break 
        
        # 4. ë“œë¡­ë‹¤ìš´ ëª©ë¡ ì—…ë°ì´íŠ¸
        self.combo_font.clear()
        
        if matched_families:
            final_list = sorted(list(matched_families))
            self.combo_font.addItems(final_list)
            
            # ìš°ì„ ìˆœìœ„: Gmarket > Nanum > Malgun
            # ì‚¬ìš©ìê°€ "GmarketSansTTFBold"ë¥¼ ëŒ€í‘œë¡œ ì–¸ê¸‰í–ˆìœ¼ë¯€ë¡œ 'Gmarket Sans'ê°€ í¬í•¨ëœ ê±¸ ìµœìš°ì„ ìœ¼ë¡œ ì°¾ìŒ
            target_set = False
            
            # 1ìˆœìœ„: Gmarket Sans (Bold ì„ í˜¸í•˜ì§€ë§Œ Family ë ˆë²¨ì´ë¯€ë¡œ Gmarket Sans ì°¾ê¸°)
            for i in range(self.combo_font.count()):
                text = self.combo_font.itemText(i)
                if "Gmarket Sans" in text: # Gmarket Sans TTF ë“±
                    self.combo_font.setCurrentIndex(i)
                    target_set = True
                    break
            
            # 2ìˆœìœ„: Gmarket ì•„ë¬´ê±°ë‚˜
            if not target_set:
                for i in range(self.combo_font.count()):
                    text = self.combo_font.itemText(i)
                    if "Gmarket" in text:
                        self.combo_font.setCurrentIndex(i)
                        target_set = True
                        break
                        
            # 3ìˆœìœ„: Nanum
            if not target_set:
                for i in range(self.combo_font.count()):
                    text = self.combo_font.itemText(i)
                    if "Nanum" in text:
                        self.combo_font.setCurrentIndex(i)
                        break
                        
        else:
            # ë§¤ì¹­ë˜ëŠ” ê²Œ ì—†ì„ ë•Œì˜ í´ë°±
            fallback_fonts = ["Malgun Gothic", "ë§‘ì€ ê³ ë”•", "Arial"]
            available_fallbacks = [f for f in fallback_fonts if f in all_families]
            self.combo_font.addItems(available_fallbacks if available_fallbacks else ["Arial"])

        if hasattr(self, 'video_log') and self.video_log:
            self.video_log.append(f"â„¹ï¸ í°íŠ¸ ë¡œë“œ ì™„ë£Œ: {len(matched_families)}ê°œì˜ í°íŠ¸ íŒ¨ë°€ë¦¬ (Gmarket/Nanum/Malgun/Load)")




    def on_video_merge_finished(self, msg, elapsed):
        try:
            self.btn_merge_video.setEnabled(True)
            h, m, s = int(elapsed // 3600), int((elapsed % 3600) // 60), int(elapsed % 60)
            log_msg = f"âœ… {msg} (ì†Œìš” ì‹œê°„: {h:02d}:{m:02d}:{s:02d})"
            self.video_log.append(log_msg)
            print(log_msg) # ì½˜ì†” ì¶œë ¥ ì¶”ê°€
        except Exception as e:
            print(f"Error in on_video_merge_finished: {e}")
            traceback.print_exc()

    def create_slider(self, min_val, max_val, default):
        slider = QSlider(Qt.Horizontal)
        slider.setRange(min_val, max_val)
        slider.setValue(default)
        return slider

    def on_apikey_changed(self, index):
        api_key = self.combo_apikey.currentData()
        if api_key:
            self.tts_client.set_api_key(api_key)
            self.tts_log.append(f"â„¹ï¸ API Key ë³€ê²½ë¨: {self.combo_apikey.currentText()}")

    def generate_audio(self):
        text = self.tts_input.toPlainText().strip()
        if not text:
            self.tts_log.append("âŒ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
            return

        voice_id = self.combo_voice.currentData()
        model_id = self.combo_model.currentData()
        stability = self.slider_stability.value() / 100.0
        similarity = self.slider_similarity.value() / 100.0
        style = self.slider_style.value() / 100.0
        speed = self.slider_speed.value() / 100.0
        volume = self.slider_tts_volume.value() / 100.0
        trim_end = self.spin_tts_trim.value() # íŠ¸ë¦¬ë° ê°’

        # íŒŒì‹± ë¡œì§: ê·¸ë£¹ë³„ë¡œ í…ìŠ¤íŠ¸ ë¬¶ê¸°
        subs_map = self.parse_subtitles(text)
        tasks = []
        
        if subs_map:
            for major_id, items in subs_map.items():
                combined_tts = " ".join([item['tts'] for item in items])
                if combined_tts:
                    filename = f"{major_id}.mp3"
                    tasks.append((combined_tts, filename, items))
            self.tts_log.append(f"ğŸ“‹ ë°°ì¹˜ ëª¨ë“œ ê°ì§€: {len(tasks)}ê°œì˜ íŒŒì¼ ìƒì„± ì˜ˆì •")
        else:
            # íŒ¨í„´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ ìƒì„± (UUID íŒŒì¼ëª…)
            tasks.append((text, None, [{"original": text, "tts": text}]))

        self.btn_generate_tts.setEnabled(False)
        self.tts_log.append("â³ ìƒì„± ì‹œì‘...")

        # ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰ (tasks ë¦¬ìŠ¤íŠ¸ ì „ë‹¬)
        audio_target = self.audio_path_edit.text().strip()
        threading.Thread(target=self._run_tts_thread, args=(tasks, voice_id, model_id, stability, similarity, style, speed, volume, audio_target, trim_end), daemon=True).start()

    def _run_tts_thread(self, tasks, voice_id, model_id, stability, similarity, style, speed, volume, custom_dir, trim_end=0.0):
        success_count = 0
        try:
            for task in tasks:
                # task êµ¬ì¡°: (combined_text, filename, sub_segments)
                text_chunk = task[0]
                filename = task[1]
                sub_segments = task[2] if len(task) > 2 else None
                
                try:
                    save_path = self.tts_client.generate_audio(
                        text=text_chunk, 
                        voice_id=voice_id, 
                        model_id=model_id,
                        stability=stability,
                        similarity_boost=similarity,
                        style=style,
                        speed=speed,
                        volume=volume, # ë³¼ë¥¨ ì¶”ê°€
                        filename=filename,
                        custom_dir=custom_dir,
                        sub_segments=sub_segments # ìë§‰ ì„¸ê·¸ë¨¼íŠ¸ ì „ë‹¬
                    )
                    self.log_signal.emit(f"âœ… ìƒì„± ì™„ë£Œ: {os.path.basename(save_path)}")
                    
                    # íŠ¸ë¦¬ë° ì ìš©
                    if trim_end > 0 and os.path.exists(save_path):
                        try:
                            # ì„ì‹œ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥ í›„ ë®ì–´ì“°ê¸° (ê°™ì€ íŒŒì¼ ì‘ì„±ì´ moviepyì—ì„œ ë¬¸ì œë  ìˆ˜ ìˆìŒ)
                            temp_trim_path = save_path + ".temp.mp3"
                            
                            aclip = mpe.AudioFileClip(save_path)
                            if aclip.duration > trim_end:
                                new_dur = aclip.duration - trim_end
                                sub = aclip.subclip(0, new_dur)
                                sub.write_audiofile(temp_trim_path, logger=None, bitrate="192k")
                                aclip.close()
                                sub.close()
                                
                                # ì›ë³¸ ì‚­ì œ í›„ êµì²´
                                os.remove(save_path)
                                os.rename(temp_trim_path, save_path)
                                self.log_signal.emit(f"   âœ‚ï¸ ì¡ìŒ ì œê±° ì™„ë£Œ: {trim_end}ì´ˆ ë‹¨ì¶•ë¨")
                            else:
                                aclip.close()
                                self.log_signal.emit(f"   âš ï¸ íŒŒì¼ì´ ë„ˆë¬´ ì§§ì•„ íŠ¸ë¦¬ë° ê±´ë„ˆëœ€")
                        except Exception as te:
                             self.log_signal.emit(f"   âš ï¸ íŠ¸ë¦¬ë° ì‹¤íŒ¨: {te}")
                             
                    success_count += 1
                except Exception as e:
                    self.log_signal.emit(f"âŒ ìƒì„± ì‹¤íŒ¨ ({filename}): {e}")
            
            self.log_signal.emit(f"ğŸ‰ ì „ì²´ ì‘ì—… ì™„ë£Œ ({success_count}/{len(tasks)})")
            
        except Exception as e:
            self.error_signal.emit(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        finally:
            # ë²„íŠ¼ í™œì„±í™”ëŠ” ì‹œê·¸ë„ë¡œ ì²˜ë¦¬í•´ì•¼ ì•ˆì „í•˜ì§€ë§Œ, ì—¬ê¸°ì„  ê°„ë‹¨íˆ
            # ì‹¤ì œë¡œëŠ” ì‹œê·¸ë„ì„ í†µí•´ ë©”ì¸ ìŠ¤ë ˆë“œì—ì„œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ.
            # self.btn_generate_tts.setEnabled(True) -> UI ìŠ¤ë ˆë“œ ì ‘ê·¼ ìœ„ë°˜ ê°€ëŠ¥ì„±
            # ì—¬ê¸°ì„œëŠ” log_signalì„ í†µí•´ ê°„ì ‘ì ìœ¼ë¡œ ì•Œë¦¼.
            self.enable_button_signal.emit(True)
            
    # ë²„íŠ¼ í™œì„±í™”ë¥¼ ìœ„í•œ ì‹œê·¸ë„ ì—°ê²°ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ. 
    # ê¸°ì¡´ ì½”ë“œ êµ¬ì¡°ìƒ finished ì‹œê·¸ë„ì„ í™œìš©í•˜ê±°ë‚˜ log_signalì— ì˜ì¡´.
    # ì•ˆì „í•˜ê²Œ í•˜ê¸° ìœ„í•´ ë²„íŠ¼ í™œì„±í™” ë©”ì„œë“œ ì¶”ê°€
            
    def set_btn_enable(self, enabled):
        self.btn_generate_tts.setEnabled(enabled)

    def browse_image_path(self):
        path = QFileDialog.getExistingDirectory(self, "ì´ë¯¸ì§€ ì €ì¥ í´ë” ì„ íƒ")
        if path:
            self.image_path_edit.setText(path)

    def browse_image_path_custom(self, line_edit):
        path = QFileDialog.getExistingDirectory(self, "ì´ë¯¸ì§€ ì €ì¥ í´ë” ì„ íƒ")
        if path:
            line_edit.setText(path)

    def launch_browser_and_tabs(self):
        try:
            self.log_display.append("ğŸŒ ë¸Œë¼ìš°ì €ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
            chrome_cmd = r'C:\Program Files\Google\Chrome\Application\chrome.exe'
            user_data = r'C:\sel_chrome'
            target_url = "https://www.genspark.ai/agents?type=moa_generate_image" 
            
            if not os.path.exists(user_data):
                os.makedirs(user_data)
                
            subprocess.Popen([chrome_cmd, '--remote-debugging-port=9222', f'--user-data-dir={user_data}', target_url])
            
            # Wait for browser to open
            time.sleep(3)
            
            opt = Options()
            opt.add_experimental_option("debuggerAddress", "127.0.0.1:9222")
            self.driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opt)
            
            # Ensure 2 tabs
            if len(self.driver.window_handles) < 2:
                self.driver.execute_script(f"window.open('{target_url}');")
                
            self.log_display.append("âœ… ë¸Œë¼ìš°ì € ì—°ê²° ì„±ê³µ. ë‘ ê°œì˜ íƒ­ì„ í™•ì¸í•˜ì„¸ìš”.")
            self.status_label.setText("2ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ì…ë ¥ í›„ ì‹œì‘ ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.")
            
        except Exception as e:
            self.log_display.append(f"âŒ ë¸Œë¼ìš°ì € ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            self.status_label.setText("ì˜¤ë¥˜ ë°œìƒ (ë¡œê·¸ í™•ì¸)")

    def launch_browser_nanobanana(self):
        try:
            self.nano_log_display.append("ğŸŒ NanoBanana ë¸Œë¼ìš°ì €ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
            chrome_cmd = r'C:\Program Files\Google\Chrome\Application\chrome.exe'
            user_data = r'C:\sel_chrome_nano'
            target_url = "https://gemini.google.com/app?hl=ko" 
            
            if not os.path.exists(user_data):
                os.makedirs(user_data)
                
            subprocess.Popen([chrome_cmd, '--remote-debugging-port=9224', f'--user-data-dir={user_data}', target_url])
            
            # Wait for browser to open
            time.sleep(3)
            
            opt = Options()
            opt.add_experimental_option("debuggerAddress", "127.0.0.1:9224")
            self.driver_nano = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opt)
            
            # Ensure 2 tabs
            if len(self.driver_nano.window_handles) < 2:
                self.driver_nano.execute_script(f"window.open('{target_url}');")
                
            self.nano_log_display.append("âœ… NanoBanana ë¸Œë¼ìš°ì € ì—°ê²° ì„±ê³µ. ë‘ ê°œì˜ íƒ­ì„ í™•ì¸í•˜ì„¸ìš”.")
            self.nano_status_label.setText("2ë‹¨ê³„: í”„ë¡¬í”„íŠ¸ ì…ë ¥ í›„ ì‹œì‘ ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.")
            
        except Exception as e:
            self.nano_log_display.append(f"âŒ ë¸Œë¼ìš°ì € ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            self.nano_status_label.setText("ì˜¤ë¥˜ ë°œìƒ (ë¡œê·¸ í™•ì¸)")

    def launch_browser_imagefx(self):
        try:
            self.fx_log_display.append("ğŸŒ ImageFXìš© ë¸Œë¼ìš°ì €ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤...")
            chrome_cmd = r'C:\Program Files\Google\Chrome\Application\chrome.exe'
            user_data = r'C:\sel_chrome_fx'
            target_url = "https://labs.google/fx/ko/tools/image-fx"
            if not os.path.exists(user_data): os.makedirs(user_data)
            subprocess.Popen([chrome_cmd, '--remote-debugging-port=9223', f'--user-data-dir={user_data}', target_url])
            
            time.sleep(3)
            opt = Options()
            opt.add_experimental_option("debuggerAddress", "127.0.0.1:9223")
            self.driver_fx = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=opt)
            
            # 2ë²ˆì§¸ íƒ­
            if len(self.driver_fx.window_handles) < 2:
                self.driver_fx.execute_script(f"window.open('{target_url}');")
            
            self.fx_log_display.append("âœ… ImageFX ì¤€ë¹„ë¨. ë¡œê·¸ì¸ í›„ ì‹œì‘ ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.")
            self.fx_status_label.setText("ìƒíƒœ: ë¸Œë¼ìš°ì € ì¤€ë¹„ë¨.")
        except Exception as e:
            self.fx_log_display.append(f"âŒ ì˜¤ë¥˜: {e}")

    def start_automation_imagefx(self):
        if not hasattr(self, 'driver_fx') or self.driver_fx is None:
            QMessageBox.warning(self, "ê²½ê³ ", "ë¨¼ì € ë¸Œë¼ìš°ì €ë¥¼ ì¤€ë¹„í•´ ì£¼ì„¸ìš”.")
            return

        text = self.fx_prompt_input.toPlainText().strip()
        if not text:
            QMessageBox.warning(self, "ê²½ê³ ", "í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
            return

        # í”„ë¡¬í”„íŠ¸ íŒŒì‹±: 1. {í”„ë¡¬í”„íŠ¸} í˜•íƒœ ì§€ì›
        # ê¸°ì¡´: match = re.match(r'^(\d+)\.?\s*(.*)', line.strip())
        # ë³€ê²½: re.findall ì‚¬ìš©
        
        # 1. 1. {ë‚´ìš©} í˜•íƒœ ìš°ì„  íŒŒì‹±
        parsed_items = re.findall(r'(\d+)\s*\.\s*\{(.*?)\}', text, re.DOTALL)
        
        if parsed_items:
            items = parsed_items
        else:
            # 2. ê¸°ì¡´ ë°©ì‹ (1. ë‚´ìš©) íŒŒì‹± (ë°±ì—…)
            for line in text.split('\n'):
                match = re.match(r'^(\d+(?:-\d+)?)\.?\s*(.*)', line.strip())
                if match:
                    items.append((match.group(1), match.group(2)))

        if not items:
            QMessageBox.warning(self, "ê²½ê³ ", "ì˜¬ë°”ë¥¸ í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤ (ì˜ˆ: 1. {í”„ë¡¬í”„íŠ¸})")
            return

        target_dir = self.fx_image_path_edit.text().strip()
        self.btn_fx_start.setEnabled(False)
        self.btn_fx_stop.setEnabled(True)
        self.start_time_fx = time.time()
        if not self.ui_timer.isActive():
            self.ui_timer.start(1000)

        self.fx_worker = ImageFXMultiTabWorker("", items, self.driver_fx, target_dir)
        self.fx_worker.log_signal.connect(self.fx_log_display.append)
        self.fx_worker.progress.connect(lambda p: self.fx_status_label.setText(p))
        self.fx_worker.finished.connect(self.on_success_fx)
        self.fx_worker.error.connect(self.on_error_fx)
        self.fx_worker.start()

    def on_success_fx(self, msg, elapsed):
        self.start_time_fx = 0
        # If no other timer is running, stop the timer
        if self.start_time_gen == 0:
            self.ui_timer.stop()
            
        self.btn_fx_start.setEnabled(True)
        self.btn_fx_stop.setEnabled(False)
        self.fx_log_display.append(f"ğŸ {msg}")
        
        # ìë™ ì••ì¶• (Tab 1ê³¼ ë™ì¼ ë¡œì§ ì‚¬ìš©)
        if hasattr(self, 'fx_worker') and self.fx_worker.target_dir:
            self.fx_log_display.append("ğŸ”„ ìƒì„± ì™„ë£Œ: ìë™ ì••ì¶•(JPG ë³€í™˜)ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            self.compress_images_custom(self.fx_image_path_edit, self.fx_log_display)

    def on_error_fx(self, err):
        self.start_time_fx = 0
        if self.start_time_gen == 0:
            self.ui_timer.stop()
            
        self.btn_fx_start.setEnabled(True)
        self.btn_fx_stop.setEnabled(False)
        self.fx_log_display.append(f"â— ì˜¤ë¥˜: {err}")

    def stop_automation_imagefx(self):
        if hasattr(self, 'fx_worker') and self.fx_worker.isRunning():
            self.fx_worker.stop()
            self.fx_log_display.append("ğŸ›‘ ì¤‘ì§€ ìš”ì²­ ì¤‘... (í˜„ì¬ ì‘ì—… ì™„ë£Œ í›„ ì¤‘ë‹¨ë©ë‹ˆë‹¤)")
            self.btn_fx_stop.setEnabled(False)

    def compress_images_custom(self, path_edit, log_widget):
        target_dir = path_edit.text().strip()
        if not os.path.exists(target_dir):
            QMessageBox.warning(self, "ê²½ê³ ", "í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
            
        log_widget.append("â³ ì´ë¯¸ì§€ ì••ì¶• ì‹œì‘...")
        count = 0
        for f in os.listdir(target_dir):
            if f.lower().endswith(('.png', '.jpg', '.jpeg')):
                img_path = os.path.join(target_dir, f)
                try:
                    img = Image.open(img_path)
                    img.save(img_path, "JPEG", quality=85, optimize=True)
                    count += 1
                except:
                    pass
        log_widget.append(f"âœ… total {count} images compressed.")

    def browse_audio_path(self):
        path = QFileDialog.getExistingDirectory(self, "ì˜¤ë””ì˜¤ ì €ì¥ í´ë” ì„ íƒ")
        if path:
            self.audio_path_edit.setText(path)

    def update_timer_display(self):
        now = time.time()
        
        # GenSpark Timer
        if hasattr(self, 'start_time_gen') and self.start_time_gen > 0:
            elapsed = int(now - self.start_time_gen)
            h, m, s = elapsed // 3600, (elapsed % 3600) // 60, elapsed % 60
            if hasattr(self, 'timer_label'):
                self.timer_label.setText(f"ì†Œìš” ì‹œê°„: {h:02d}:{m:02d}:{s:02d}")
        
            if hasattr(self, 'fx_timer_label'):
                self.fx_timer_label.setText(f"ì†Œìš” ì‹œê°„: {h:02d}:{m:02d}:{s:02d}")

        # NanoBanana Timer
        if hasattr(self, 'start_time_nano') and self.start_time_nano > 0:
            elapsed = int(now - self.start_time_nano)
            h, m, s = elapsed // 3600, (elapsed % 3600) // 60, elapsed % 60
            if hasattr(self, 'nano_timer_label'):
                self.nano_timer_label.setText(f"ì†Œìš” ì‹œê°„: {h:02d}:{m:02d}:{s:02d}")

    def start_automation(self):
        if not self.driver:
            self.log_display.append("âŒ ë¸Œë¼ìš°ì €ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        text = self.image_prompt_input.toPlainText().strip()
        if not text:
            self.log_display.append("âŒ ì…ë ¥ëœ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # í”„ë¡¬í”„íŠ¸ íŒŒì‹±: (\d+)\s*\.\s*\{(.*?)\}
        self.loaded_items = re.findall(r'(\d+)\s*\.\s*\{(.*?)\}', text, re.DOTALL)
        
        if not self.loaded_items:
            self.log_display.append("âŒ í”„ë¡¬í”„íŠ¸ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ (ì˜ˆ: 1. {í”„ë¡¬í”„íŠ¸})")
            return

        self.btn_start.setEnabled(False)
        self.btn_stop.setEnabled(True)
        self.start_time_gen = time.time()
        if not self.ui_timer.isActive():
            self.ui_timer.start(1000) 
        
        # ê°€ìƒì˜ íŒŒì¼ ê²½ë¡œ ì‚¬ìš©
        self.current_file_path = "manual_input_" + time.strftime("%H%M%S")
        
        image_target = self.image_path_edit.text().strip()
        self.worker = GenSparkMultiTabWorker(self.current_file_path, self.loaded_items, self.driver, custom_target_dir=image_target)
        self.worker.progress.connect(self.status_label.setText)
        self.worker.log_signal.connect(lambda m: self.log_display.append(m))
        self.worker.finished.connect(self.on_success)
        self.worker.error.connect(self.on_error)
        self.worker.start()

    def on_success(self, msg, elapsed):
        self.start_time_gen = 0
        if self.start_time_fx == 0:
            self.ui_timer.stop()
            
        self.btn_start.setEnabled(True)
        self.btn_stop.setEnabled(False)
        self.log_display.append(f"ğŸ {msg}")
        
        # ìƒì„± ì™„ë£Œ í›„ ìë™ ì••ì¶• ì‹¤í–‰
        if hasattr(self, 'worker') and self.worker.target_dir:
            self.log_display.append("ğŸ”„ ìƒì„± ì™„ë£Œ: ìë™ ì••ì¶•(JPG ë³€í™˜)ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            self.compress_images(dir_path=self.worker.target_dir)

    def on_error(self, err):
        self.start_time_gen = 0
        if self.start_time_fx == 0:
            self.ui_timer.stop()
            
        self.btn_start.setEnabled(True)
        self.btn_stop.setEnabled(False)
        self.log_display.append(f"â— ì˜¤ë¥˜: {err}")

    def stop_automation(self):
            self.log_display.append("ğŸ›‘ ì¤‘ì§€ ìš”ì²­ ì¤‘... (í˜„ì¬ ì‘ì—… ì™„ë£Œ í›„ ì¤‘ë‹¨ë©ë‹ˆë‹¤)")
            self.btn_stop.setEnabled(False)

    def start_automation_nanobanana(self):
        if not hasattr(self, 'driver_nano') or not self.driver_nano:
            self.nano_log_display.append("âŒ ë¸Œë¼ìš°ì €ê°€ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return
        
        text = self.nano_prompt_input.toPlainText().strip()
        if not text:
            self.nano_log_display.append("âŒ ì…ë ¥ëœ í”„ë¡¬í”„íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        # í”„ë¡¬í”„íŠ¸ íŒŒì‹±: (\d+)\s*\.\s*\{(.*?)\}
        loaded_items = re.findall(r'(\d+)\s*\.\s*\{(.*?)\}', text, re.DOTALL)
        
        if not loaded_items:
            # Fallback for old format
            loaded_items = []
            for line in text.split('\n'):
                match = re.match(r'^(\d+(?:-\d+)?)\.?\s*(.*)', line.strip())
                if match:
                    loaded_items.append((match.group(1), match.group(2)))

        if not loaded_items:
            self.nano_log_display.append("âŒ í”„ë¡¬í”„íŠ¸ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤ (ì˜ˆ: 1. {í”„ë¡¬í”„íŠ¸})")
            return

        self.btn_nano_start.setEnabled(False)
        self.btn_nano_stop.setEnabled(True)
        self.start_time_nano = time.time()
        if not self.ui_timer.isActive():
            self.ui_timer.start(1000) 
        
        file_path = "nano_" + time.strftime("%H%M%S")
        image_target = self.nano_image_path_edit.text().strip()
        
        self.worker_nano = NanoBananaMultiTabWorker(file_path, loaded_items, self.driver_nano, custom_target_dir=image_target)
        self.worker_nano.progress.connect(self.nano_status_label.setText)
        self.worker_nano.log_signal.connect(lambda m: self.nano_log_display.append(m))
        self.worker_nano.finished.connect(self.on_success_nano)
        self.worker_nano.error.connect(self.on_error_nano)
        self.worker_nano.start()

    def on_success_nano(self, msg, elapsed):
        self.start_time_nano = 0
        if self.start_time_gen == 0 and self.start_time_fx == 0:
            self.ui_timer.stop()
            
        self.btn_nano_start.setEnabled(True)
        self.btn_nano_stop.setEnabled(False)
        self.nano_log_display.append(f"ğŸ {msg}")
        
        # ìƒì„± ì™„ë£Œ í›„ ìë™ ì••ì¶• ì‹¤í–‰
        if hasattr(self, 'worker_nano') and self.worker_nano.target_dir:
            self.nano_log_display.append("ğŸ”„ ìƒì„± ì™„ë£Œ: ìë™ ì••ì¶•(JPG ë³€í™˜)ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
            self.compress_images(dir_path=self.worker_nano.target_dir)

    def on_error_nano(self, err):
        self.start_time_nano = 0
        if self.start_time_gen == 0 and self.start_time_fx == 0:
            self.ui_timer.stop()
            
        self.btn_nano_start.setEnabled(True)
        self.btn_nano_stop.setEnabled(False)
        self.nano_log_display.append(f"â— ì˜¤ë¥˜: {err}")

    def stop_automation_nanobanana(self):
        if hasattr(self, 'worker_nano') and self.worker_nano.isRunning():
            self.worker_nano.stop()
            self.nano_log_display.append("ğŸ›‘ ì¤‘ì§€ ìš”ì²­ ì¤‘... (í˜„ì¬ ì‘ì—… ì™„ë£Œ í›„ ì¤‘ë‹¨ë©ë‹ˆë‹¤)")
            self.btn_nano_stop.setEnabled(False)

    def compress_images(self, dir_path=None):
        if not dir_path:
            dir_path = QFileDialog.getExistingDirectory(self, "ì´ë¯¸ì§€ê°€ ìˆëŠ” í´ë” ì„ íƒ")
            
        if not dir_path:
            return
            
        self.log_display.append(f"ğŸ“¦ ì••ì¶•(JPG ë³€í™˜) ì‹œì‘: {dir_path}")
        try:
            count = 0
            saved_size = 0
            for root, dirs, files in os.walk(dir_path):
                for file in files:
                    lower_file = file.lower()
                    if lower_file.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.webp', '.jfif')):
                        full_path = os.path.join(root, file)
                        try:
                            old_size = os.path.getsize(full_path)
                            
                            # ì´ë¯¸ì§€ ì—´ê¸° ë° RGB ë³€í™˜
                            img = Image.open(full_path)
                            rgb_img = img.convert('RGB')
                            
                            # ìƒˆ íŒŒì¼ ê²½ë¡œ (í™•ì¥ìë¥¼ jpgë¡œ ë³€ê²½)
                            file_base = os.path.splitext(full_path)[0]
                            new_path = file_base + ".jpg"
                            
                            # JPGë¡œ ì €ì¥ (ì••ì¶•ë¥  85%)
                            rgb_img.save(new_path, "JPEG", optimize=True, quality=85)
                            
                            new_size = os.path.getsize(new_path)
                            saved_size += (old_size - new_size)
                            count += 1
                            
                            # ì›ë³¸ì´ jpgê°€ ì•„ë‹ˆì—ˆê³ , íŒŒì¼ëª…ì´ ë‹¬ë¼ì¡Œë‹¤ë©´ ì›ë³¸ ì‚­ì œ
                            if full_path != new_path:
                                os.remove(full_path)
                                
                        except Exception as e:
                            self.log_display.append(f"  âŒ {file} ì‹¤íŒ¨: {e}")
                            
            mb_saved = saved_size / (1024 * 1024)
            self.log_display.append(f"âœ… ë³€í™˜ ì™„ë£Œ: {count}ê°œ íŒŒì¼ ì²˜ë¦¬ë¨.")
            self.log_display.append(f"ğŸ“‰ ì´ ì ˆì•½ ìš©ëŸ‰: {mb_saved:.2f} MB")
            
        except Exception as e:
            self.log_display.append(f"âŒ ì••ì¶• ì¤‘ ì˜¤ë¥˜: {e}")


    def initTab7(self):
        layout = QVBoxLayout()

        # 1. Filter Group
        filter_layout = QGridLayout()
        
        # API Key
        self.combo_yt_key = QComboBox()
        # Load keys from DB (using tts_client if available)
        self.yt_keys = []
        if hasattr(self, 'tts_client') and self.tts_client:
            self.yt_keys = self.tts_client.get_youtube_keys()
            for k in self.yt_keys:
                self.combo_yt_key.addItem(k['name'], k['api_key'])
        
        filter_layout.addWidget(QLabel("í‚¤ (API Key):"), 0, 0)
        filter_layout.addWidget(self.combo_yt_key, 0, 1)

        # Search Date (Days)
        self.combo_yt_days = QComboBox()
        self.combo_yt_days.addItem("1 ì¼ê°„", 1)
        self.combo_yt_days.addItem("2 ì¼ê°„", 2)
        self.combo_yt_days.addItem("3 ì¼ê°„", 3)
        self.combo_yt_days.addItem("4 ì¼ê°„", 4)
        self.combo_yt_days.addItem("5 ì¼ê°„", 5)
        
        filter_layout.addWidget(QLabel("ê²€ìƒ‰ì¼ì:"), 0, 2)
        filter_layout.addWidget(self.combo_yt_days, 0, 3)

        # Video Type
        self.combo_yt_type = QComboBox()
        self.combo_yt_type.addItem("ì‡¼ì¸  (Short)", "short")
        self.combo_yt_type.addItem("ì „ì²´ (Any)", "any")
        self.combo_yt_type.addItem("ì¤‘ì˜ìƒ (Medium, 4~20ë¶„)", "medium")
        self.combo_yt_type.addItem("ì¥ì˜ìƒ (Long, 20ë¶„+)", "long")
        
        filter_layout.addWidget(QLabel("ì˜ìƒì¢…ë¥˜:"), 1, 0)
        filter_layout.addWidget(self.combo_yt_type, 1, 1)

        # Search Query
        self.edit_yt_query = QLineEdit()
        self.edit_yt_query.setPlaceholderText("ê²€ìƒ‰ì–´ ì…ë ¥")
        self.edit_yt_query.returnPressed.connect(self.start_youtube_search)
        
        self.btn_yt_search = QPushButton("ê²€ìƒ‰")
        self.btn_yt_search.setStyleSheet("background-color: #0056b3; color: white; font-weight: bold;")
        self.btn_yt_search.clicked.connect(self.start_youtube_search)

        filter_layout.addWidget(QLabel("ê²€ìƒ‰ì–´:"), 1, 2)
        
        query_layout = QHBoxLayout()
        query_layout.addWidget(self.edit_yt_query)
        query_layout.addWidget(self.btn_yt_search)
        filter_layout.addLayout(query_layout, 1, 3)
        
        layout.addLayout(filter_layout)
        
        # 2. Result Table
        self.table_youtube = QTableWidget()
        self.table_youtube.setColumnCount(13)
        self.table_youtube.setHorizontalHeaderLabels([
            "ë²ˆí˜¸", "ì¸ë„¤ì¼", "ì±„ë„ëª…", "ì œëª©", "ì¡°íšŒìˆ˜", "êµ¬ë…ì", "ì¡°íšŒìˆ˜/êµ¬ë…ì", "ì˜ìƒê¸¸ì´", "ì˜ìƒìˆ˜", "ê¸°ë³¸ì–¸ì–´", "ì˜¤ë””ì˜¤ì–¸ì–´", "ì±„ë„êµ­ê°€", "ì—…ë¡œë“œë‚ ì§œ"
        ])
        
        # Style
        self.table_youtube.verticalHeader().setVisible(False)
        self.table_youtube.setEditTriggers(QAbstractItemView.NoEditTriggers)
        self.table_youtube.setSelectionBehavior(QAbstractItemView.SelectRows)
        self.table_youtube.setIconSize(QRect(0,0,120,90).size()) # Thumbnail Size
        self.table_youtube.setColumnWidth(1, 130) # Thumbnail Column
        self.table_youtube.cellClicked.connect(self.on_table_cell_clicked) # Click Event
        
        header = self.table_youtube.horizontalHeader()
        # ëª¨ë“  ì»¬ëŸ¼ì´ ë‚´ìš©ì— ë§ì¶° ëŠ˜ì–´ë‚˜ë„ë¡ ì„¤ì •
        header.setSectionResizeMode(QHeaderView.ResizeToContents)
        # ì¸ë„¤ì¼(1), ì œëª©(3) ë“± ì¼ë¶€ ì»¬ëŸ¼ì€ ê³ ì •í•˜ê±°ë‚˜ ë¹„ìœ¨ ì¡°ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‚˜ ìš°ì„  ë‹¤ ë³´ì´ê²Œ ì„¤ì •
        
        layout.addWidget(self.table_youtube)

        # 3. Log
        layout.addWidget(QLabel("ë¡œê·¸:"))
        self.log_youtube = QTextEdit()
        self.log_youtube.setReadOnly(True)
        self.log_youtube.setMaximumHeight(100)
        self.log_youtube.setStyleSheet("background-color: #1E1E1E; color: #D4D4D4;")
        layout.addWidget(self.log_youtube)

        self.tab7.setLayout(layout)

    def start_youtube_search(self):
        # 1. Validation
        api_key = self.combo_yt_key.currentData()
        if not api_key:
            if self.combo_yt_key.count() > 0:
                # If data was not set properly but text exists (fallback)
                idx = self.combo_yt_key.currentIndex()
                if 0 <= idx < len(self.yt_keys):
                     api_key = self.yt_keys[idx]['api_key']
            
            if not api_key:
                QMessageBox.warning(self, "ê²½ê³ ", "YouTube API í‚¤ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”. (DBì— í‚¤ê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤)")
                return

        query = self.edit_yt_query.text().strip()
        if not query:
            QMessageBox.warning(self, "ê²½ê³ ", "ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            return
            
        days = self.combo_yt_days.currentData()
        video_type = self.combo_yt_type.currentData()
        
        # 2. UI Update
        self.btn_yt_search.setEnabled(False)
        self.table_youtube.setSortingEnabled(False) # Disable sorting while clearing/inserting
        self.table_youtube.setRowCount(0)
        self.log_youtube.append(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: '{query}' (ìµœê·¼ {days}ì¼, {video_type})")
        
        # 3. Start Worker
        self.worker_yt = YoutubeSearchWorker(api_key, query, days, video_type)
        self.worker_yt.log_signal.connect(self.log_youtube.append)
        self.worker_yt.finished.connect(self.on_yt_search_done)
        self.worker_yt.error.connect(lambda e: [self.log_youtube.append(f"âŒ {e}"), self.btn_yt_search.setEnabled(True)])
        self.worker_yt.start()

    def on_yt_search_done(self, results):
        self.btn_yt_search.setEnabled(True)
        if not results:
            self.log_youtube.append("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        self.table_youtube.setRowCount(len(results))
        self.table_youtube.setStyleSheet("QTableWidget::item { padding: 5px; }")
        
        img_tasks = []
        
        for r, row in enumerate(results):
            # Helper for alignment
            def make_item(text, align):
                it = QTableWidgetItem(str(text))
                it.setTextAlignment(align)
                return it

            # Helper for numeric items
            def make_numeric_item(text, align, color=None, font=None):
                it = NumericTableWidgetItem(str(text))
                it.setTextAlignment(align)
                if color: it.setForeground(color)
                if font: it.setFont(font)
                return it

            # 0: Number (Center)
            self.table_youtube.setItem(r, 0, make_numeric_item(row['number'], Qt.AlignCenter))
            
            # 1: Thumbnail (Placeholder first)
            thumb_item = QTableWidgetItem("Loading...")
            thumb_item.setData(Qt.UserRole, row.get('video_id')) # Store Video ID
            self.table_youtube.setItem(r, 1, thumb_item)
            if row['thumbnail_url']:
                img_tasks.append((r, row['thumbnail_url']))
            
            # 2: Channel (Left)
            chan_item = make_item(row['channel_name'], Qt.AlignLeft | Qt.AlignVCenter)
            chan_item.setData(Qt.UserRole, row.get('channel_id')) # Store Channel ID
            self.table_youtube.setItem(r, 2, chan_item)
            
            # 3: Title (Left)
            self.table_youtube.setItem(r, 3, make_item(row['title'], Qt.AlignLeft | Qt.AlignVCenter))
            
            # 4: Views (Right) - Numeric
            self.table_youtube.setItem(r, 4, make_numeric_item(f"{row['view_count']:,}", Qt.AlignRight | Qt.AlignVCenter))
            
            # 5: Subs (Right) - Numeric
            self.table_youtube.setItem(r, 5, make_numeric_item(f"{row['subscriber_count']:,}", Qt.AlignRight | Qt.AlignVCenter))
            
            # 6: Ratio (Right) [New] - Numeric
            ratio = 0
            if row['subscriber_count'] > 0:
                ratio = (row['view_count'] / row['subscriber_count']) * 100
            
            # ìƒ‰ìƒ ê°•ì¡°: 100% ì´ìƒì´ë©´ ì´ˆë¡, 50% ì´ìƒ íŒŒë‘, ê·¸ì™¸ í‰ë²”
            ratio_text = f"{ratio:.1f}%"
            ratio_color = QColor("#D4D4D4")
            ratio_font = None
            
            if ratio >= 100:
                ratio_color = QColor("#4CAF50") # Green
                ratio_font = QFont("Arial", 9, QFont.Bold)
            elif ratio >= 50:
                ratio_color = QColor("#2196F3") # Blue
                ratio_font = QFont("Arial", 9, QFont.Bold)
                 
            self.table_youtube.setItem(r, 6, make_numeric_item(ratio_text, Qt.AlignRight | Qt.AlignVCenter, ratio_color, ratio_font))

            # 7: Duration (Center) - Moved here
            self.table_youtube.setItem(r, 7, make_item(row.get('duration_str', '-'), Qt.AlignCenter))

            # 8: Video Total (Center) - Numeric
            self.table_youtube.setItem(r, 8, make_numeric_item(f"{row['video_total']:,}", Qt.AlignCenter))
            
            # 9: Lang (Center)
            self.table_youtube.setItem(r, 9, make_item(row['lang'], Qt.AlignCenter))
            
            # 10: Audio Lang (Center)
            self.table_youtube.setItem(r, 10, make_item(row['audio_lang'], Qt.AlignCenter))
            
            # 11: Country (Center)
            self.table_youtube.setItem(r, 11, make_item(row['country'], Qt.AlignCenter))
            
            # 12: Date (Center)
            date_str = row['published_at'].replace("T", " ").replace("Z", "")
            self.table_youtube.setItem(r, 12, make_item(date_str, Qt.AlignCenter))
            
            # Row Height adjustment for thumbnail
            self.table_youtube.setRowHeight(r, 96)
            
        # 4. Enable Sorting (Turn on after populating to avoid weird jumps during insert, or just set it here)
        self.table_youtube.setSortingEnabled(True)

        # Start Image Loader
        if img_tasks:
            self.worker_img = ImageLoadWorker(img_tasks)
            self.worker_img.loaded.connect(self.on_thumb_loaded)
            self.worker_img.start()

    def on_thumb_loaded(self, row, pixmap):
        item = QTableWidgetItem()
        # Scale pixmap to fit icon size nicely?
        # Icon handles scaling usually, but good to be explicit if needed.
        # scaled_pix = pixmap.scaled(120, 90, Qt.KeepAspectRatio, Qt.SmoothTransformation)
        from PyQt5.QtGui import QIcon
        # ê¸°ì¡´ ì•„ì´í…œì„ ê°€ì ¸ì™€ì„œ ì•„ì´ì½˜ë§Œ ì„¤ì • (ë°ì´í„° ë³´ì¡´)
        item = self.table_youtube.item(row, 1)
        if not item:
            item = QTableWidgetItem()
            self.table_youtube.setItem(row, 1, item)
            
        item.setIcon(QIcon(pixmap))
        item.setText("") # Remove loading text

    def on_table_cell_clicked(self, row, col):
        # ì„ íƒ ì‹œì—ë„ ì»¬ëŸ¼ í¬ê¸° ìœ ì§€ (ë˜ëŠ” ì¬ì¡°ì •)
        header = self.table_youtube.horizontalHeader()
        header.setSectionResizeMode(QHeaderView.ResizeToContents)

        item = self.table_youtube.item(row, col)
        if not item: return
        
        data_id = item.data(Qt.UserRole)
        if not data_id: return
        
        url = ""
        if col == 1: # Thumbnail -> Video
            url = f"https://www.youtube.com/watch?v={data_id}"
        elif col == 2: # Channel Name -> Channel
            url = f"https://www.youtube.com/channel/{data_id}"
            
        if url:
            self.log_youtube.append(f"ğŸŒ ë§í¬ ì—´ê¸°: {url}")
            webbrowser.open(url)            

    def initTabAudioNormal(self):
        layout = QVBoxLayout()
        
        # ì•ˆë‚´
        layout.addWidget(QLabel("ğŸ“¢ MP3 ì˜¤ë””ì˜¤ íŒŒì¼ì˜ ë³¼ë¥¨ì„ ì¼ì •í•˜ê²Œ í‰ì¤€í™”(Normalization) í•©ë‹ˆë‹¤."))
        layout.addWidget(QLabel("   (ElevenLabs ìë§‰ ì‹±í¬(Duration)ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šìœ¼ë¯€ë¡œ ì•ˆì‹¬í•˜ê³  ì‚¬ìš©í•˜ì„¸ìš”.)"))

        # í´ë” ì„ íƒ ê·¸ë£¹
        dir_group = QGroupBox("í´ë” ì„ íƒ")
        dir_layout = QGridLayout()
        
        self.an_input_dir = QLineEdit(r"D:\youtube")
        btn_in = QPushButton("ì…ë ¥ í´ë”")
        btn_in.clicked.connect(lambda: self.browse_folder(self.an_input_dir))
        
        self.an_output_dir = QLineEdit(r"D:\youtube\normalized")
        btn_out = QPushButton("ì¶œë ¥ í´ë”")
        btn_out.clicked.connect(lambda: self.browse_folder(self.an_output_dir))
        
        dir_layout.addWidget(QLabel("ì…ë ¥(ì›ë³¸) í´ë”:"), 0, 0)
        dir_layout.addWidget(self.an_input_dir, 0, 1)
        dir_layout.addWidget(btn_in, 0, 2)
        
        dir_layout.addWidget(QLabel("ì¶œë ¥(ì €ì¥) í´ë”:"), 1, 0)
        dir_layout.addWidget(self.an_output_dir, 1, 1)
        dir_layout.addWidget(btn_out, 1, 2)
        
        dir_group.setLayout(dir_layout)
        layout.addWidget(dir_group)
        
        # ì‹œì‘ ë²„íŠ¼
        self.btn_start_an = QPushButton("ğŸ”Š ì˜¤ë””ì˜¤ í‰ì¤€í™” ì‹œì‘ (Start Normalization)")
        self.btn_start_an.setStyleSheet("height: 50px; font-weight: bold; background-color: #009688; color: white; border-radius: 8px;")
        self.btn_start_an.clicked.connect(self.start_audio_normal)
        layout.addWidget(self.btn_start_an)
        
        # ë¡œê·¸
        self.an_log = QTextEdit()
        self.an_log.setReadOnly(True)
        self.an_log.setStyleSheet("background-color: #1E1E1E; color: #D4D4D4;")
        layout.addWidget(self.an_log)
        
        self.tab_audio_normal.setLayout(layout)

    def start_audio_normal(self):
        i_path = self.an_input_dir.text().strip()
        o_path = self.an_output_dir.text().strip()
        
        if not os.path.exists(i_path):
            QMessageBox.warning(self, "ê²½ê³ ", "ì…ë ¥ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return

        self.btn_start_an.setEnabled(False)
        self.an_log.append("â³ ì‘ì—… ì‹œì‘...")
        
        self.an_worker = AudioNormalWorker(i_path, o_path)
        self.an_worker.log_signal.connect(self.an_log.append)
        self.an_worker.finished.connect(lambda m: [self.an_log.append(f"ğŸ {m}"), self.btn_start_an.setEnabled(True)])
        self.an_worker.error.connect(lambda e: [self.an_log.append(f"âŒ {e}"), self.btn_start_an.setEnabled(True)])
        self.an_worker.start()

    def start_batch_video_effect(self):
        input_dir = self.eff_input_dir.text().strip()
        output_dir = self.eff_output_dir.text().strip()
        
        if not input_dir or not os.path.exists(input_dir):
            QMessageBox.warning(self, "ê²½ê³ ", "ì…ë ¥ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
            
        if not output_dir:
            QMessageBox.warning(self, "ê²½ê³ ", "ì¶œë ¥ í´ë”ë¥¼ ì§€ì •í•´ì£¼ì„¸ìš”.")
            return
            
        if not os.path.exists(output_dir):
            try:
                os.makedirs(output_dir)
            except:
                QMessageBox.warning(self, "ê²½ê³ ", "ì¶œë ¥ í´ë”ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return

        # ì„¤ì •ê°’ ì½ê¸°
        style = {
            'font_family': self.combo_font.currentText(),
            'font_size': self.spin_font_size.value(),
            'text_color': self.color_text,
            'outline_color': self.color_outline,
            'bg_color': self.color_bg,
            'bg_opacity': int(self.slider_bg_opacity.value() * 2.55),
            'use_bg': self.checkbox_use_bg.isChecked(),
            'use_outline': self.checkbox_use_outline.isChecked()
        }
        volume = self.slider_volume.value() / 100.0
        trim_end = self.spin_trim_end.value()
        
        # Effect Config
        effect_config = {
            'type': self.combo_effect_type.currentIndex(), 
            # 0: None, 1: Zoom, 2: Pan L->R, 3: Pan R->L
            'start_scale': self.spin_start_scale.value(),
            'end_scale': self.spin_end_scale.value(),
            'pan_speed': self.spin_pan_speed.value(),
            'random': self.chk_random_effect.isChecked()
        }

        self.btn_start_single.setEnabled(False)
        self.single_log.append(f"â³ ì¼ê´„ ì‘ì—… ì‹œì‘: {input_dir}")
        self.single_log.append(f"   ì¶œë ¥ ëŒ€ìƒ: {output_dir}")

        self.batch_eff_worker = BatchVideoEffectWorker(
            input_dir, output_dir, style, volume, trim_end, effect_config
        )
        self.batch_eff_worker.log_signal.connect(self.single_log.append)
        self.batch_eff_worker.finished.connect(lambda m, t: [self.single_log.append(f"ğŸ {m}"), self.btn_start_single.setEnabled(True)])
        self.batch_eff_worker.error.connect(lambda e: [self.single_log.append(f"âŒ {e}"), self.btn_start_single.setEnabled(True)])
        self.batch_eff_worker.start()

class BatchVideoEffectWorker(VideoMergerWorker):
    def __init__(self, input_dir, output_dir, style=None, volume=1.0, trim_end=0.0, effect_config=None):
        # ë¶€ëª¨ ìƒì„±ì í˜¸ì¶œ (ê²½ë¡œëŠ” input_dirë¡œ ì„¤ì •)
        super().__init__(input_dir, input_dir, output_dir, subtitles=None, style=style, volume=volume, trim_end=trim_end)
        self.input_dir = input_dir
        self.output_dir = output_dir
        self.effect_config = effect_config # ë¶€ëª¨ process_single_videoê°€ ì´ ì†ì„±ì„ ì°¸ì¡°í•˜ì—¬ íš¨ê³¼ ì ìš©
        
    def run(self):
        start_time = time.time()
        try:
            # MP3 íŒŒì¼ ê²€ìƒ‰
            if not os.path.exists(self.input_dir):
                self.error.emit(f"ì…ë ¥ í´ë” ì—†ìŒ: {self.input_dir}")
                return

            all_files = os.listdir(self.input_dir)
            mp3_files = [f for f in all_files if f.lower().endswith('.mp3')]
            
            if not mp3_files:
                self.error.emit("ì…ë ¥ í´ë”ì— .mp3 íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return
                
            # ìì—°ìŠ¤ëŸ¬ìš´ ì •ë ¬ (1.mp3, 2.mp3, 10.mp3)
            mp3_files.sort(key=lambda s: [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\d+)', s)])
            
            total = len(mp3_files)
            success_count = 0
            
            for idx, mp3 in enumerate(mp3_files):
                base_name = os.path.splitext(mp3)[0]
                audio_path = os.path.join(self.input_dir, mp3)
                output_path = os.path.join(self.output_dir, f"{base_name}.mp4")
                
                # ì´ë¯¸ì§€ ì°¾ê¸° (ê°™ì€ í´ë” ë‚´)
                img_path = None
                for ext in ['.png', '.jpg', '.jpeg', '.webp']:
                    check = os.path.join(self.input_dir, base_name + ext)
                    if os.path.exists(check):
                        img_path = check
                        break
                        
                if not img_path:
                    self.log_signal.emit(f"âš ï¸ [{idx+1}/{total}] ì´ë¯¸ì§€ ì—†ìŒ, ê±´ë„ˆëœ€: {base_name}")
                    continue
                
                self.log_signal.emit(f"ğŸ¬ [{idx+1}/{total}] ì²˜ë¦¬ ì¤‘: {base_name}")
                
                # [NEW] ëœë¤ íš¨ê³¼ ë¡œì§
                if self.effect_config and self.effect_config.get('random'):
                    import random
                    new_type = random.randint(1, 3) # 1~3 (Zoom, PanLR, PanRL)
                    self.effect_config['type'] = new_type
                    
                    eff_names = ["None", "Zoom", "Pan(L->R)", "Pan(R->L)"]
                    if 0 <= new_type < len(eff_names):
                        self.log_signal.emit(f"   ğŸ² ëœë¤ íš¨ê³¼ ì ìš©: {eff_names[new_type]}")
                
                # ìë§‰ ìë™ ë¡œë“œ (ë¶€ëª¨ í´ë˜ìŠ¤ê°€ JSON ìë™ ë¡œë“œí•¨)
                # Task ì¤€ë¹„ (img, audio, output, base_name, manual_subs=None)
                task = (img_path, audio_path, output_path, base_name, None)
                
                # process_single_video í˜¸ì¶œ
                res = self.process_single_video(task)
                if res:
                    success_count += 1
            
            elapsed = time.time() - start_time
            self.finished.emit(f"ì „ì²´ ì‘ì—… ì™„ë£Œ: {success_count}/{total} ì„±ê³µ", elapsed)
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            self.error.emit(f"ì˜¤ë¥˜: {e}")

class VideoConcatenatorWorker(QThread):
    log_signal = pyqtSignal(str)
    finished = pyqtSignal(str, float)
    error = pyqtSignal(str)

    def __init__(self, video_dir, output_file, watermark_path=None):
        super().__init__()
        self.video_dir = video_dir
        self.output_file = output_file
        self.watermark_path = watermark_path

    def run(self):
        start_time = time.time()
        temp_list_path = ""
        try:
            self.log_signal.emit("ğŸ“‚ ì˜ìƒ í•©ì¹˜ê¸° ì¤€ë¹„ ì¤‘ (Concat Demuxer Mode)...")
            
            ffmpeg_exe = "ffmpeg"
            try:
                import imageio_ffmpeg
                ffmpeg_exe = imageio_ffmpeg.get_ffmpeg_exe()
            except: pass

            if not os.path.exists(self.video_dir):
                self.error.emit("ì…ë ¥ í´ë”ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return

            all_files = os.listdir(self.video_dir)
            files = [os.path.join(self.video_dir, f) for f in all_files if f.lower().endswith(('.mp4', '.avi', '.mov', '.mkv'))]
            
            if not files:
                self.error.emit("í•©ì¹  ì˜ìƒ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return

            files.sort(key=lambda s: [int(t) if t.isdigit() else t.lower() for t in re.split(r'(\d+)', s)])
            
            self.log_signal.emit(f"ğŸ”¢ ì´ {len(files)}ê°œì˜ ì˜ìƒ íŒŒì¼ ë°œê²¬.")

            temp_list_path = os.path.join(self.video_dir, f"concat_list_{int(time.time())}.txt")
            with open(temp_list_path, "w", encoding='utf-8') as f:
                for vid_path in files:
                    safe_path = vid_path.replace("\\", "/").replace("'", "'\\''")
                    f.write(f"file '{safe_path}'\n")
            
            command = [ffmpeg_exe]
            command.extend(["-y", "-f", "concat", "-safe", "0", "-i", temp_list_path])
            
            map_options = []
            
            if self.watermark_path and os.path.exists(self.watermark_path):
                command.extend(["-i", self.watermark_path])
                filter_complex = "[1:v]scale=100:-1[wm];[0:v][wm]overlay=20:20[v_out]"
                command.extend(["-filter_complex", filter_complex])
                map_options = ["-map", "[v_out]", "-map", "0:a"]
            else:
                map_options = ["-map", "0"]

            command.extend(map_options)
            
            command.extend(["-c:v", "libx264", "-preset", "medium", "-pix_fmt", "yuv420p"])
            command.extend(["-c:a", "aac", "-b:a", "192k"])
            
            command.append(self.output_file)
            
            self.log_signal.emit(f"ğŸš€ í•©ì¹˜ê¸° ì‹¤í–‰ (íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ë°©ì‹)...")
            
            creation_flags = 0x08000000 if os.name == 'nt' else 0
            process = subprocess.Popen(
                command, 
                stdout=subprocess.PIPE, 
                stderr=subprocess.PIPE, 
                universal_newlines=True, 
                encoding='utf-8',
                creationflags=creation_flags
            )
            
            stdout, stderr = process.communicate()
            
            if process.returncode != 0:
                self.error.emit(f"âŒ FFmpeg ì˜¤ë¥˜: {stderr}")
            else:
                elapsed = time.time() - start_time
                self.finished.emit(f"âœ… ì™„ë£Œ: {os.path.basename(self.output_file)}", elapsed)
            
        except Exception as e:
            self.error.emit(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
        finally:
            if temp_list_path and os.path.exists(temp_list_path):
                try: os.remove(temp_list_path)
                except: pass

def exception_hook(exctype, value, tb):
    tb_str = "".join(traceback.format_exception(exctype, value, tb))
    print(tb_str)
    # Use static method for QMessageBox if possible, or just create it
    QMessageBox.critical(None, "Fatal Error", f"ì‹¬ê°í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤:\n\n{tb_str}")
    sys.exit(1)

class NumericTableWidgetItem(QTableWidgetItem):
    def __lt__(self, other):
        try:
            # ì‰¼í‘œ, %, ê³µë°± ì œê±° í›„ ë¹„êµ
            v1 = float(self.text().replace(',', '').replace('%', '').strip())
            v2 = float(other.text().replace(',', '').replace('%', '').strip())
            return v1 < v2
        except ValueError:
            # ìˆ«ìê°€ ì•„ë‹ˆë©´ ë¬¸ìì—´ ë¹„êµ
            return super().__lt__(other)

if __name__ == '__main__':
    sys.excepthook = exception_hook
    app = QApplication(sys.argv)
    
    # ë‹¤í¬ í…Œë§ˆ ì ìš©
    # Modern Dark Theme Setup
    app.setStyle("Fusion")
    
    # 1. Color Palette (VS Code Dark Theme Inspired)
    dark_palette = QPalette()
    
    # Backgrounds
    dark_palette.setColor(QPalette.Window, QColor(30, 30, 30))         # Main Window Background
    dark_palette.setColor(QPalette.WindowText, QColor(220, 220, 220))  # Main Text
    dark_palette.setColor(QPalette.Base, QColor(25, 25, 25))           # Input Fields Background
    dark_palette.setColor(QPalette.AlternateBase, QColor(35, 35, 35))  # Alternate Background
    dark_palette.setColor(QPalette.ToolTipBase, QColor(25, 25, 25))    # Tooltip Background
    dark_palette.setColor(QPalette.ToolTipText, QColor(220, 220, 220)) # Tooltip Text
    dark_palette.setColor(QPalette.Text, QColor(220, 220, 220))        # Input Text
    
    # Buttons & Inputs
    dark_palette.setColor(QPalette.Button, QColor(45, 45, 45))         # Button Background
    dark_palette.setColor(QPalette.ButtonText, QColor(220, 220, 220))  # Button Text
    dark_palette.setColor(QPalette.BrightText, Qt.red)
    
    # Links & Highlights
    dark_palette.setColor(QPalette.Link, QColor(0, 122, 204))          # Link Color
    dark_palette.setColor(QPalette.Highlight, QColor(0, 122, 204))     # Selection Background
    dark_palette.setColor(QPalette.HighlightedText, Qt.white)          # Selection Text
    
    # Disabled States
    dark_palette.setColor(QPalette.Disabled, QPalette.Text, QColor(127, 127, 127))
    dark_palette.setColor(QPalette.Disabled, QPalette.ButtonText, QColor(127, 127, 127))
    dark_palette.setColor(QPalette.Disabled, QPalette.Button, QColor(35, 35, 35))
    
    app.setPalette(dark_palette)
    
    # 2. Modern Stylesheet (QSS)
    app.setStyleSheet("""
        /* Global Reset */
        * {
            outline: none;
        }
        
        /* Tooltips */
        QToolTip { 
            color: #dcdcdc; 
            background-color: #252526; 
            border: 1px solid #3e3e42; 
        }

        /* Message Boxes */
        QMessageBox {
            background-color: #1e1e1e;
        }
        QMessageBox QLabel {
            color: #dcdcdc;
        }

        /* Input Fields (LineEdit, TextEdit, SpinBox, etc.) */
        QLineEdit, QTextEdit, QPlainTextEdit, QSpinBox, QDoubleSpinBox, QComboBox {
            background-color: #2d2d2d; /* Slightly lighter than base for visibility */
            color: #dcdcdc;
            border: 1px solid #3e3e42;
            border-radius: 4px;
            padding: 5px;
            selection-background-color: #007acc;
        }
        QLineEdit:focus, QTextEdit:focus, QPlainTextEdit:focus, QSpinBox:focus, QComboBox:focus {
            border: 1px solid #007acc;
            background-color: #1e1e1e;
        }
        
        /* Buttons - Modern Flat Look */
        QPushButton {
            background-color: #0e639c; /* Primary Blue */
            color: white;
            border: none;
            border-radius: 4px;
            padding: 6px 16px;
            font-weight: bold;
        }
        QPushButton:hover {
            background-color: #1177bb;
        }
        QPushButton:pressed {
            background-color: #094771;
            padding-top: 7px; /* Press effect */
            padding-left: 17px;
        }
        QPushButton:disabled {
            background-color: #3e3e42;
            color: #888888;
        }
        
        /* Group Box */
        QGroupBox {
            border: 1px solid #454545;
            border-radius: 6px;
            margin-top: 12px;
            padding-top: 10px;
            font-weight: bold;
        }
        QGroupBox::title {
            subcontrol-origin: margin;
            subcontrol-position: top left;
            padding: 0 5px;
            color: #007acc; /* Accent Color for Titles */
        }
        
        /* Tab Widget */
        QTabWidget::pane {
            border: 1px solid #3e3e42;
            background-color: #1e1e1e;
            top: -1px; /* Align with tab bar */
        }
        QTabBar::tab {
            background: #2d2d2d;
            color: #aaaaaa;
            padding: 8px 20px;
            margin-right: 2px;
            border-top-left-radius: 4px;
            border-top-right-radius: 4px;
        }
        QTabBar::tab:selected {
            background: #1e1e1e;
            color: #ffffff;
            border-top: 2px solid #007acc; /* Top Accent Line */
            font-weight: bold;
        }
        QTabBar::tab:hover:!selected {
            background: #3e3e40;
            color: #ffffff;
        }
        
        /* Table Widget */
        QTableWidget {
            gridline-color: #333333;
            background-color: #1e1e1e;
            selection-background-color: #094771; /* Darker Blue Selection */
            selection-color: white;
            border: 1px solid #3e3e42;
        }
        QHeaderView::section {
            background-color: #252526;
            color: #dcdcdc;
            padding: 6px;
            border: 1px solid #333333;
            font-weight: bold;
        }
        QHeaderView::section:horizontal {
            border-bottom: 2px solid #3e3e42;
        }
        QHeaderView::section:vertical {
            border-right: 2px solid #3e3e42;
        }
        
        /* Scrollbars (Webkit-like style for Qt) */
        QScrollBar:vertical {
            border: none;
            background: #1e1e1e;
            width: 14px;
            margin: 0px 0px 0px 0px;
        }
        QScrollBar::handle:vertical {
            background: #424242;
            min-height: 20px;
            border-radius: 7px;
            margin: 2px;
        }
        QScrollBar::handle:vertical:hover {
            background: #686868;
        }
        QScrollBar::add-line:vertical, QScrollBar::sub-line:vertical {
            height: 0px;
        }
        
        QScrollBar:horizontal {
            border: none;
            background: #1e1e1e;
            height: 14px;
            margin: 0px 0px 0px 0px;
        }
        QScrollBar::handle:horizontal {
            background: #424242;
            min-width: 20px;
            border-radius: 7px;
            margin: 2px;
        }
        QScrollBar::handle:horizontal:hover {
            background: #686868;
        }
        QScrollBar::add-line:horizontal, QScrollBar::sub-line:horizontal {
            width: 0px;
        }
    """)
    
    try:
        ex = MainApp()
        ex.show()
        sys.exit(app.exec_())
    except Exception as e:
        msg = traceback.format_exc()
        QMessageBox.critical(None, "Error in MainApp", f"MainApp ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜:\n\n{msg}")